<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Bayes’ Theorem Fundamentals – bayes-theorem-book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../parts/introduction-to-bayesian-thinking/setting-up-python-environment.html" rel="next">
<link href="../../parts/introduction-to-bayesian-thinking/understanding-probability.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a2a08d6480f1a07d2e84f5b3bded3372.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../parts/introduction-to-bayesian-thinking/intro.html">Introduction to Bayesian Thinking</a></li><li class="breadcrumb-item"><a href="../../parts/introduction-to-bayesian-thinking/bayes-theorem-fundamentals.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem Fundamentals</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">bayes-theorem-book</a> 
        <div class="sidebar-tools-main">
    <a href="../../bayes-theorem-book.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/introduction-to-bayesian-thinking/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Bayesian Thinking</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/understanding-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/bayes-theorem-fundamentals.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem Fundamentals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/setting-up-python-environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Setting Up Python Environment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/mathematical-foundations/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/probability-theory-essentials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/statistical-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Basic Probability Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/linear-algebra-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/implementing-bayes-theorem/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Implementing Bayes' Theorem</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/basic-implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Basic Implementation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/working-with-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/discrete-probability-examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Discrete Probability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/bayesian-inference/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/parameter-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/conjugate-priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Conjugate Priors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/prior-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Prior Selection</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/markov-chain-monte-carlo-mcmc/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Markov Chain Monte Carlo (MCMC)</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/introduction-to-mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Monte Carlo Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/mcmc-algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Introduction to MCMC</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/implementation-with-pymc3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Implementation with PyMC</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/practical-applications/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/ab-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/text-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Introduction to Text Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/medical-diagnosis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Disease Testing with Bayes’ Theorem</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/advanced-topics/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/hierarchical-bayesian-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Hierarchical Bayesian Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/bayesian-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/gaussian-processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/real-world-applications/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Real-World Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/finance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Finance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/marketing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Bayesian Methods in Customer Segmentation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/scientific-applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Scientific Applications</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/best-practices-and-advanced-tools/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Best Practices and Advanced Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/code-organization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Code Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/performance-optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Understanding Performance Bottlenecks in Bayesian Computations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/modern-bayesian-libraries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Modern Bayesian Libraries</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#bayes-life-and-work" id="toc-bayes-life-and-work" class="nav-link active" data-scroll-target="#bayes-life-and-work"><span class="header-section-number">3.0.1</span> Bayes’ Life and Work</a></li>
  <li><a href="#early-applications-of-bayes-theorem" id="toc-early-applications-of-bayes-theorem" class="nav-link" data-scroll-target="#early-applications-of-bayes-theorem"><span class="header-section-number">3.0.2</span> Early Applications of Bayes’ Theorem</a></li>
  <li><a href="#the-development-of-bayesian-statistics" id="toc-the-development-of-bayesian-statistics" class="nav-link" data-scroll-target="#the-development-of-bayesian-statistics"><span class="header-section-number">3.0.3</span> The Development of Bayesian Statistics</a></li>
  <li><a href="#the-theorem-and-its-components" id="toc-the-theorem-and-its-components" class="nav-link" data-scroll-target="#the-theorem-and-its-components"><span class="header-section-number">3.1</span> The Theorem and its Components</a>
  <ul class="collapse">
  <li><a href="#introducing-bayes-theorem" id="toc-introducing-bayes-theorem" class="nav-link" data-scroll-target="#introducing-bayes-theorem"><span class="header-section-number">3.1.1</span> Introducing Bayes’ Theorem</a></li>
  <li><a href="#understanding-conditional-probability" id="toc-understanding-conditional-probability" class="nav-link" data-scroll-target="#understanding-conditional-probability"><span class="header-section-number">3.1.2</span> Understanding Conditional Probability</a></li>
  <li><a href="#the-formula-a-detailed-breakdown" id="toc-the-formula-a-detailed-breakdown" class="nav-link" data-scroll-target="#the-formula-a-detailed-breakdown"><span class="header-section-number">3.1.3</span> The Formula: A Detailed Breakdown</a></li>
  <li><a href="#visualizing-bayes-theorem" id="toc-visualizing-bayes-theorem" class="nav-link" data-scroll-target="#visualizing-bayes-theorem"><span class="header-section-number">3.1.4</span> Visualizing Bayes’ Theorem</a></li>
  </ul></li>
  <li><a href="#prior-probability" id="toc-prior-probability" class="nav-link" data-scroll-target="#prior-probability"><span class="header-section-number">3.2</span> Prior Probability</a>
  <ul class="collapse">
  <li><a href="#defining-prior-probability" id="toc-defining-prior-probability" class="nav-link" data-scroll-target="#defining-prior-probability"><span class="header-section-number">3.2.1</span> Defining Prior Probability</a></li>
  <li><a href="#types-of-priors-informative-non-informative" id="toc-types-of-priors-informative-non-informative" class="nav-link" data-scroll-target="#types-of-priors-informative-non-informative"><span class="header-section-number">3.2.2</span> Types of Priors (Informative, Non-Informative)</a></li>
  <li><a href="#choosing-an-appropriate-prior" id="toc-choosing-an-appropriate-prior" class="nav-link" data-scroll-target="#choosing-an-appropriate-prior"><span class="header-section-number">3.2.3</span> Choosing an Appropriate Prior</a></li>
  <li><a href="#impact-of-prior-on-posterior" id="toc-impact-of-prior-on-posterior" class="nav-link" data-scroll-target="#impact-of-prior-on-posterior"><span class="header-section-number">3.2.4</span> Impact of Prior on Posterior</a></li>
  </ul></li>
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood"><span class="header-section-number">3.3</span> Likelihood</a>
  <ul class="collapse">
  <li><a href="#defining-likelihood" id="toc-defining-likelihood" class="nav-link" data-scroll-target="#defining-likelihood"><span class="header-section-number">3.3.1</span> Defining Likelihood</a></li>
  <li><a href="#likelihood-function" id="toc-likelihood-function" class="nav-link" data-scroll-target="#likelihood-function"><span class="header-section-number">3.3.2</span> Likelihood Function</a></li>
  <li><a href="#interpreting-likelihood-values" id="toc-interpreting-likelihood-values" class="nav-link" data-scroll-target="#interpreting-likelihood-values"><span class="header-section-number">3.3.3</span> Interpreting Likelihood Values</a></li>
  <li><a href="#relationship-between-likelihood-and-prior" id="toc-relationship-between-likelihood-and-prior" class="nav-link" data-scroll-target="#relationship-between-likelihood-and-prior"><span class="header-section-number">3.3.4</span> Relationship between Likelihood and Prior</a></li>
  </ul></li>
  <li><a href="#posterior-probability" id="toc-posterior-probability" class="nav-link" data-scroll-target="#posterior-probability"><span class="header-section-number">3.4</span> Posterior Probability</a>
  <ul class="collapse">
  <li><a href="#defining-posterior-probability" id="toc-defining-posterior-probability" class="nav-link" data-scroll-target="#defining-posterior-probability"><span class="header-section-number">3.4.1</span> Defining Posterior Probability</a></li>
  <li><a href="#interpreting-posterior-distributions" id="toc-interpreting-posterior-distributions" class="nav-link" data-scroll-target="#interpreting-posterior-distributions"><span class="header-section-number">3.4.2</span> Interpreting Posterior Distributions</a></li>
  <li><a href="#updating-beliefs-with-data" id="toc-updating-beliefs-with-data" class="nav-link" data-scroll-target="#updating-beliefs-with-data"><span class="header-section-number">3.4.3</span> Updating Beliefs with Data</a></li>
  <li><a href="#posterior-predictive-distribution" id="toc-posterior-predictive-distribution" class="nav-link" data-scroll-target="#posterior-predictive-distribution"><span class="header-section-number">3.4.4</span> Posterior Predictive Distribution</a></li>
  </ul></li>
  <li><a href="#practical-examples-and-applications" id="toc-practical-examples-and-applications" class="nav-link" data-scroll-target="#practical-examples-and-applications"><span class="header-section-number">3.5</span> Practical Examples and Applications</a>
  <ul class="collapse">
  <li><a href="#simple-example-medical-diagnosis" id="toc-simple-example-medical-diagnosis" class="nav-link" data-scroll-target="#simple-example-medical-diagnosis"><span class="header-section-number">3.5.1</span> Simple Example: Medical Diagnosis</a></li>
  <li><a href="#illustrative-example-spam-filtering" id="toc-illustrative-example-spam-filtering" class="nav-link" data-scroll-target="#illustrative-example-spam-filtering"><span class="header-section-number">3.5.2</span> Illustrative Example: Spam Filtering</a></li>
  <li><a href="#example-credit-risk-assessment" id="toc-example-credit-risk-assessment" class="nav-link" data-scroll-target="#example-credit-risk-assessment"><span class="header-section-number">3.5.3</span> Example: Credit Risk Assessment</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../parts/introduction-to-bayesian-thinking/intro.html">Introduction to Bayesian Thinking</a></li><li class="breadcrumb-item"><a href="../../parts/introduction-to-bayesian-thinking/bayes-theorem-fundamentals.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem Fundamentals</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem Fundamentals</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="bayes-life-and-work" class="level3" data-number="3.0.1">
<h3 data-number="3.0.1" class="anchored" data-anchor-id="bayes-life-and-work"><span class="header-section-number">3.0.1</span> Bayes’ Life and Work</h3>
<p>Thomas Bayes (c.&nbsp;1701 – 1761) was an English statistician, philosopher, and Presbyterian minister. His life, while documented to a degree, remains relatively obscure compared to many other influential figures in mathematics. He was born in London and educated privately, displaying a clear aptitude for mathematics. While his most famous work, “An Essay towards solving a Problem in the Doctrine of Chances,” wasn’t published until after his death (1763, by Richard Price), his contributions to probability theory proved to be foundational for later developments in statistics. Bayes’ essay tackled the problem of inverse probability—essentially, how to update our beliefs about an event based on new evidence. His theorem, which arose from his attempt to solve this problem, provides a rigorous framework for such updates. His work predates many of the mathematical tools we use today, making his achievement all the more remarkable. He also made contributions to other areas of mathematics, although these remain lesser-known.</p>
</section>
<section id="early-applications-of-bayes-theorem" class="level3" data-number="3.0.2">
<h3 data-number="3.0.2" class="anchored" data-anchor-id="early-applications-of-bayes-theorem"><span class="header-section-number">3.0.2</span> Early Applications of Bayes’ Theorem</h3>
<p>Despite being published posthumously, Bayes’ theorem didn’t immediately gain widespread recognition. Its initial applications were limited and largely confined to specific problems within probability theory itself. However, it gradually found use in a variety of contexts throughout the 19th and early 20th centuries. Some key early applications include:</p>
<ul>
<li><p><strong>Astronomy:</strong> Early astronomers used Bayesian methods (though perhaps not explicitly labeled as such) to refine estimates of celestial positions and distances based on observational data. The inherent uncertainty in measurements lent itself naturally to the Bayesian framework.</p></li>
<li><p><strong>Medical Diagnosis:</strong> Even in the absence of powerful computational tools, rudimentary Bayesian reasoning was employed to assess the probability of a disease given certain symptoms. This involved subjective estimations of prior probabilities and likelihoods, which became more refined as more data became available.</p></li>
</ul>
</section>
<section id="the-development-of-bayesian-statistics" class="level3" data-number="3.0.3">
<h3 data-number="3.0.3" class="anchored" data-anchor-id="the-development-of-bayesian-statistics"><span class="header-section-number">3.0.3</span> The Development of Bayesian Statistics</h3>
<p>The development of Bayesian statistics wasn’t a linear progression. For a considerable period, the frequentist approach to statistics dominated, largely due to the computational difficulties associated with applying Bayes’ theorem. Calculating posterior probabilities often involved complex integrals, which were computationally intractable for many problems. However, many key developments propelled Bayesian statistics forward:</p>
<ul>
<li><p><strong>Laplace’s Contributions:</strong> Pierre-Simon Laplace independently derived Bayes’ theorem and applied it extensively to various problems, including celestial mechanics. His work significantly broadened the theorem’s reach and helped solidify its place in probability theory.</p></li>
<li><p><strong>The Advent of Computers:</strong> The advent of digital computers revolutionized Bayesian statistics. Previously intractable calculations became feasible, allowing for the application of Bayesian methods to increasingly complex problems. Computational techniques such as Markov Chain Monte Carlo (MCMC) methods became instrumental in sampling from posterior distributions.</p></li>
<li><p><strong>Subjective Probability and Prior Elicitation:</strong> The incorporation of subjective prior probabilities became a central aspect of Bayesian statistics. Methods for eliciting and representing these priors, and dealing with situations where there is limited data available, were developed and refined. This allowed Bayesian methods to be applied in scenarios where classical frequentist methods struggled.</p></li>
</ul>
<p>Let’s illustrate a simple Bayesian update using Python:</p>
<div id="06d62ece" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior probability of event A</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>prior_prob_A <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood of observing evidence B given A is true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>likelihood_B_given_A <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood of observing evidence B given A is false</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>likelihood_B_given_not_A <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#Calculate the probability of B</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>prob_B <span class="op">=</span> (prior_prob_A <span class="op">*</span> likelihood_B_given_A) <span class="op">+</span> ((<span class="dv">1</span><span class="op">-</span>prior_prob_A) <span class="op">*</span> likelihood_B_given_not_A)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior probability of A given B using Bayes' theorem</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>posterior_prob_A_given_B <span class="op">=</span> (prior_prob_A <span class="op">*</span> likelihood_B_given_A) <span class="op">/</span> prob_B</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prior probability of A: </span><span class="sc">{</span>prior_prob_A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Likelihood of B given A: </span><span class="sc">{</span>likelihood_B_given_A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Likelihood of B given not A: </span><span class="sc">{</span>likelihood_B_given_not_A<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior probability of A given B: </span><span class="sc">{</span>posterior_prob_A_given_B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">#Visualizing the update</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> <span class="st">'Prior A'</span>, <span class="st">'Posterior A'</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> [prior_prob_A, posterior_prob_A_given_B]</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>explode <span class="op">=</span> (<span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>fig1, ax1 <span class="op">=</span> plt.subplots()</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax1.pie(sizes, explode<span class="op">=</span>explode, labels<span class="op">=</span>labels, autopct<span class="op">=</span><span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        shadow<span class="op">=</span><span class="va">True</span>, startangle<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax1.axis(<span class="st">'equal'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Prior vs Posterior Probability of A'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Prior probability of A: 0.2
Likelihood of B given A: 0.8
Likelihood of B given not A: 0.3
Posterior probability of A given B: 0.4000000000000001</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayes-theorem-fundamentals_files/figure-html/cell-2-output-2.png" width="540" height="409" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code demonstrates a basic Bayesian update. The mathematical formulation is:</p>
<p><span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(P(A|B)\)</span> is the posterior probability of A given B.</li>
<li><span class="math inline">\(P(B|A)\)</span> is the likelihood of B given A.</li>
<li><span class="math inline">\(P(A)\)</span> is the prior probability of A.</li>
<li><span class="math inline">\(P(B)\)</span> is the marginal likelihood of B (calculated using the law of total probability).</li>
</ul>
<p>The chart visually represents the shift from prior to posterior probability. Modern Bayesian statistics relies heavily on complex computational methods and continues to evolve, finding applications in fields ranging from machine learning and medical diagnosis to finance and climate modeling.</p>
</section>
<section id="the-theorem-and-its-components" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-theorem-and-its-components"><span class="header-section-number">3.1</span> The Theorem and its Components</h2>
<section id="introducing-bayes-theorem" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="introducing-bayes-theorem"><span class="header-section-number">3.1.1</span> Introducing Bayes’ Theorem</h3>
<p>Bayes’ theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis based on new evidence. It provides a mathematical framework for revising our beliefs in light of new information. Instead of simply accepting new data at face value, Bayes’ theorem allows us to combine this data with our pre-existing beliefs (prior knowledge) to arrive at a more refined understanding (posterior knowledge). This iterative process of updating beliefs is essential in many real-world applications. The core idea is that our confidence in a hypothesis should change as we gather more relevant data.</p>
</section>
<section id="understanding-conditional-probability" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="understanding-conditional-probability"><span class="header-section-number">3.1.2</span> Understanding Conditional Probability</h3>
<p>Before diving into Bayes’ theorem itself, it’s essential to grasp the concept of conditional probability. Conditional probability refers to the probability of an event occurring given that another event has already occurred. We denote the conditional probability of event <em>A</em> occurring given that event <em>B</em> has occurred as <span class="math inline">\(P(A|B)\)</span>. This is read as “the probability of A given B”.</p>
<p>For example, consider the probability of rain (<span class="math inline">\(A\)</span>) given that it’s cloudy (<span class="math inline">\(B\)</span>). This is different from the overall probability of rain, as knowing it’s cloudy increases the likelihood of rain. Conditional probability is formally defined as:</p>
<p><span class="math inline">\(P(A|B) = \frac{P(A \cap B)}{P(B)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(P(A|B)\)</span> is the conditional probability of A given B.</li>
<li><span class="math inline">\(P(A \cap B)\)</span> is the probability of both A and B occurring (joint probability).</li>
<li><span class="math inline">\(P(B)\)</span> is the probability of B occurring.</li>
</ul>
<p>Note that <span class="math inline">\(P(B)\)</span> must be greater than 0; we cannot condition on an event that has a zero probability of occurring.</p>
</section>
<section id="the-formula-a-detailed-breakdown" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="the-formula-a-detailed-breakdown"><span class="header-section-number">3.1.3</span> The Formula: A Detailed Breakdown</h3>
<p>Bayes’ theorem provides a way to calculate the conditional probability <span class="math inline">\(P(A|B)\)</span> when we know the conditional probability <span class="math inline">\(P(B|A)\)</span>, along with the prior probabilities <span class="math inline">\(P(A)\)</span> and <span class="math inline">\(P(B)\)</span>. The theorem states:</p>
<p><span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span></p>
<p>Let’s break down each component:</p>
<ul>
<li><p><strong><span class="math inline">\(P(A|B)\)</span> (Posterior Probability):</strong> This is the probability of event A occurring <em>given that</em> event B has occurred. This is what we want to calculate using Bayes’ theorem. It represents our updated belief about A after considering the evidence B.</p></li>
<li><p><strong><span class="math inline">\(P(B|A)\)</span> (Likelihood):</strong> This is the probability of event B occurring <em>given that</em> event A has occurred. This represents how likely the evidence B is if the hypothesis A is true.</p></li>
<li><p><strong><span class="math inline">\(P(A)\)</span> (Prior Probability):</strong> This is the initial probability of event A occurring <em>before</em> considering any new evidence. This reflects our pre-existing beliefs or knowledge about A.</p></li>
<li><p><strong><span class="math inline">\(P(B)\)</span> (Evidence Probability or Marginal Likelihood):</strong> This is the probability of event B occurring, regardless of whether A is true or false. It can be calculated using the law of total probability:</p>
<p><span class="math inline">\(P(B) = P(B|A)P(A) + P(B|\neg A)P(\neg A)\)</span></p>
<p>where <span class="math inline">\(\neg A\)</span> denotes the complement of A (A not occurring).</p></li>
</ul>
</section>
<section id="visualizing-bayes-theorem" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="visualizing-bayes-theorem"><span class="header-section-number">3.1.4</span> Visualizing Bayes’ Theorem</h3>
<p>A simple example helps illustrate Bayes’ theorem visually. Let’s say we’re testing for a rare disease.</p>
<div id="dd7bd425" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior probabilities</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>prior_disease <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># 1% prevalence of the disease</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>prior_no_disease <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihoods (test accuracy)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>likelihood_positive_given_disease <span class="op">=</span> <span class="fl">0.95</span>  <span class="co"># Test is 95% accurate when disease is present</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>likelihood_positive_given_no_disease <span class="op">=</span> <span class="fl">0.05</span> <span class="co"># Test has a 5% false positive rate</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the probability of a positive test result</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>prob_positive <span class="op">=</span> (likelihood_positive_given_disease <span class="op">*</span> prior_disease) <span class="op">+</span> (likelihood_positive_given_no_disease <span class="op">*</span> prior_no_disease)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Bayes' Theorem to calculate the posterior probability of having the disease given a positive test</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>posterior_disease_given_positive <span class="op">=</span> (likelihood_positive_given_disease <span class="op">*</span> prior_disease) <span class="op">/</span> prob_positive</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Probability of having the disease given a positive test: </span><span class="sc">{</span>posterior_disease_given_positive<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> <span class="st">'Prior Disease'</span>, <span class="st">'Posterior Disease given Positive Test'</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> [prior_disease, posterior_disease_given_positive]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>fig1, ax1 <span class="op">=</span> plt.subplots()</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>ax1.pie(sizes, labels<span class="op">=</span>labels, autopct<span class="op">=</span><span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>, shadow<span class="op">=</span><span class="va">True</span>, startangle<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>ax1.axis(<span class="st">'equal'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Prior vs Posterior Probability of Disease'</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability of having the disease given a positive test: 0.1610</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayes-theorem-fundamentals_files/figure-html/cell-3-output-2.png" width="556" height="412" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This Python code calculates and visualizes the Bayesian update for the disease example. The pie chart clearly shows the shift in probability from the prior to the posterior. Even with a seemingly accurate test, the rarity of the disease means the posterior probability of having the disease, given a positive test, is still relatively low.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Prior Probability P(A)] --&gt; B{Evidence B};
    C[Likelihood P(B|A)] --&gt; B;
    B --&gt; D[Posterior Probability P(A|B)];
    E[Prior Probability P(¬A)] --&gt; B;
    F[Likelihood P(B|¬A)] --&gt; B;
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This Mermaid diagram shows the flow of information in Bayes’ theorem, highlighting the relationship between the prior, likelihood, and posterior probabilities. This visual representation aids in understanding how the evidence updates our prior belief.</p>
</section>
</section>
<section id="prior-probability" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="prior-probability"><span class="header-section-number">3.2</span> Prior Probability</h2>
<section id="defining-prior-probability" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="defining-prior-probability"><span class="header-section-number">3.2.1</span> Defining Prior Probability</h3>
<p>In Bayesian statistics, the prior probability, denoted as <span class="math inline">\(P(A)\)</span>, represents the initial belief or knowledge about the probability of an event A occurring <em>before</em> observing any new data or evidence. It’s a subjective assessment reflecting our understanding of the event based on existing information, previous experience, or expert opinion. The choice of prior is a essential step in Bayesian analysis, as it significantly influences the resulting posterior probability. A well-chosen prior incorporates relevant information effectively, while a poorly chosen one can lead to inaccurate or misleading conclusions.</p>
</section>
<section id="types-of-priors-informative-non-informative" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="types-of-priors-informative-non-informative"><span class="header-section-number">3.2.2</span> Types of Priors (Informative, Non-Informative)</h3>
<p>Priors can be broadly categorized into two types:</p>
<ul>
<li><p><strong>Informative Priors:</strong> These priors reflect substantial prior knowledge about the event. They are often based on previous studies, expert judgment, or theoretical considerations. An informative prior “informs” the analysis by incorporating existing knowledge into the Bayesian update. For example, if we’re estimating the success rate of a new drug, an informative prior might be based on the success rates of similar drugs in clinical trials.</p></li>
<li><p><strong>Non-informative (or Weakly Informative) Priors:</strong> These priors represent minimal or vague prior knowledge. They aim to let the data speak for itself, minimizing the influence of prior beliefs. A non-informative prior is often used when there is limited or no prior information available. However, it is important to note that truly “non-informative” priors are difficult to define and often have some implicit assumptions. Examples include uniform priors (assigning equal probability to all possible values) or Jeffreys priors (which are invariant under reparameterization).</p></li>
</ul>
</section>
<section id="choosing-an-appropriate-prior" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="choosing-an-appropriate-prior"><span class="header-section-number">3.2.3</span> Choosing an Appropriate Prior</h3>
<p>Choosing an appropriate prior is a essential step and often involves a degree of subjectivity. Several factors influence the choice:</p>
<ul>
<li><p><strong>Available Prior Knowledge:</strong> The most significant factor is the amount and quality of prior knowledge. If substantial relevant information exists, an informative prior is appropriate. With little prior knowledge, a non-informative prior is often preferred.</p></li>
<li><p><strong>Computational Considerations:</strong> Some priors might lead to computationally challenging calculations. The choice of prior should be balanced against the computational resources available.</p></li>
<li><p><strong>Sensitivity Analysis:</strong> It’s good practice to perform sensitivity analysis to check how sensitive the posterior probability is to different choices of priors. If the posterior is relatively insensitive to changes in the prior, it suggests that the data strongly outweighs the prior belief.</p></li>
</ul>
</section>
<section id="impact-of-prior-on-posterior" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="impact-of-prior-on-posterior"><span class="header-section-number">3.2.4</span> Impact of Prior on Posterior</h3>
<p>The prior significantly impacts the posterior probability. Let’s illustrate this with a Python example:</p>
<div id="e7022f96" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bayes_update(prior, likelihood, evidence):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Performs a Bayesian update."""</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> (likelihood <span class="op">*</span> prior) <span class="op">/</span> evidence</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> posterior</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood of observing data given hypothesis</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Different priors</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>prior1 <span class="op">=</span> <span class="fl">0.2</span>  <span class="co">#Weak prior</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>prior2 <span class="op">=</span> <span class="fl">0.8</span> <span class="co">#Strong prior</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">#Evidence Probability (Marginal Likelihood)  - Needs to be calculated based on the priors and likelihoods.  Simplified here for illustration purposes.</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>evidence <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">#Bayesian Updates</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>posterior1 <span class="op">=</span> bayes_update(prior1, likelihood, evidence)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>posterior2 <span class="op">=</span> bayes_update(prior2, likelihood, evidence)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prior 1: </span><span class="sc">{</span>prior1<span class="sc">:.2f}</span><span class="ss">, Posterior 1: </span><span class="sc">{</span>posterior1<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prior 2: </span><span class="sc">{</span>prior2<span class="sc">:.2f}</span><span class="ss">, Posterior 2: </span><span class="sc">{</span>posterior2<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co">#Visualization</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'Prior 1'</span>, <span class="st">'Posterior 1'</span>, <span class="st">'Prior 2'</span>, <span class="st">'Posterior 2'</span>]</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> [prior1, posterior1, prior2, posterior2]</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>ax.pie(sizes, labels<span class="op">=</span>labels, autopct<span class="op">=</span><span class="st">'</span><span class="sc">%1.1f%%</span><span class="st">'</span>, startangle<span class="op">=</span><span class="dv">140</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Impact of Prior on Posterior'</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Prior 1: 0.20, Posterior 1: 0.28
Prior 2: 0.80, Posterior 2: 1.12</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayes-theorem-fundamentals_files/figure-html/cell-4-output-2.png" width="540" height="415" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This Python code demonstrates how different priors, even when combined with the same likelihood and evidence, result in different posterior probabilities. The chart visualizes this difference, showing that the prior strongly influences the final conclusion. The mathematical relationship is again:</p>
<p><span class="math inline">\(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)</span></p>
<p>A strong prior can significantly pull the posterior towards its value, especially when the data provides weak or ambiguous evidence. This highlights the importance of carefully considering and justifying the choice of prior.</p>
</section>
</section>
<section id="likelihood" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="likelihood"><span class="header-section-number">3.3</span> Likelihood</h2>
<section id="defining-likelihood" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="defining-likelihood"><span class="header-section-number">3.3.1</span> Defining Likelihood</h3>
<p>In Bayesian statistics, the likelihood function quantifies the probability of observing the available data given a specific hypothesis or parameter value. Unlike a probability, the likelihood is <em>not</em> a probability distribution over the parameters. Instead, it’s a function of the parameters, showing how likely the observed data is for different parameter values. It essentially measures how well a given parameter value explains the observed data. The higher the likelihood, the better the parameter value fits the data.</p>
</section>
<section id="likelihood-function" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="likelihood-function"><span class="header-section-number">3.3.2</span> Likelihood Function</h3>
<p>The likelihood function is usually denoted as <span class="math inline">\(L(\theta|x)\)</span>, where:</p>
<ul>
<li><p><span class="math inline">\(\theta\)</span> represents the parameter(s) of interest (e.g., the mean of a normal distribution, the probability of success in a binomial distribution). This is what we want to estimate.</p></li>
<li><p><span class="math inline">\(x\)</span> represents the observed data.</p></li>
</ul>
<p>The likelihood function is directly related to the probability density function (PDF) or probability mass function (PMF) of the data, but its interpretation is different. For example, if the data <span class="math inline">\(x\)</span> follows a normal distribution with mean <span class="math inline">\(\theta\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>, the likelihood function is given by:</p>
<p><span class="math inline">\(L(\theta|x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\theta)^2}{2\sigma^2}\right)\)</span></p>
<p>Note that this is the same formula as the PDF of a normal distribution, but here we treat it as a function of <span class="math inline">\(\theta\)</span> (the parameter we are trying to estimate), with the data <em>x</em> being fixed.</p>
</section>
<section id="interpreting-likelihood-values" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="interpreting-likelihood-values"><span class="header-section-number">3.3.3</span> Interpreting Likelihood Values</h3>
<p>Likelihood values themselves are not probabilities. They don’t need to sum or integrate to 1. We’re interested in <em>relative</em> likelihoods:</p>
<ul>
<li><p>A higher likelihood value indicates that the given parameter value is more consistent with the observed data.</p></li>
<li><p>A lower likelihood value indicates that the given parameter value is less consistent with the observed data.</p></li>
</ul>
<p>We often work with the log-likelihood, <span class="math inline">\(\log L(\theta|x)\)</span>, which simplifies calculations and makes it easier to compare likelihoods.</p>
</section>
<section id="relationship-between-likelihood-and-prior" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="relationship-between-likelihood-and-prior"><span class="header-section-number">3.3.4</span> Relationship between Likelihood and Prior</h3>
<p>The likelihood and prior probability are the two key ingredients in Bayes’ theorem. They combine to produce the posterior probability:</p>
<p><span class="math inline">\(P(\theta|x) \propto L(\theta|x)P(\theta)\)</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(P(\theta|x)\)</span> is the posterior probability distribution of the parameter <span class="math inline">\(\theta\)</span> given the data <span class="math inline">\(x\)</span>.</p></li>
<li><p><span class="math inline">\(L(\theta|x)\)</span> is the likelihood function.</p></li>
<li><p><span class="math inline">\(P(\theta)\)</span> is the prior probability distribution of the parameter <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
<p>The symbol <span class="math inline">\(\propto\)</span> indicates proportionality—the posterior is proportional to the product of the likelihood and prior. The constant of proportionality is determined by ensuring the posterior integrates to 1 (for continuous parameters) or sums to 1 (for discrete parameters).</p>
<p>Let’s visualize the relationship between likelihood and prior:</p>
<div id="0194d293" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">5</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Prior distribution</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>prior_mean <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>prior_std <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> norm(loc<span class="op">=</span>prior_mean, scale<span class="op">=</span>prior_std)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood function (assuming known variance)</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>likelihood_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>likelihood_std <span class="op">=</span> <span class="dv">1</span> <span class="co">#Assume known standard deviation</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> norm(loc<span class="op">=</span>likelihood_mean, scale<span class="op">=</span>likelihood_std)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">#Range for plotting</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>theta_range <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">#Calculate Likelihood and Prior</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>prior_probs <span class="op">=</span> prior.pdf(theta_range)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>likelihood_probs <span class="op">=</span> likelihood.pdf(theta_range)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_range, prior_probs, label<span class="op">=</span><span class="st">'Prior'</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>plt.plot(theta_range, likelihood_probs, label<span class="op">=</span><span class="st">'Likelihood'</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Theta'</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Prior and Likelihood'</span>)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayes-theorem-fundamentals_files/figure-html/cell-5-output-1.png" width="597" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code generates a plot showing a hypothetical prior and likelihood. The posterior (not plotted here for simplicity) would be proportional to the product of these two curves. The posterior would represent a compromise between the prior belief and the information provided by the data. Observe that the prior influences the shape and location of the posterior distribution. In essence, the prior represents our prior belief, while the likelihood incorporates the information from the observed data. The posterior distribution is their combination.</p>
</section>
</section>
<section id="posterior-probability" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="posterior-probability"><span class="header-section-number">3.4</span> Posterior Probability</h2>
<section id="defining-posterior-probability" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="defining-posterior-probability"><span class="header-section-number">3.4.1</span> Defining Posterior Probability</h3>
<p>In Bayesian statistics, the posterior probability, denoted as <span class="math inline">\(P(\theta|x)\)</span>, represents the updated probability distribution of a parameter <span class="math inline">\(\theta\)</span> after observing data <span class="math inline">\(x\)</span>. It combines prior knowledge about the parameter (prior probability, <span class="math inline">\(P(\theta)\)</span>) with the information obtained from the data (likelihood, <span class="math inline">\(L(\theta|x)\)</span>). The posterior distribution is the central result of a Bayesian analysis, summarizing our updated beliefs about the parameter after incorporating the evidence. It’s essential to understand that the posterior is not just a single value, but a probability distribution, reflecting the uncertainty that remains about the parameter even after observing the data.</p>
</section>
<section id="interpreting-posterior-distributions" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="interpreting-posterior-distributions"><span class="header-section-number">3.4.2</span> Interpreting Posterior Distributions</h3>
<p>The posterior distribution is interpreted as a probability distribution over the possible values of the parameter <span class="math inline">\(\theta\)</span>. Different aspects of the posterior distribution offer valuable insights:</p>
<ul>
<li><p><strong>Mean/Median/Mode:</strong> These summary statistics provide point estimates for the parameter. The mean gives the average value of <span class="math inline">\(\theta\)</span> according to the posterior distribution, the median provides the value that splits the posterior distribution in half, and the mode represents the most likely value of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>Credible Intervals:</strong> These intervals provide a range of plausible values for the parameter with a specified probability. For example, a 95% credible interval contains the values of <span class="math inline">\(\theta\)</span> that have a 95% probability of including the true value, given the observed data.</p></li>
<li><p><strong>Shape:</strong> The shape of the posterior distribution reveals information about the uncertainty and potential multimodality of the parameter. A narrow, peaked distribution indicates high certainty about the parameter value, while a wide, flat distribution implies significant uncertainty.</p></li>
</ul>
</section>
<section id="updating-beliefs-with-data" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="updating-beliefs-with-data"><span class="header-section-number">3.4.3</span> Updating Beliefs with Data</h3>
<p>The Bayesian approach elegantly reflects the updating of beliefs with new data. The prior distribution captures our initial understanding, while the likelihood function incorporates the information from the observed data. Bayes’ theorem formalizes this update:</p>
<p><span class="math inline">\(P(\theta|x) = \frac{L(\theta|x)P(\theta)}{P(x)}\)</span></p>
<p>where <span class="math inline">\(P(x)\)</span>, the marginal likelihood (evidence), acts as a normalizing constant, ensuring that the posterior integrates (or sums) to 1. This equation shows how the posterior is proportional to the product of the likelihood and the prior. The evidence plays a essential role, determining the scale of the posterior distribution.</p>
</section>
<section id="posterior-predictive-distribution" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="posterior-predictive-distribution"><span class="header-section-number">3.4.4</span> Posterior Predictive Distribution</h3>
<p>The posterior predictive distribution, <span class="math inline">\(P(\tilde{x}|x)\)</span>, provides a probability distribution for future observations, <span class="math inline">\(\tilde{x}\)</span>, given the observed data, <span class="math inline">\(x\)</span>. It incorporates uncertainty about the model parameters by averaging over the posterior distribution. This is formally expressed as:</p>
<p><span class="math inline">\(P(\tilde{x}|x) = \int P(\tilde{x}|\theta)P(\theta|x) d\theta\)</span></p>
<p>This integral averages the predictive distribution of future observations (<span class="math inline">\(P(\tilde{x}|\theta)\)</span>) across all possible parameter values (<span class="math inline">\(\theta\)</span>), weighted by their posterior probabilities (<span class="math inline">\(P(\theta|x)\)</span>). This approach offers a more realistic and detailed prediction that accounts for the uncertainty in the estimated parameters.</p>
<p>Let’s illustrate with a Python example using simulated data:</p>
<div id="79706672" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm, beta</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate data from a normal distribution</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>true_mean <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>true_std <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(loc<span class="op">=</span>true_mean, scale<span class="op">=</span>true_std, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior distribution (Normal)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>prior_mean <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>prior_std <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> norm(loc<span class="op">=</span>prior_mean, scale<span class="op">=</span>prior_std)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior (using conjugate prior for simplicity)</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>posterior_mean <span class="op">=</span> np.mean(data)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>posterior_std <span class="op">=</span> true_std<span class="op">/</span>np.sqrt(<span class="bu">len</span>(data)) <span class="co">#approximation for large sample</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> norm(loc<span class="op">=</span>posterior_mean, scale<span class="op">=</span>posterior_std)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples from posterior predictive distribution</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>posterior_samples <span class="op">=</span> posterior.rvs(size<span class="op">=</span>num_samples)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>predictive_samples <span class="op">=</span> np.random.normal(loc<span class="op">=</span>posterior_samples, scale<span class="op">=</span>true_std, size<span class="op">=</span>num_samples)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the posterior predictive distribution</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.hist(predictive_samples, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Posterior Predictive'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Future Observation'</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Posterior Predictive Distribution'</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bayes-theorem-fundamentals_files/figure-html/cell-6-output-1.png" width="597" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code simulates data, defines a prior, calculates a simplified posterior (assuming known variance for convenience), and then generates samples from the posterior predictive distribution. The histogram shows the distribution of predicted future observations, reflecting the uncertainty inherent in the Bayesian approach. Note that a more realistic scenario would involve a full Bayesian update without assuming known variance, requiring more complex computational methods.</p>
</section>
</section>
<section id="practical-examples-and-applications" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="practical-examples-and-applications"><span class="header-section-number">3.5</span> Practical Examples and Applications</h2>
<section id="simple-example-medical-diagnosis" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="simple-example-medical-diagnosis"><span class="header-section-number">3.5.1</span> Simple Example: Medical Diagnosis</h3>
<p>Bayes’ theorem is exceptionally useful in medical diagnosis. Consider a test for a rare disease:</p>
<ul>
<li>Let D represent the event that a patient has the disease.</li>
<li>Let T represent the event that the test is positive.</li>
</ul>
<p>We are interested in finding <span class="math inline">\(P(D|T)\)</span>, the probability that a patient has the disease given a positive test result. We need the following information:</p>
<ul>
<li><p><strong>Prior probability of disease:</strong> <span class="math inline">\(P(D)\)</span> – The prevalence of the disease in the population. This might be a low value for a rare disease.</p></li>
<li><p><strong>Sensitivity:</strong> <span class="math inline">\(P(T|D)\)</span> – The probability of a positive test given the disease is present (true positive rate).</p></li>
<li><p><strong>Specificity:</strong> <span class="math inline">\(P(\neg T|\neg D)\)</span> – The probability of a negative test given the disease is absent (true negative rate). This is equal to <span class="math inline">\(1 - P(T|\neg D)\)</span>.</p></li>
<li><p><strong>Positive Predictive Value:</strong> This is what we ultimately want to calculate: <span class="math inline">\(P(D|T)\)</span>.</p></li>
</ul>
<p>Bayes’ theorem allows us to calculate the positive predictive value (PPV):</p>
<p><span class="math inline">\(P(D|T) = \frac{P(T|D)P(D)}{P(T)}\)</span></p>
<p>Where <span class="math inline">\(P(T)\)</span> (the probability of a positive test) is calculated using the law of total probability:</p>
<p><span class="math inline">\(P(T) = P(T|D)P(D) + P(T|\neg D)P(\neg D)\)</span></p>
<p>Let’s assume the following values:</p>
<ul>
<li><span class="math inline">\(P(D) = 0.01\)</span> (1% prevalence)</li>
<li><span class="math inline">\(P(T|D) = 0.95\)</span> (95% sensitivity)</li>
<li><span class="math inline">\(P(T|\neg D) = 0.05\)</span> (5% false positive rate)</li>
</ul>
<p>Using this information, we can calculate <span class="math inline">\(P(D|T)\)</span>. This will show that even with a highly sensitive test, the probability of actually having the disease given a positive test might still be surprisingly low due to the low prior probability of the disease. This emphasizes the importance of considering both the test results and the base rate of the condition. We leave the explicit calculation as an exercise for the reader, encouraging them to perform it using Python code similar to the examples in previous sections.</p>
</section>
<section id="illustrative-example-spam-filtering" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="illustrative-example-spam-filtering"><span class="header-section-number">3.5.2</span> Illustrative Example: Spam Filtering</h3>
<p>Spam filtering utilizes Bayesian techniques to classify emails as spam or not spam. The process involves:</p>
<ol type="1">
<li><p><strong>Training:</strong> A model is trained on a dataset of labeled emails (spam and not spam), learning the association between words and the email class.</p></li>
<li><p><strong>Prior Probabilities:</strong> The model establishes prior probabilities for spam and not spam emails based on the training data.</p></li>
<li><p><strong>Likelihoods:</strong> For each word in a new email, the model calculates the likelihood of that word appearing in spam emails and in non-spam emails.</p></li>
<li><p><strong>Bayesian Update:</strong> Bayes’ theorem combines the prior probabilities and the likelihoods of words in the new email to calculate the posterior probability that the email is spam.</p></li>
<li><p><strong>Classification:</strong> If the posterior probability of spam exceeds a certain threshold, the email is classified as spam. Otherwise, it’s considered not spam.</p></li>
</ol>
<p>The calculation is similar to the medical diagnosis example but extended to multiple features (words). The simplification using the Naive Bayes assumption (assuming independence between words) makes it computationally efficient. The implementation using Python would involve techniques such as creating frequency tables of words in spam and non-spam emails. This is a common application of Bayes’ theorem and forms the basis of many robust spam filters.</p>
</section>
<section id="example-credit-risk-assessment" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="example-credit-risk-assessment"><span class="header-section-number">3.5.3</span> Example: Credit Risk Assessment</h3>
<p>Credit risk assessment uses Bayesian methods to predict the probability of a borrower defaulting on a loan.</p>
<ul>
<li><p><strong>Prior Information:</strong> Prior information might include the borrower’s credit history, income level, debt-to-income ratio, and other relevant financial data. These factors can inform the prior probability of default (<span class="math inline">\(P(\text{Default})\)</span>).</p></li>
<li><p><strong>New Data:</strong> New data may include the specific characteristics of the loan application, such as loan amount, interest rate, and loan term. This informs the likelihood of default given certain loan characteristics (<span class="math inline">\(P(\text{Data}|\text{Default})\)</span>).</p></li>
<li><p><strong>Bayesian Update:</strong> Bayes’ theorem combines prior information with new data to compute the posterior probability of default, providing a more refined risk assessment. This can be implemented using various Bayesian models, such as logistic regression models with appropriate priors.</p></li>
<li><p><strong>Decision:</strong> The lender uses the posterior probability to decide whether to approve or reject the loan application, balancing the potential profit against the risk of default.</p></li>
</ul>
<p>Python implementations often employ Bayesian logistic regression or other Bayesian models to handle the probabilistic nature of the problem. Libraries like PyMC or Stan would be used to perform the Bayesian inference, providing the posterior distribution of the probability of default. Visualization would then typically involve plots of the posterior distribution and credible intervals to quantify the uncertainty of the prediction. This application highlights the power of Bayesian methods in handling complex real-world scenarios where uncertainty is significant.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../parts/introduction-to-bayesian-thinking/understanding-probability.html" class="pagination-link" aria-label="Introduction to Probability">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../parts/introduction-to-bayesian-thinking/setting-up-python-environment.html" class="pagination-link" aria-label="Setting Up Python Environment">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Setting Up Python Environment</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>