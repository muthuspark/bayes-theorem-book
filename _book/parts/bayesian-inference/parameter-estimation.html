<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Introduction to Parameter Estimation – bayes-theorem-book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../parts/bayesian-inference/conjugate-priors.html" rel="next">
<link href="../../parts/bayesian-inference/intro.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a2a08d6480f1a07d2e84f5b3bded3372.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/intro.html">Bayesian Inference</a></li><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/parameter-estimation.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Parameter Estimation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">bayes-theorem-book</a> 
        <div class="sidebar-tools-main">
    <a href="../../bayes-theorem-book.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/introduction-to-bayesian-thinking/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Bayesian Thinking</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/understanding-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/bayes-theorem-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem Fundamentals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/setting-up-python-environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Setting Up Python Environment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/mathematical-foundations/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/probability-theory-essentials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/statistical-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Basic Probability Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/linear-algebra-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/implementing-bayes-theorem/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Implementing Bayes' Theorem</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/basic-implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Basic Implementation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/working-with-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/discrete-probability-examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Discrete Probability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/bayesian-inference/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/parameter-estimation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/conjugate-priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Conjugate Priors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/prior-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Prior Selection</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/markov-chain-monte-carlo-mcmc/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Markov Chain Monte Carlo (MCMC)</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/introduction-to-mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Monte Carlo Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/mcmc-algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Introduction to MCMC</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/implementation-with-pymc3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Implementation with PyMC</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/practical-applications/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/ab-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/text-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Introduction to Text Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/medical-diagnosis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Disease Testing with Bayes’ Theorem</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/advanced-topics/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/hierarchical-bayesian-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Hierarchical Bayesian Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/bayesian-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/gaussian-processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/real-world-applications/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Real-World Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/finance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Finance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/marketing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Bayesian Methods in Customer Segmentation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/scientific-applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Scientific Applications</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/best-practices-and-advanced-tools/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Best Practices and Advanced Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/code-organization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Code Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/performance-optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Understanding Performance Bottlenecks in Bayesian Computations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/modern-bayesian-libraries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Modern Bayesian Libraries</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#frequentist-vs.-bayesian-approaches" id="toc-frequentist-vs.-bayesian-approaches" class="nav-link active" data-scroll-target="#frequentist-vs.-bayesian-approaches"><span class="header-section-number">11.0.1</span> Frequentist vs.&nbsp;Bayesian Approaches</a></li>
  <li><a href="#the-role-of-bayes-theorem" id="toc-the-role-of-bayes-theorem" class="nav-link" data-scroll-target="#the-role-of-bayes-theorem"><span class="header-section-number">11.0.2</span> The Role of Bayes’ Theorem</a></li>
  <li><a href="#prior-and-posterior-distributions" id="toc-prior-and-posterior-distributions" class="nav-link" data-scroll-target="#prior-and-posterior-distributions"><span class="header-section-number">11.0.3</span> Prior and Posterior Distributions</a></li>
  <li><a href="#point-estimation" id="toc-point-estimation" class="nav-link" data-scroll-target="#point-estimation"><span class="header-section-number">11.1</span> Point Estimation</a>
  <ul class="collapse">
  <li><a href="#maximum-likelihood-estimation-mle" id="toc-maximum-likelihood-estimation-mle" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-mle"><span class="header-section-number">11.1.1</span> Maximum Likelihood Estimation (MLE)</a></li>
  <li><a href="#maximum-a-posteriori-map-estimation" id="toc-maximum-a-posteriori-map-estimation" class="nav-link" data-scroll-target="#maximum-a-posteriori-map-estimation"><span class="header-section-number">11.1.2</span> Maximum A Posteriori (MAP) Estimation</a></li>
  <li><a href="#comparing-mle-and-map" id="toc-comparing-mle-and-map" class="nav-link" data-scroll-target="#comparing-mle-and-map"><span class="header-section-number">11.1.3</span> Comparing MLE and MAP</a></li>
  <li><a href="#python-implementation-of-mle-and-map" id="toc-python-implementation-of-mle-and-map" class="nav-link" data-scroll-target="#python-implementation-of-mle-and-map"><span class="header-section-number">11.1.4</span> Python Implementation of MLE and MAP</a></li>
  </ul></li>
  <li><a href="#credible-intervals" id="toc-credible-intervals" class="nav-link" data-scroll-target="#credible-intervals"><span class="header-section-number">11.2</span> Credible Intervals</a>
  <ul class="collapse">
  <li><a href="#definition-and-interpretation" id="toc-definition-and-interpretation" class="nav-link" data-scroll-target="#definition-and-interpretation"><span class="header-section-number">11.2.1</span> Definition and Interpretation</a></li>
  <li><a href="#calculating-credible-intervals" id="toc-calculating-credible-intervals" class="nav-link" data-scroll-target="#calculating-credible-intervals"><span class="header-section-number">11.2.2</span> Calculating Credible Intervals</a></li>
  <li><a href="#equal-tailed-vs.-highest-posterior-density-hpd-intervals" id="toc-equal-tailed-vs.-highest-posterior-density-hpd-intervals" class="nav-link" data-scroll-target="#equal-tailed-vs.-highest-posterior-density-hpd-intervals"><span class="header-section-number">11.2.3</span> Equal-tailed vs.&nbsp;Highest Posterior Density (HPD) Intervals</a></li>
  <li><a href="#python-implementation-of-credible-intervals" id="toc-python-implementation-of-credible-intervals" class="nav-link" data-scroll-target="#python-implementation-of-credible-intervals"><span class="header-section-number">11.2.4</span> Python Implementation of Credible Intervals</a></li>
  <li><a href="#choosing-the-credible-interval-level" id="toc-choosing-the-credible-interval-level" class="nav-link" data-scroll-target="#choosing-the-credible-interval-level"><span class="header-section-number">11.2.5</span> Choosing the Credible Interval Level</a></li>
  </ul></li>
  <li><a href="#advanced-topics-in-parameter-estimation" id="toc-advanced-topics-in-parameter-estimation" class="nav-link" data-scroll-target="#advanced-topics-in-parameter-estimation"><span class="header-section-number">11.3</span> Advanced Topics in Parameter Estimation</a>
  <ul class="collapse">
  <li><a href="#bayesian-model-comparison" id="toc-bayesian-model-comparison" class="nav-link" data-scroll-target="#bayesian-model-comparison"><span class="header-section-number">11.3.1</span> Bayesian Model Comparison</a></li>
  <li><a href="#hierarchical-models" id="toc-hierarchical-models" class="nav-link" data-scroll-target="#hierarchical-models"><span class="header-section-number">11.3.2</span> Hierarchical Models</a></li>
  <li><a href="#dealing-with-high-dimensional-data" id="toc-dealing-with-high-dimensional-data" class="nav-link" data-scroll-target="#dealing-with-high-dimensional-data"><span class="header-section-number">11.3.3</span> Dealing with High-Dimensional Data</a></li>
  <li><a href="#computational-methods-mcmc" id="toc-computational-methods-mcmc" class="nav-link" data-scroll-target="#computational-methods-mcmc"><span class="header-section-number">11.3.4</span> Computational Methods (MCMC)</a></li>
  </ul></li>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies"><span class="header-section-number">11.4</span> Case Studies</a>
  <ul class="collapse">
  <li><a href="#example-estimating-the-mean-of-a-normal-distribution" id="toc-example-estimating-the-mean-of-a-normal-distribution" class="nav-link" data-scroll-target="#example-estimating-the-mean-of-a-normal-distribution"><span class="header-section-number">11.4.1</span> Example: Estimating the Mean of a Normal Distribution</a></li>
  <li><a href="#example-estimating-the-parameter-of-a-binomial-distribution" id="toc-example-estimating-the-parameter-of-a-binomial-distribution" class="nav-link" data-scroll-target="#example-estimating-the-parameter-of-a-binomial-distribution"><span class="header-section-number">11.4.2</span> Example: Estimating the Parameter of a Binomial Distribution</a></li>
  <li><a href="#example-bayesian-linear-regression" id="toc-example-bayesian-linear-regression" class="nav-link" data-scroll-target="#example-bayesian-linear-regression"><span class="header-section-number">11.4.3</span> Example: Bayesian Linear Regression</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/intro.html">Bayesian Inference</a></li><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/parameter-estimation.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Parameter Estimation</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Parameter Estimation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Parameter estimation is a fundamental problem in statistics, aiming to determine the values of unknown parameters in a statistical model based on observed data. This chapter explores parameter estimation through the lens of Bayes’ Theorem, contrasting it with frequentist approaches. We will learn how Bayes’ Theorem allows us to incorporate prior knowledge and update our beliefs about parameters as we gather more data.</p>
<section id="frequentist-vs.-bayesian-approaches" class="level3" data-number="11.0.1">
<h3 data-number="11.0.1" class="anchored" data-anchor-id="frequentist-vs.-bayesian-approaches"><span class="header-section-number">11.0.1</span> Frequentist vs.&nbsp;Bayesian Approaches</h3>
<p>Frequentist and Bayesian approaches to parameter estimation differ fundamentally in their interpretation of probability.</p>
<ul>
<li><p><strong>Frequentist Approach:</strong> Frequentists view probability as the long-run frequency of an event. Parameter estimation focuses on point estimates (e.g., maximum likelihood estimate) and confidence intervals, which are constructed based on the sampling distribution of the estimator. The true parameter is considered fixed, and the uncertainty is solely attributed to the variability of the data. For example, maximum likelihood estimation (MLE) seeks to find the parameter values that maximize the likelihood function, <span class="math inline">\(L(\theta|x) = P(x|\theta)\)</span>, where <span class="math inline">\(x\)</span> is the observed data and <span class="math inline">\(\theta\)</span> is the parameter.</p></li>
<li><p><strong>Bayesian Approach:</strong> Bayesians view probability as a degree of belief. The unknown parameter <span class="math inline">\(\theta\)</span> is treated as a random variable with a probability distribution. The Bayesian approach uses Bayes’ Theorem to update the prior distribution (our initial belief about <span class="math inline">\(\theta\)</span>) based on the observed data to obtain the posterior distribution. This posterior distribution represents our updated belief about <span class="math inline">\(\theta\)</span> after observing the data.</p></li>
</ul>
</section>
<section id="the-role-of-bayes-theorem" class="level3" data-number="11.0.2">
<h3 data-number="11.0.2" class="anchored" data-anchor-id="the-role-of-bayes-theorem"><span class="header-section-number">11.0.2</span> The Role of Bayes’ Theorem</h3>
<p>Bayes’ Theorem provides the mathematical framework for updating our beliefs about parameters in light of new data. The theorem states:</p>
<p><span class="math inline">\(P(\theta|x) = \frac{P(x|\theta)P(\theta)}{P(x)}\)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(P(\theta|x)\)</span> is the posterior distribution of <span class="math inline">\(\theta\)</span> given the data <span class="math inline">\(x\)</span>. This is what we want to estimate.</li>
<li><span class="math inline">\(P(x|\theta)\)</span> is the likelihood function, representing the probability of observing the data given a specific value of <span class="math inline">\(\theta\)</span>.</li>
<li><span class="math inline">\(P(\theta)\)</span> is the prior distribution of <span class="math inline">\(\theta\)</span>, representing our initial belief about the parameter before observing the data.</li>
<li><span class="math inline">\(P(x)\)</span> is the marginal likelihood (or evidence), which acts as a normalizing constant. It can often be calculated as: <span class="math inline">\(P(x) = \int P(x|\theta)P(\theta)d\theta\)</span>.</li>
</ul>
<p>In practice, we often work with the proportional relationship:</p>
<p><span class="math inline">\(P(\theta|x) \propto P(x|\theta)P(\theta)\)</span></p>
<p>This means that the posterior distribution is proportional to the product of the likelihood and the prior.</p>
</section>
<section id="prior-and-posterior-distributions" class="level3" data-number="11.0.3">
<h3 data-number="11.0.3" class="anchored" data-anchor-id="prior-and-posterior-distributions"><span class="header-section-number">11.0.3</span> Prior and Posterior Distributions</h3>
<p>The choice of prior distribution reflects our prior knowledge or beliefs about the parameter. A non-informative prior expresses a lack of strong prior knowledge, while an informative prior incorporates existing information. The posterior distribution is then obtained by combining the prior and the likelihood.</p>
<p>Let’s illustrate this with a simple example using Python. Suppose we are estimating the mean <span class="math inline">\(\mu\)</span> of a normal distribution with known variance <span class="math inline">\(\sigma^2\)</span>. We’ll assume a normal prior for <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span></p>
<p>and observe data <span class="math inline">\(x_1, x_2, ..., x_n\)</span> which are i.i.d. from <span class="math inline">\(N(\mu, \sigma^2)\)</span>. The likelihood is:</p>
<p><span class="math inline">\(P(x|\mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x_i-\mu)^2}{2\sigma^2})\)</span></p>
<div id="506cc166" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>mu_0 <span class="op">=</span> <span class="dv">0</span>  </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>sigma_0 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood parameters</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span>sigma, size<span class="op">=</span><span class="dv">10</span>) <span class="co"># Observed data, generating from a distribution with mean 2</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate posterior parameters</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>mu_n <span class="op">=</span> (mu_0 <span class="op">/</span> sigma_0<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> np.<span class="bu">sum</span>(data) <span class="op">/</span> sigma<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> sigma_0<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> n <span class="op">/</span> sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sigma_n <span class="op">=</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> sigma_0<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> n <span class="op">/</span> sigma<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot prior and posterior</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x, mu_0, sigma_0), label<span class="op">=</span><span class="st">'Prior'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x, mu_n, sigma_n), label<span class="op">=</span><span class="st">'Posterior'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'μ'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Prior and Posterior Distributions of μ'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="parameter-estimation_files/figure-html/cell-2-output-1.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code generates a plot showing how the prior distribution is updated to the posterior distribution after observing the data. Note that the posterior is a compromise between the prior and the information from the data. As we observe more data, the influence of the prior diminishes, and the posterior becomes increasingly dominated by the data likelihood.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
A[Prior Distribution] --&gt; B(Bayes' Theorem);
C[Likelihood Function] --&gt; B;
B --&gt; D[Posterior Distribution];
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This diagram illustrates how Bayes’ Theorem combines the prior and likelihood to produce the posterior distribution.</p>
</section>
<section id="point-estimation" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="point-estimation"><span class="header-section-number">11.1</span> Point Estimation</h2>
<p>Point estimation aims to provide a single best guess for the unknown parameter(s) of a statistical model. In the Bayesian framework, this is often done by summarizing the posterior distribution. We’ll look at two common approaches: Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation.</p>
<section id="maximum-likelihood-estimation-mle" class="level3" data-number="11.1.1">
<h3 data-number="11.1.1" class="anchored" data-anchor-id="maximum-likelihood-estimation-mle"><span class="header-section-number">11.1.1</span> Maximum Likelihood Estimation (MLE)</h3>
<p>Maximum Likelihood Estimation (MLE) is a frequentist approach. It finds the parameter value that maximizes the likelihood function, which is the probability of observing the data given the parameter value. Formally, we want to find <span class="math inline">\(\hat{\theta}_{MLE}\)</span> such that:</p>
<p><span class="math inline">\(\hat{\theta}_{MLE} = \arg \max_{\theta} L(\theta|x) = \arg \max_{\theta} P(x|\theta)\)</span></p>
<p>where <span class="math inline">\(L(\theta|x)\)</span> is the likelihood function, <span class="math inline">\(x\)</span> represents the observed data, and <span class="math inline">\(\theta\)</span> is the parameter we are estimating. Often, it’s easier to work with the log-likelihood, <span class="math inline">\(\log L(\theta|x)\)</span>, since it simplifies calculations and doesn’t change the location of the maximum.</p>
</section>
<section id="maximum-a-posteriori-map-estimation" class="level3" data-number="11.1.2">
<h3 data-number="11.1.2" class="anchored" data-anchor-id="maximum-a-posteriori-map-estimation"><span class="header-section-number">11.1.2</span> Maximum A Posteriori (MAP) Estimation</h3>
<p>Maximum A Posteriori (MAP) estimation is a Bayesian approach. It finds the mode of the posterior distribution, i.e., the parameter value that maximizes the posterior probability. Formally, we want to find <span class="math inline">\(\hat{\theta}_{MAP}\)</span> such that:</p>
<p><span class="math inline">\(\hat{\theta}_{MAP} = \arg \max_{\theta} P(\theta|x) = \arg \max_{\theta} \frac{P(x|\theta)P(\theta)}{P(x)} = \arg \max_{\theta} P(x|\theta)P(\theta)\)</span></p>
<p>Since <span class="math inline">\(P(x)\)</span> is independent of <span class="math inline">\(\theta\)</span>, we can ignore it in the maximization. Therefore, the MAP estimate is the parameter value that maximizes the product of the likelihood and the prior.</p>
</section>
<section id="comparing-mle-and-map" class="level3" data-number="11.1.3">
<h3 data-number="11.1.3" class="anchored" data-anchor-id="comparing-mle-and-map"><span class="header-section-number">11.1.3</span> Comparing MLE and MAP</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 39%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>MLE</th>
<th>MAP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Approach</td>
<td>Frequentist</td>
<td>Bayesian</td>
</tr>
<tr class="even">
<td>Goal</td>
<td>Maximize likelihood</td>
<td>Maximize posterior probability</td>
</tr>
<tr class="odd">
<td>Prior</td>
<td>Implicitly assumes uniform prior</td>
<td>Explicitly uses a prior distribution</td>
</tr>
<tr class="even">
<td>Computation</td>
<td>Often simpler</td>
<td>Can be more complex, depending on prior</td>
</tr>
<tr class="odd">
<td>Interpretation</td>
<td>Point estimate, no uncertainty</td>
<td>Point estimate, reflects prior belief</td>
</tr>
</tbody>
</table>
<p>As the number of data points increases, the influence of the prior diminishes, and the MAP estimate often converges to the MLE estimate. However, with limited data, the prior can significantly affect the MAP estimate.</p>
</section>
<section id="python-implementation-of-mle-and-map" class="level3" data-number="11.1.4">
<h3 data-number="11.1.4" class="anchored" data-anchor-id="python-implementation-of-mle-and-map"><span class="header-section-number">11.1.4</span> Python Implementation of MLE and MAP</h3>
<p>Let’s revisit the example of estimating the mean (<span class="math inline">\(\mu\)</span>) of a normal distribution with known variance (<span class="math inline">\(\sigma^2\)</span>). We’ll assume a normal prior for <span class="math inline">\(\mu\)</span>.</p>
<div id="d1c23b7e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="fl">1.5</span>, <span class="fl">2.1</span>, <span class="fl">1.8</span>, <span class="fl">2.3</span>, <span class="fl">1.9</span>])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Known variance</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># MLE</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>mle <span class="op">=</span> np.mean(data)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># MAP (assuming a normal prior)</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>mu_0 <span class="op">=</span> <span class="dv">2</span>   <span class="co"># Prior mean</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>sigma_0 <span class="op">=</span> <span class="dv">1</span> <span class="co"># Prior standard deviation</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>mu_map <span class="op">=</span> (np.<span class="bu">sum</span>(data) <span class="op">/</span> (sigma<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> mu_0 <span class="op">/</span> (sigma_0<span class="op">**</span><span class="dv">2</span>)) <span class="op">/</span> (<span class="bu">len</span>(data) <span class="op">/</span> (sigma<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span> <span class="op">/</span> (sigma_0<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>plt.hist(data, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Data Histogram'</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x, mle, sigma<span class="op">/</span>np.sqrt(<span class="bu">len</span>(data))), label<span class="op">=</span><span class="st">'MLE'</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x, mu_map, np.sqrt(<span class="dv">1</span><span class="op">/</span>(<span class="bu">len</span>(data)<span class="op">/</span>sigma<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>sigma_0<span class="op">**</span><span class="dv">2</span>))), label<span class="op">=</span><span class="st">'MAP'</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'μ'</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MLE: </span><span class="sc">{</span>mle<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAP: </span><span class="sc">{</span>mu_map<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="parameter-estimation_files/figure-html/cell-3-output-1.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>MLE: 1.92
MAP: 1.9238095238095236</code></pre>
</div>
</div>
<p>This code calculates both the MLE and MAP estimates for <span class="math inline">\(\mu\)</span> and visualizes them against a histogram of the data. You can observe how MLE and MAP might differ when the prior has an effect, especially with a small sample size. As the sample size increases the MLE and MAP estimators should converge.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
A[Data] --&gt; B(Likelihood Function);
C[Prior Distribution] --&gt; D(Bayes Theorem);
B --&gt; D;
D --&gt; E[Posterior Distribution];
F[MAP: mode of Posterior] --&gt; E;
G[MLE: maximum of Likelihood] --&gt; B;
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This diagram shows how both MLE and MAP approaches relate to the likelihood function and, in the case of MAP, the prior and posterior distributions.</p>
</section>
</section>
<section id="credible-intervals" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="credible-intervals"><span class="header-section-number">11.2</span> Credible Intervals</h2>
<p>While point estimates provide a single value for an unknown parameter, credible intervals offer a range of plausible values, reflecting the uncertainty in the estimate. Credible intervals are a key feature of Bayesian inference.</p>
<section id="definition-and-interpretation" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="definition-and-interpretation"><span class="header-section-number">11.2.1</span> Definition and Interpretation</h3>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> credible interval for a parameter <span class="math inline">\(\theta\)</span> is an interval <span class="math inline">\([a, b]\)</span> such that:</p>
<p><span class="math inline">\(P(a \le \theta \le b | x) = 1 - \alpha\)</span></p>
<p>where <span class="math inline">\(x\)</span> represents the observed data. This means that the probability that the true value of <span class="math inline">\(\theta\)</span> lies within the interval <span class="math inline">\([a, b]\)</span>, given the observed data, is <span class="math inline">\(1-\alpha\)</span>. The interpretation is fundamentally probabilistic: there’s a <span class="math inline">\(1-\alpha\)</span> probability that the true parameter value is within the credible interval. This is different from the frequentist confidence interval, which has a frequentist interpretation about the procedure rather than a statement about a single interval.</p>
</section>
<section id="calculating-credible-intervals" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="calculating-credible-intervals"><span class="header-section-number">11.2.2</span> Calculating Credible Intervals</h3>
<p>Calculating credible intervals depends on the form of the posterior distribution. If the posterior is easily integrable, we can find the interval directly. If not, we can use numerical methods such as Markov Chain Monte Carlo (MCMC) sampling techniques (covered in later chapters) to obtain samples from the posterior and estimate the credible interval from these samples.</p>
<p>For simple cases, if we have the cumulative distribution function (CDF) of the posterior distribution, <span class="math inline">\(F(\theta|x)\)</span>, we can find the credible interval <span class="math inline">\([a, b]\)</span> by solving:</p>
<p><span class="math inline">\(F(a|x) = \frac{\alpha}{2}\)</span> and <span class="math inline">\(F(b|x) = 1 - \frac{\alpha}{2}\)</span></p>
</section>
<section id="equal-tailed-vs.-highest-posterior-density-hpd-intervals" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="equal-tailed-vs.-highest-posterior-density-hpd-intervals"><span class="header-section-number">11.2.3</span> Equal-tailed vs.&nbsp;Highest Posterior Density (HPD) Intervals</h3>
<p>There are different ways to construct credible intervals:</p>
<ul>
<li><p><strong>Equal-tailed intervals:</strong> These intervals are defined by the equations above. They are simple to calculate but might not be the shortest interval containing <span class="math inline">\(1-\alpha\)</span> probability mass.</p></li>
<li><p><strong>Highest Posterior Density (HPD) intervals:</strong> These intervals contain the values of <span class="math inline">\(\theta\)</span> with the highest posterior density. They are always the shortest intervals containing <span class="math inline">\(1-\alpha\)</span> probability mass. Finding HPD intervals often requires numerical optimization techniques.</p></li>
</ul>
</section>
<section id="python-implementation-of-credible-intervals" class="level3" data-number="11.2.4">
<h3 data-number="11.2.4" class="anchored" data-anchor-id="python-implementation-of-credible-intervals"><span class="header-section-number">11.2.4</span> Python Implementation of Credible Intervals</h3>
<p>Let’s demonstrate calculating equal-tailed credible intervals using the previous example of estimating the mean of a normal distribution.</p>
<div id="85f6545e" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior parameters (from previous example, assuming we have posterior distribution)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>mu_n <span class="op">=</span> <span class="fl">2.0</span>  <span class="co">#Posterior mean</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>sigma_n <span class="op">=</span> <span class="fl">0.2</span> <span class="co">#Posterior standard deviation</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span> <span class="co"># 95% Credible Interval</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Calculate quantiles</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>lower_bound <span class="op">=</span> norm.ppf(alpha<span class="op">/</span><span class="dv">2</span>, loc<span class="op">=</span>mu_n, scale<span class="op">=</span>sigma_n)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>upper_bound <span class="op">=</span> norm.ppf(<span class="dv">1</span> <span class="op">-</span> alpha<span class="op">/</span><span class="dv">2</span>, loc<span class="op">=</span>mu_n, scale<span class="op">=</span>sigma_n)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(mu_n <span class="op">-</span> <span class="dv">3</span><span class="op">*</span>sigma_n, mu_n <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>sigma_n, <span class="dv">100</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x, mu_n, sigma_n), label<span class="op">=</span><span class="st">'Posterior Distribution'</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, <span class="dv">0</span>, norm.pdf(x, mu_n, sigma_n), where<span class="op">=</span>(x <span class="op">&gt;=</span> lower_bound) <span class="op">&amp;</span> (x <span class="op">&lt;=</span> upper_bound), color<span class="op">=</span><span class="st">'skyblue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span><span class="dv">1</span><span class="op">-</span>alpha<span class="sc">:.0%}</span><span class="ss"> Credible Interval'</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'μ'</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Credible Interval'</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"95% Credible Interval: [</span><span class="sc">{</span>lower_bound<span class="sc">:.2f}</span><span class="ss">, </span><span class="sc">{</span>upper_bound<span class="sc">:.2f}</span><span class="ss">]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="parameter-estimation_files/figure-html/cell-4-output-1.png" width="597" height="449" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>95% Credible Interval: [1.61, 2.39]</code></pre>
</div>
</div>
<p>This code calculates and plots a 95% equal-tailed credible interval for the posterior distribution of <span class="math inline">\(\mu\)</span>.</p>
</section>
<section id="choosing-the-credible-interval-level" class="level3" data-number="11.2.5">
<h3 data-number="11.2.5" class="anchored" data-anchor-id="choosing-the-credible-interval-level"><span class="header-section-number">11.2.5</span> Choosing the Credible Interval Level</h3>
<p>The choice of the credible interval level (e.g., 95%, 99%) depends on the context and the desired level of certainty. A higher credible interval level implies a wider interval, reflecting greater uncertainty. A 95% credible interval is commonly used, but other levels might be appropriate depending on the application’s risk tolerance. There isn’t a universally optimal level.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
A[Posterior Distribution] --&gt; B(CDF);
B --&gt; C{Find quantiles};
C --&gt; D[Credible Interval];
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This diagram shows how to obtain a credible interval from the posterior distribution via the CDF.</p>
</section>
</section>
<section id="advanced-topics-in-parameter-estimation" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="advanced-topics-in-parameter-estimation"><span class="header-section-number">11.3</span> Advanced Topics in Parameter Estimation</h2>
<p>This section briefly introduces some more advanced topics in Bayesian parameter estimation, providing a foundation for further exploration.</p>
<section id="bayesian-model-comparison" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="bayesian-model-comparison"><span class="header-section-number">11.3.1</span> Bayesian Model Comparison</h3>
<p>Often, we have multiple competing models to explain the same data. Bayesian model comparison provides a formal framework for selecting the best model. The key concept is the <em>Bayes factor</em>, which is the ratio of the marginal likelihoods of two models:</p>
<p><span class="math inline">\(B_{12} = \frac{P(x|M_1)}{P(x|M_2)}\)</span></p>
<p>where <span class="math inline">\(P(x|M_i)\)</span> is the marginal likelihood of model <span class="math inline">\(M_i\)</span>. A Bayes factor greater than 1 favors model <span class="math inline">\(M_1\)</span>, while a Bayes factor less than 1 favors model <span class="math inline">\(M_2\)</span>. The marginal likelihood is often difficult to calculate analytically, requiring numerical methods like MCMC. Another approach is to use model evidence. The model with the higher model evidence is favored. Model evidence is calculated by integrating the likelihood over the prior:</p>
<p><span class="math inline">\(P(x|M) = \int P(x|\theta, M) P(\theta|M) d\theta\)</span></p>
<p>Where <span class="math inline">\(M\)</span> denotes the model.</p>
</section>
<section id="hierarchical-models" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="hierarchical-models"><span class="header-section-number">11.3.2</span> Hierarchical Models</h3>
<p>Hierarchical models are useful when dealing with data from multiple related sources or groups. They introduce parameters at different levels, allowing for sharing of information across groups. For example, we might model the performance of students in different schools, allowing for school-specific effects while also borrowing strength across schools to estimate overall effects. This can be represented by multi-level models. A simple hierarchical model might look like:</p>
<p><span class="math inline">\(\theta_i \sim N(\mu, \tau^2)\)</span> (group-level parameters) <span class="math inline">\(x_i \sim N(\theta_i, \sigma^2)\)</span> (individual observations)</p>
<p>Here, <span class="math inline">\(\theta_i\)</span> are group-level parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span> represent the hyperparameters for the group-level distribution, and <span class="math inline">\(\sigma^2\)</span> is the variance of individual observations.</p>
</section>
<section id="dealing-with-high-dimensional-data" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="dealing-with-high-dimensional-data"><span class="header-section-number">11.3.3</span> Dealing with High-Dimensional Data</h3>
<p>High-dimensional data (many parameters relative to the number of data points) pose challenges for Bayesian estimation. Techniques like regularization (e.g., adding priors that shrink parameters towards zero) or dimensionality reduction are essential to avoid overfitting and ensure stable posterior estimates. Prior selection plays a critical role in high-dimensional settings. Sparsity-inducing priors, like Laplace or horseshoe priors, are particularly useful in shrinking many parameters to exactly zero, effectively performing variable selection.</p>
</section>
<section id="computational-methods-mcmc" class="level3" data-number="11.3.4">
<h3 data-number="11.3.4" class="anchored" data-anchor-id="computational-methods-mcmc"><span class="header-section-number">11.3.4</span> Computational Methods (MCMC)</h3>
<p>For complex models, analytical solutions are often intractable. Markov Chain Monte Carlo (MCMC) methods provide a powerful approach to approximate the posterior distribution by generating a sample from it. MCMC algorithms, such as Metropolis-Hastings and Gibbs sampling, construct a Markov chain whose stationary distribution is the target posterior. By running the chain for a sufficient number of iterations, we can obtain a sample that accurately represents the posterior. Libraries like PyMC provide tools for implementing MCMC in Python.</p>
<div id="13a0fa57" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Simple linear regression with PyMC</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="fl">2.1</span>, <span class="fl">3.9</span>, <span class="fl">6.2</span>, <span class="fl">7.8</span>, <span class="fl">10.1</span>])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Priors</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    intercept <span class="op">=</span> pm.Normal(<span class="st">"intercept"</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    slope <span class="op">=</span> pm.Normal(<span class="st">"slope"</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pm.HalfNormal(<span class="st">"sigma"</span>, sigma<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> intercept <span class="op">+</span> slope <span class="op">*</span> X</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> pm.Normal(<span class="st">"y_obs"</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>y)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling using MCMC</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">1000</span>, tune<span class="op">=</span><span class="dv">1000</span>) <span class="co">#tune helps the algorithm to converge faster</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>pm.plot_trace(trace)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [intercept, slope, sigma]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.
There were 58 divergences after tuning. Increase `target_accept` or reparameterize.
The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="parameter-estimation_files/figure-html/cell-5-output-5.png" width="915" height="505" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mean</th>
<th data-quarto-table-cell-role="th">sd</th>
<th data-quarto-table-cell-role="th">hdi_3%</th>
<th data-quarto-table-cell-role="th">hdi_97%</th>
<th data-quarto-table-cell-role="th">mcse_mean</th>
<th data-quarto-table-cell-role="th">mcse_sd</th>
<th data-quarto-table-cell-role="th">ess_bulk</th>
<th data-quarto-table-cell-role="th">ess_tail</th>
<th data-quarto-table-cell-role="th">r_hat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">intercept</td>
<td>0.023</td>
<td>0.748</td>
<td>-0.989</td>
<td>1.024</td>
<td>0.048</td>
<td>0.036</td>
<td>614.0</td>
<td>417.0</td>
<td>1.01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">slope</td>
<td>2.000</td>
<td>0.221</td>
<td>1.648</td>
<td>2.269</td>
<td>0.014</td>
<td>0.010</td>
<td>612.0</td>
<td>415.0</td>
<td>1.01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">sigma</td>
<td>0.403</td>
<td>0.448</td>
<td>0.097</td>
<td>0.977</td>
<td>0.026</td>
<td>0.019</td>
<td>425.0</td>
<td>501.0</td>
<td>1.01</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This code performs a simple linear regression using PyMC, demonstrating MCMC sampling to obtain posterior estimates of the model parameters. Note that <code>tune</code> is added to allow the sampler to find a good starting point for the MCMC chain. The trace plot visualizes the MCMC samples and helps assess convergence. The summary provides statistics like the mean, standard deviation, and credible intervals for each parameter. The use of priors such as <code>HalfNormal</code> helps to guide and constrain the MCMC algorithm.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
A[Prior] --&gt; B(Likelihood);
B --&gt; C[Posterior];
C --&gt; D[MCMC Sampling];
D --&gt; E[Posterior Sample];
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This diagram illustrates the role of MCMC in approximating the posterior distribution. The algorithm iteratively samples from the posterior distribution until the samples accurately represent the target distribution.</p>
</section>
</section>
<section id="case-studies" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="case-studies"><span class="header-section-number">11.4</span> Case Studies</h2>
<p>This section presents practical examples of Bayesian parameter estimation using Python.</p>
<section id="example-estimating-the-mean-of-a-normal-distribution" class="level3" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="example-estimating-the-mean-of-a-normal-distribution"><span class="header-section-number">11.4.1</span> Example: Estimating the Mean of a Normal Distribution</h3>
<p>Let’s revisit the problem of estimating the mean (<span class="math inline">\(\mu\)</span>) of a normal distribution with known variance (<span class="math inline">\(\sigma^2\)</span>) from a sample of data <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>. We assume a normal prior for <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span></p>
<p>The likelihood is given by:</p>
<p><span class="math inline">\(P(x|\mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)\)</span></p>
<p>The posterior distribution, using Bayes’ Theorem, is also a normal distribution:</p>
<p><span class="math inline">\(\mu | x \sim N(\mu_n, \sigma_n^2)\)</span></p>
<p>where:</p>
<p><span class="math inline">\(\mu_n = \frac{\frac{\mu_0}{\sigma_0^2} + \frac{\sum_{i=1}^n x_i}{\sigma^2}}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}\)</span></p>
<p><span class="math inline">\(\sigma_n^2 = \frac{1}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}\)</span></p>
<div id="5b3e6917" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>mu_0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>sigma_0 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood parameters (assuming known sigma)</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">2</span>, scale<span class="op">=</span>sigma, size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior parameters</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>mu_n <span class="op">=</span> (mu_0 <span class="op">/</span> sigma_0<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> np.<span class="bu">sum</span>(data) <span class="op">/</span> sigma<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> sigma_0<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> n <span class="op">/</span> sigma<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>sigma_n <span class="op">=</span> np.sqrt(<span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">/</span> sigma_0<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> n <span class="op">/</span> sigma<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x, mu_0, sigma_0), label<span class="op">=</span><span class="st">'Prior'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x, mu_n, sigma_n), label<span class="op">=</span><span class="st">'Posterior'</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.hist(data, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Data Histogram'</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'μ'</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior Mean: </span><span class="sc">{</span>mu_n<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Posterior Standard Deviation: </span><span class="sc">{</span>sigma_n<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="parameter-estimation_files/figure-html/cell-6-output-1.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Posterior Mean: 1.83
Posterior Standard Deviation: 0.30</code></pre>
</div>
</div>
<p>This code generates a plot showing the prior, posterior, and data histogram.</p>
</section>
<section id="example-estimating-the-parameter-of-a-binomial-distribution" class="level3" data-number="11.4.2">
<h3 data-number="11.4.2" class="anchored" data-anchor-id="example-estimating-the-parameter-of-a-binomial-distribution"><span class="header-section-number">11.4.2</span> Example: Estimating the Parameter of a Binomial Distribution</h3>
<p>Let’s estimate the success probability (<span class="math inline">\(\theta\)</span>) of a binomial distribution. We observe <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> trials. We assume a Beta prior for <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math inline">\(\theta \sim Beta(\alpha, \beta)\)</span></p>
<p>The likelihood is:</p>
<p><span class="math inline">\(P(k|\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k}\)</span></p>
<p>The posterior distribution is also a Beta distribution:</p>
<p><span class="math inline">\(\theta | k \sim Beta(\alpha + k, \beta + n - k)\)</span></p>
<div id="542165d2" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>alpha_index <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>beta_index <span class="op">=</span> <span class="dv">1</span>  <span class="co">#Uniform prior</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior parameters</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>alpha_post <span class="op">=</span> alpha_index <span class="op">+</span> k</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>beta_post <span class="op">=</span> beta_index <span class="op">+</span> n <span class="op">-</span> k</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>plt.plot(x, beta.pdf(x, alpha_index, beta_index), label<span class="op">=</span><span class="st">'Prior'</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>plt.plot(x, beta.pdf(x, alpha_post, beta_post), label<span class="op">=</span><span class="st">'Posterior'</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="parameter-estimation_files/figure-html/cell-7-output-1.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code shows how the prior Beta distribution is updated to the posterior Beta distribution after observing the binomial data.</p>
</section>
<section id="example-bayesian-linear-regression" class="level3" data-number="11.4.3">
<h3 data-number="11.4.3" class="anchored" data-anchor-id="example-bayesian-linear-regression"><span class="header-section-number">11.4.3</span> Example: Bayesian Linear Regression</h3>
<p>Bayesian linear regression models the relationship between a dependent variable <span class="math inline">\(y\)</span> and independent variables <span class="math inline">\(X\)</span> as:</p>
<p><span class="math inline">\(y_i = X_i \beta + \epsilon_i\)</span></p>
<p>where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>. We can assign priors to <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> (e.g., normal and inverse gamma, respectively). Inference is performed using MCMC sampling.</p>
<div id="0c2fdd7d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Simulate some data</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">100</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>true_slope <span class="op">=</span> <span class="fl">2.5</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>true_intercept <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> true_slope <span class="op">*</span> X <span class="op">+</span> true_intercept</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y_true <span class="op">+</span> np.random.normal(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">100</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Priors</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    intercept <span class="op">=</span> pm.Normal(<span class="st">"intercept"</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    slope <span class="op">=</span> pm.Normal(<span class="st">"slope"</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pm.HalfNormal(<span class="st">"sigma"</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Likelihood</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> intercept <span class="op">+</span> slope <span class="op">*</span> X</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> pm.Normal(<span class="st">"y_obs"</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>y)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#MCMC</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">2000</span>, tune<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>pm.plot_trace(trace)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [intercept, slope, sigma]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 2 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="parameter-estimation_files/figure-html/cell-8-output-5.png" width="912" height="505" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mean</th>
<th data-quarto-table-cell-role="th">sd</th>
<th data-quarto-table-cell-role="th">hdi_3%</th>
<th data-quarto-table-cell-role="th">hdi_97%</th>
<th data-quarto-table-cell-role="th">mcse_mean</th>
<th data-quarto-table-cell-role="th">mcse_sd</th>
<th data-quarto-table-cell-role="th">ess_bulk</th>
<th data-quarto-table-cell-role="th">ess_tail</th>
<th data-quarto-table-cell-role="th">r_hat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">intercept</td>
<td>0.826</td>
<td>0.182</td>
<td>0.473</td>
<td>1.167</td>
<td>0.003</td>
<td>0.002</td>
<td>4012.0</td>
<td>4046.0</td>
<td>1.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">slope</td>
<td>2.514</td>
<td>0.031</td>
<td>2.455</td>
<td>2.573</td>
<td>0.000</td>
<td>0.000</td>
<td>3970.0</td>
<td>3961.0</td>
<td>1.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">sigma</td>
<td>0.924</td>
<td>0.067</td>
<td>0.793</td>
<td>1.043</td>
<td>0.001</td>
<td>0.001</td>
<td>4605.0</td>
<td>4258.0</td>
<td>1.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This uses PyMC to perform Bayesian linear regression, illustrating the use of MCMC for posterior inference. The trace plot visualizes the samples. The summary shows the posterior means, standard deviations, and credible intervals for the intercept and slope.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../parts/bayesian-inference/intro.html" class="pagination-link" aria-label="Bayesian Inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Bayesian Inference</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../parts/bayesian-inference/conjugate-priors.html" class="pagination-link" aria-label="Conjugate Priors">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Conjugate Priors</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>