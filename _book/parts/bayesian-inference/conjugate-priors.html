<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Conjugate Priors – bayes-theorem-book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../parts/bayesian-inference/prior-selection.html" rel="next">
<link href="../../parts/bayesian-inference/parameter-estimation.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-a2a08d6480f1a07d2e84f5b3bded3372.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/intro.html">Bayesian Inference</a></li><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/conjugate-priors.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Conjugate Priors</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">bayes-theorem-book</a> 
        <div class="sidebar-tools-main">
    <a href="../../bayes-theorem-book.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/introduction-to-bayesian-thinking/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Bayesian Thinking</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/understanding-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/bayes-theorem-fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem Fundamentals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/introduction-to-bayesian-thinking/setting-up-python-environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Setting Up Python Environment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/mathematical-foundations/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/probability-theory-essentials.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Introduction to Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/statistical-concepts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Basic Probability Concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/mathematical-foundations/linear-algebra-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Linear Algebra Review</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/implementing-bayes-theorem/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Implementing Bayes' Theorem</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/basic-implementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Basic Implementation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/working-with-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/implementing-bayes-theorem/discrete-probability-examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Discrete Probability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/bayesian-inference/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/parameter-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Parameter Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/conjugate-priors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Conjugate Priors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/bayesian-inference/prior-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Prior Selection</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/markov-chain-monte-carlo-mcmc/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Markov Chain Monte Carlo (MCMC)</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/introduction-to-mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Monte Carlo Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/mcmc-algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Introduction to MCMC</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/markov-chain-monte-carlo-mcmc/implementation-with-pymc3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Implementation with PyMC</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/practical-applications/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/ab-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/text-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Introduction to Text Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/practical-applications/medical-diagnosis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Disease Testing with Bayes’ Theorem</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/advanced-topics/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/hierarchical-bayesian-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Hierarchical Bayesian Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/bayesian-neural-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/advanced-topics/gaussian-processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/real-world-applications/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Real-World Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/finance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Finance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/marketing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Bayesian Methods in Customer Segmentation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/real-world-applications/scientific-applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Scientific Applications</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../parts/best-practices-and-advanced-tools/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Best Practices and Advanced Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/code-organization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Code Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/performance-optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Understanding Performance Bottlenecks in Bayesian Computations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../parts/best-practices-and-advanced-tools/modern-bayesian-libraries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Modern Bayesian Libraries</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-conjugate-priors" id="toc-introduction-to-conjugate-priors" class="nav-link active" data-scroll-target="#introduction-to-conjugate-priors"><span class="header-section-number">12.0.1</span> Introduction to Conjugate Priors</a></li>
  <li><a href="#what-are-conjugate-priors" id="toc-what-are-conjugate-priors" class="nav-link" data-scroll-target="#what-are-conjugate-priors"><span class="header-section-number">12.0.2</span> What are Conjugate Priors?</a></li>
  <li><a href="#advantages-of-using-conjugate-priors" id="toc-advantages-of-using-conjugate-priors" class="nav-link" data-scroll-target="#advantages-of-using-conjugate-priors"><span class="header-section-number">12.0.3</span> Advantages of Using Conjugate Priors</a></li>
  <li><a href="#limitations-of-conjugate-priors" id="toc-limitations-of-conjugate-priors" class="nav-link" data-scroll-target="#limitations-of-conjugate-priors"><span class="header-section-number">12.0.4</span> Limitations of Conjugate Priors</a></li>
  <li><a href="#beta-binomial-model" id="toc-beta-binomial-model" class="nav-link" data-scroll-target="#beta-binomial-model"><span class="header-section-number">12.1</span> Beta-Binomial Model</a>
  <ul class="collapse">
  <li><a href="#the-binomial-likelihood" id="toc-the-binomial-likelihood" class="nav-link" data-scroll-target="#the-binomial-likelihood"><span class="header-section-number">12.1.1</span> The Binomial Likelihood</a></li>
  <li><a href="#the-beta-prior" id="toc-the-beta-prior" class="nav-link" data-scroll-target="#the-beta-prior"><span class="header-section-number">12.1.2</span> The Beta Prior</a></li>
  <li><a href="#posterior-distribution-derivation" id="toc-posterior-distribution-derivation" class="nav-link" data-scroll-target="#posterior-distribution-derivation"><span class="header-section-number">12.1.3</span> Posterior Distribution Derivation</a></li>
  <li><a href="#bayesian-inference-with-beta-binomial" id="toc-bayesian-inference-with-beta-binomial" class="nav-link" data-scroll-target="#bayesian-inference-with-beta-binomial"><span class="header-section-number">12.1.4</span> Bayesian Inference with Beta-Binomial</a></li>
  <li><a href="#python-implementation-with-pymc" id="toc-python-implementation-with-pymc" class="nav-link" data-scroll-target="#python-implementation-with-pymc"><span class="header-section-number">12.1.5</span> Python Implementation with PyMC</a></li>
  </ul></li>
  <li><a href="#normal-normal-model" id="toc-normal-normal-model" class="nav-link" data-scroll-target="#normal-normal-model"><span class="header-section-number">12.2</span> Normal-Normal Model</a>
  <ul class="collapse">
  <li><a href="#the-normal-likelihood" id="toc-the-normal-likelihood" class="nav-link" data-scroll-target="#the-normal-likelihood"><span class="header-section-number">12.2.1</span> The Normal Likelihood</a></li>
  <li><a href="#the-normal-prior" id="toc-the-normal-prior" class="nav-link" data-scroll-target="#the-normal-prior"><span class="header-section-number">12.2.2</span> The Normal Prior</a></li>
  <li><a href="#posterior-distribution-derivation-1" id="toc-posterior-distribution-derivation-1" class="nav-link" data-scroll-target="#posterior-distribution-derivation-1"><span class="header-section-number">12.2.3</span> Posterior Distribution Derivation</a></li>
  <li><a href="#bayesian-inference-with-normal-normal" id="toc-bayesian-inference-with-normal-normal" class="nav-link" data-scroll-target="#bayesian-inference-with-normal-normal"><span class="header-section-number">12.2.4</span> Bayesian Inference with Normal-Normal</a></li>
  <li><a href="#python-implementation-with-pymc-1" id="toc-python-implementation-with-pymc-1" class="nav-link" data-scroll-target="#python-implementation-with-pymc-1"><span class="header-section-number">12.2.5</span> Python Implementation with PyMC</a></li>
  <li><a href="#handling-unknown-variance" id="toc-handling-unknown-variance" class="nav-link" data-scroll-target="#handling-unknown-variance"><span class="header-section-number">12.2.6</span> Handling Unknown Variance</a></li>
  </ul></li>
  <li><a href="#dirichlet-multinomial-model" id="toc-dirichlet-multinomial-model" class="nav-link" data-scroll-target="#dirichlet-multinomial-model"><span class="header-section-number">12.3</span> Dirichlet-Multinomial Model</a>
  <ul class="collapse">
  <li><a href="#the-multinomial-likelihood" id="toc-the-multinomial-likelihood" class="nav-link" data-scroll-target="#the-multinomial-likelihood"><span class="header-section-number">12.3.1</span> The Multinomial Likelihood</a></li>
  <li><a href="#the-dirichlet-prior" id="toc-the-dirichlet-prior" class="nav-link" data-scroll-target="#the-dirichlet-prior"><span class="header-section-number">12.3.2</span> The Dirichlet Prior</a></li>
  <li><a href="#posterior-distribution-derivation-2" id="toc-posterior-distribution-derivation-2" class="nav-link" data-scroll-target="#posterior-distribution-derivation-2"><span class="header-section-number">12.3.3</span> Posterior Distribution Derivation</a></li>
  <li><a href="#bayesian-inference-with-dirichlet-multinomial" id="toc-bayesian-inference-with-dirichlet-multinomial" class="nav-link" data-scroll-target="#bayesian-inference-with-dirichlet-multinomial"><span class="header-section-number">12.3.4</span> Bayesian Inference with Dirichlet-Multinomial</a></li>
  <li><a href="#python-implementation-with-pymc-2" id="toc-python-implementation-with-pymc-2" class="nav-link" data-scroll-target="#python-implementation-with-pymc-2"><span class="header-section-number">12.3.5</span> Python Implementation with PyMC</a></li>
  <li><a href="#applications-in-text-mining-and-other-fields" id="toc-applications-in-text-mining-and-other-fields" class="nav-link" data-scroll-target="#applications-in-text-mining-and-other-fields"><span class="header-section-number">12.3.6</span> Applications in Text Mining and other fields</a></li>
  </ul></li>
  <li><a href="#beyond-the-basic-models" id="toc-beyond-the-basic-models" class="nav-link" data-scroll-target="#beyond-the-basic-models"><span class="header-section-number">12.4</span> Beyond the Basic Models</a>
  <ul class="collapse">
  <li><a href="#other-conjugate-prior-pairs" id="toc-other-conjugate-prior-pairs" class="nav-link" data-scroll-target="#other-conjugate-prior-pairs"><span class="header-section-number">12.4.1</span> Other Conjugate Prior Pairs</a></li>
  <li><a href="#choosing-appropriate-conjugate-priors" id="toc-choosing-appropriate-conjugate-priors" class="nav-link" data-scroll-target="#choosing-appropriate-conjugate-priors"><span class="header-section-number">12.4.2</span> Choosing Appropriate Conjugate Priors</a></li>
  <li><a href="#approximations-when-conjugate-priors-are-unavailable" id="toc-approximations-when-conjugate-priors-are-unavailable" class="nav-link" data-scroll-target="#approximations-when-conjugate-priors-are-unavailable"><span class="header-section-number">12.4.3</span> Approximations when Conjugate Priors are Unavailable</a></li>
  </ul></li>
  <li><a href="#case-studies-and-applications" id="toc-case-studies-and-applications" class="nav-link" data-scroll-target="#case-studies-and-applications"><span class="header-section-number">12.5</span> Case Studies and Applications</a>
  <ul class="collapse">
  <li><a href="#ab-testing-with-beta-binomial" id="toc-ab-testing-with-beta-binomial" class="nav-link" data-scroll-target="#ab-testing-with-beta-binomial"><span class="header-section-number">12.5.1</span> A/B Testing with Beta-Binomial</a></li>
  <li><a href="#estimating-population-means-with-normal-normal" id="toc-estimating-population-means-with-normal-normal" class="nav-link" data-scroll-target="#estimating-population-means-with-normal-normal"><span class="header-section-number">12.5.2</span> Estimating Population Means with Normal-Normal</a></li>
  <li><a href="#topic-modeling-with-dirichlet-multinomial" id="toc-topic-modeling-with-dirichlet-multinomial" class="nav-link" data-scroll-target="#topic-modeling-with-dirichlet-multinomial"><span class="header-section-number">12.5.3</span> Topic Modeling with Dirichlet-Multinomial</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/intro.html">Bayesian Inference</a></li><li class="breadcrumb-item"><a href="../../parts/bayesian-inference/conjugate-priors.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Conjugate Priors</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Conjugate Priors</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-to-conjugate-priors" class="level3" data-number="12.0.1">
<h3 data-number="12.0.1" class="anchored" data-anchor-id="introduction-to-conjugate-priors"><span class="header-section-number">12.0.1</span> Introduction to Conjugate Priors</h3>
<p>Bayesian inference involves updating our prior beliefs about a parameter <span class="math inline">\(\theta\)</span> given observed data <span class="math inline">\(X\)</span>. We represent our prior beliefs using a prior distribution <span class="math inline">\(p(\theta)\)</span>. After observing data, we update our beliefs using Bayes’ theorem:</p>
<p><span class="math inline">\(p(\theta|X) = \frac{p(X|\theta)p(\theta)}{p(X)}\)</span></p>
<p>where <span class="math inline">\(p(\theta|X)\)</span> is the posterior distribution, <span class="math inline">\(p(X|\theta)\)</span> is the likelihood function, and <span class="math inline">\(p(X)\)</span> is the marginal likelihood (often a normalizing constant). Calculating the posterior distribution can be computationally challenging. This is where conjugate priors come in handy.</p>
<p>A conjugate prior is a prior distribution that, when combined with the likelihood function, results in a posterior distribution that belongs to the same family of distributions as the prior. This simplifies calculations significantly, as the posterior’s parameters can be determined directly from the prior and the likelihood, without resorting to complex numerical integration or approximation techniques. This elegance makes conjugate priors a powerful tool in Bayesian analysis, especially for pedagogical purposes and in situations where computational resources are limited.</p>
</section>
<section id="what-are-conjugate-priors" class="level3" data-number="12.0.2">
<h3 data-number="12.0.2" class="anchored" data-anchor-id="what-are-conjugate-priors"><span class="header-section-number">12.0.2</span> What are Conjugate Priors?</h3>
<p>Formally, let <span class="math inline">\(p(\theta)\)</span> be the prior distribution and <span class="math inline">\(p(X|\theta)\)</span> be the likelihood function. If the posterior distribution <span class="math inline">\(p(\theta|X)\)</span> is in the same family of distributions as the prior <span class="math inline">\(p(\theta)\)</span>, then the prior is said to be conjugate to the likelihood. The choice of conjugate prior depends heavily on the likelihood function. Some common examples include:</p>
<ul>
<li><p><strong>Beta distribution as a conjugate prior for the Bernoulli likelihood:</strong> If we model the probability of success in a Bernoulli trial with a parameter <span class="math inline">\(\theta\)</span>, then a Beta distribution, <span class="math inline">\(Beta(\alpha, \beta)\)</span>, is a conjugate prior. The posterior will also be a Beta distribution with updated parameters.</p></li>
<li><p><strong>Normal distribution as a conjugate prior for the Normal likelihood:</strong> If we model data as coming from a Normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>, then a Normal distribution, <span class="math inline">\(N(\mu_0, \sigma_0^2)\)</span>, is a conjugate prior for <span class="math inline">\(\mu\)</span>. The posterior will also be a Normal distribution.</p></li>
<li><p><strong>Gamma distribution as a conjugate prior for the Poisson likelihood:</strong> If we model count data with a Poisson distribution, with parameter <span class="math inline">\(\lambda\)</span>, then a Gamma distribution, <span class="math inline">\(Gamma(\alpha, \beta)\)</span>, is a conjugate prior. The posterior will also be a Gamma distribution.</p></li>
</ul>
</section>
<section id="advantages-of-using-conjugate-priors" class="level3" data-number="12.0.3">
<h3 data-number="12.0.3" class="anchored" data-anchor-id="advantages-of-using-conjugate-priors"><span class="header-section-number">12.0.3</span> Advantages of Using Conjugate Priors</h3>
<ul>
<li><p><strong>Analytical tractability:</strong> The biggest advantage is the ability to derive the posterior distribution analytically. This avoids computationally intensive methods like Markov Chain Monte Carlo (MCMC), making inference faster and easier.</p></li>
<li><p><strong>Intuitive interpretation:</strong> The parameters of the conjugate prior often have intuitive interpretations, making it easier to specify the prior and understand the results. For example, the parameters of a Beta prior directly relate to our prior belief about the probability of success.</p></li>
<li><p><strong>Simplicity:</strong> The mathematical expressions for updating the posterior are relatively simple and easy to implement.</p></li>
</ul>
</section>
<section id="limitations-of-conjugate-priors" class="level3" data-number="12.0.4">
<h3 data-number="12.0.4" class="anchored" data-anchor-id="limitations-of-conjugate-priors"><span class="header-section-number">12.0.4</span> Limitations of Conjugate Priors</h3>
<ul>
<li><p><strong>Limited flexibility:</strong> The biggest limitation is the restriction to a specific family of distributions. If the true prior belief doesn’t closely resemble a conjugate prior, forcing the use of a conjugate prior can lead to inaccurate inferences.</p></li>
<li><p><strong>Oversimplification:</strong> The assumption of conjugacy might oversimplify the true relationships in the data.</p></li>
<li><p><strong>Potential for misspecification:</strong> Improperly choosing the parameters of the conjugate prior can significantly bias the posterior, leading to erroneous conclusions. Careful consideration is needed when selecting prior parameters.</p></li>
</ul>
<div id="fc20aeb5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Beta prior and Bernoulli likelihood</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>alpha_prior <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>beta_prior <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed data (number of successes and failures)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>successes <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>failures <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior parameters</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>alpha_posterior <span class="op">=</span> alpha_prior <span class="op">+</span> successes</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>beta_posterior <span class="op">=</span> beta_prior <span class="op">+</span> failures</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate x values for plotting</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot prior and posterior distributions</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.plot(x, beta.pdf(x, alpha_prior, beta_prior), label<span class="op">=</span><span class="st">'Prior'</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.plot(x, beta.pdf(x, alpha_posterior, beta_posterior), label<span class="op">=</span><span class="st">'Posterior'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'θ'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability Density'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Beta Prior and Posterior'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-2-output-1.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This Python code demonstrates the update of a Beta prior given Bernoulli data, illustrating the conjugacy property visually. The plot shows how the posterior distribution shifts based on the observed data.</p>
</section>
<section id="beta-binomial-model" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="beta-binomial-model"><span class="header-section-number">12.1</span> Beta-Binomial Model</h2>
<section id="the-binomial-likelihood" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="the-binomial-likelihood"><span class="header-section-number">12.1.1</span> The Binomial Likelihood</h3>
<p>The binomial distribution is a fundamental probability model for the number of successes in a fixed number of independent Bernoulli trials. Each trial has a probability of success <span class="math inline">\(\theta\)</span>, where <span class="math inline">\(0 \le \theta \le 1\)</span>. If we observe <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> trials, the likelihood function is given by:</p>
<p><span class="math inline">\(p(k|\theta, n) = \binom{n}{k} \theta^k (1-\theta)^{n-k}\)</span></p>
<p>where <span class="math inline">\(\binom{n}{k} = \frac{n!}{k!(n-k)!}\)</span> is the binomial coefficient. This likelihood describes the probability of observing <span class="math inline">\(k\)</span> successes given the parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>. In a Bayesian context, we treat <span class="math inline">\(\theta\)</span> as a random variable, and the likelihood quantifies the plausibility of different values of <span class="math inline">\(\theta\)</span> given the observed data.</p>
</section>
<section id="the-beta-prior" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="the-beta-prior"><span class="header-section-number">12.1.2</span> The Beta Prior</h3>
<p>A natural choice for the prior distribution of <span class="math inline">\(\theta\)</span> is the Beta distribution. The Beta distribution is defined on the interval [0, 1] and is parameterized by two positive shape parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math inline">\(p(\theta|\alpha, \beta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}\)</span></p>
<p>where <span class="math inline">\(B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\)</span> is the Beta function, and <span class="math inline">\(\Gamma(\cdot)\)</span> is the gamma function. The parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> control the shape of the distribution. A larger <span class="math inline">\(\alpha\)</span> relative to <span class="math inline">\(\beta\)</span> shifts the distribution towards larger values of <span class="math inline">\(\theta\)</span>, representing a prior belief that <span class="math inline">\(\theta\)</span> is likely to be higher. Conversely, a larger <span class="math inline">\(\beta\)</span> relative to <span class="math inline">\(\alpha\)</span> shifts the distribution towards smaller values of <span class="math inline">\(\theta\)</span>. When <span class="math inline">\(\alpha = \beta = 1\)</span>, the Beta distribution is uniform, representing a lack of prior information.</p>
</section>
<section id="posterior-distribution-derivation" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="posterior-distribution-derivation"><span class="header-section-number">12.1.3</span> Posterior Distribution Derivation</h3>
<p>Because the Beta distribution is a conjugate prior for the binomial likelihood, the posterior distribution is also a Beta distribution. Applying Bayes’ theorem:</p>
<p><span class="math inline">\(p(\theta|k, n, \alpha, \beta) \propto p(k|\theta, n) p(\theta|\alpha, \beta)\)</span></p>
<p><span class="math inline">\(p(\theta|k, n, \alpha, \beta) \propto \binom{n}{k} \theta^k (1-\theta)^{n-k} \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1}\)</span></p>
<p>Ignoring terms that don’t depend on <span class="math inline">\(\theta\)</span>, we obtain:</p>
<p><span class="math inline">\(p(\theta|k, n, \alpha, \beta) \propto \theta^{k+\alpha-1} (1-\theta)^{n-k+\beta-1}\)</span></p>
<p>This is the kernel of a Beta distribution with parameters <span class="math inline">\(\alpha' = \alpha + k\)</span> and <span class="math inline">\(\beta' = \beta + n - k\)</span>. Therefore, the posterior distribution is:</p>
<p><span class="math inline">\(p(\theta|k, n, \alpha, \beta) = Beta(\alpha + k, \beta + n - k)\)</span></p>
</section>
<section id="bayesian-inference-with-beta-binomial" class="level3" data-number="12.1.4">
<h3 data-number="12.1.4" class="anchored" data-anchor-id="bayesian-inference-with-beta-binomial"><span class="header-section-number">12.1.4</span> Bayesian Inference with Beta-Binomial</h3>
<p>The Beta-Binomial model allows for straightforward Bayesian inference. We start with a Beta prior reflecting our prior beliefs about <span class="math inline">\(\theta\)</span>. After observing data (number of successes <span class="math inline">\(k\)</span> in <span class="math inline">\(n\)</span> trials), we update our beliefs by calculating the posterior Beta distribution using the updated parameters <span class="math inline">\(\alpha' = \alpha + k\)</span> and <span class="math inline">\(\beta' = \beta + n - k\)</span>. We can then use the posterior distribution to make inferences about <span class="math inline">\(\theta\)</span>, such as calculating credible intervals or point estimates (e.g., mean or median).</p>
</section>
<section id="python-implementation-with-pymc" class="level3" data-number="12.1.5">
<h3 data-number="12.1.5" class="anchored" data-anchor-id="python-implementation-with-pymc"><span class="header-section-number">12.1.5</span> Python Implementation with PyMC</h3>
<div id="fca368d7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed data</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">6</span>  <span class="co"># Number of successes</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span> <span class="co"># Number of trials</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>alpha_prior <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>beta_prior <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior distribution</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> pm.Beta(<span class="st">"theta"</span>, alpha<span class="op">=</span>alpha_prior, beta<span class="op">=</span>beta_prior)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> pm.Binomial(<span class="st">"y_obs"</span>, p<span class="op">=</span>theta, n<span class="op">=</span>n, observed<span class="op">=</span>k)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior analysis</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(trace, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [theta]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 4 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-3-output-5.png" width="540" height="440" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="b92483a1" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior predictive distribution</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>ppc <span class="op">=</span> pm.sample_posterior_predictive(trace, var_names<span class="op">=</span>[<span class="st">"y_obs"</span>], model<span class="op">=</span>model)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ppc.keys())</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.hist(ppc.posterior_predictive[<span class="st">'y_obs'</span>], bins<span class="op">=</span><span class="bu">range</span>(n <span class="op">+</span> <span class="dv">2</span>))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of successes"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Frequency"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Posterior Predictive Distribution"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling: [y_obs]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>KeysView(Inference data with groups:
    &gt; posterior_predictive
    &gt; observed_data)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-4-output-5.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This PyMC code defines a Beta-Binomial model, samples from the posterior distribution using Hamiltonian Monte Carlo (HMC), and visualizes the results. The posterior plot shows the updated belief about <span class="math inline">\(\theta\)</span> after observing the data, and the posterior predictive distribution shows the probability of observing different numbers of successes in future experiments. Remember to install PyMC: <code>pip install pymc</code></p>
</section>
</section>
<section id="normal-normal-model" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="normal-normal-model"><span class="header-section-number">12.2</span> Normal-Normal Model</h2>
<section id="the-normal-likelihood" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="the-normal-likelihood"><span class="header-section-number">12.2.1</span> The Normal Likelihood</h3>
<p>We often model continuous data using the normal (Gaussian) distribution. Suppose we have a sample of <span class="math inline">\(n\)</span> data points, <span class="math inline">\(x_1, x_2, ..., x_n\)</span>, which are assumed to be independent and identically distributed (i.i.d.) from a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>. The likelihood function is given by:</p>
<p><span class="math inline">\(p(X|\mu, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left(-\frac{\sum_{i=1}^{n}(x_i - \mu)^2}{2\sigma^2}\right)\)</span></p>
<p>where <span class="math inline">\(X = (x_1, x_2, ..., x_n)\)</span>. This likelihood expresses the probability of observing the data given a specific value of <span class="math inline">\(\mu\)</span> and the known <span class="math inline">\(\sigma^2\)</span>.</p>
</section>
<section id="the-normal-prior" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="the-normal-prior"><span class="header-section-number">12.2.2</span> The Normal Prior</h3>
<p>A conjugate prior for the normal mean <span class="math inline">\(\mu\)</span> when the variance <span class="math inline">\(\sigma^2\)</span> is known is another normal distribution. We specify a prior distribution for <span class="math inline">\(\mu\)</span> as:</p>
<p><span class="math inline">\(p(\mu|\mu_0, \sigma_0^2) = \frac{1}{\sqrt{2\pi\sigma_0^2}} \exp\left(-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}\right)\)</span></p>
<p>where <span class="math inline">\(\mu_0\)</span> represents our prior belief about the mean, and <span class="math inline">\(\sigma_0^2\)</span> represents the uncertainty in our prior belief. A smaller <span class="math inline">\(\sigma_0^2\)</span> indicates a stronger prior belief.</p>
</section>
<section id="posterior-distribution-derivation-1" class="level3" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="posterior-distribution-derivation-1"><span class="header-section-number">12.2.3</span> Posterior Distribution Derivation</h3>
<p>Using Bayes’ theorem, the posterior distribution is proportional to the product of the likelihood and the prior:</p>
<p><span class="math inline">\(p(\mu|X, \mu_0, \sigma_0^2, \sigma^2) \propto p(X|\mu, \sigma^2) p(\mu|\mu_0, \sigma_0^2)\)</span></p>
<p>After some algebraic manipulation (completing the square), we find that the posterior distribution is also a normal distribution:</p>
<p><span class="math inline">\(p(\mu|X, \mu_0, \sigma_0^2, \sigma^2) = N(\mu_n, \sigma_n^2)\)</span></p>
<p>where:</p>
<p><span class="math inline">\(\mu_n = \frac{\frac{\mu_0}{\sigma_0^2} + \frac{n\bar{x}}{\sigma^2}}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}\)</span></p>
<p><span class="math inline">\(\sigma_n^2 = \frac{1}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}\)</span></p>
<p>Here, <span class="math inline">\(\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i\)</span> is the sample mean. The posterior mean <span class="math inline">\(\mu_n\)</span> is a weighted average of the prior mean <span class="math inline">\(\mu_0\)</span> and the sample mean <span class="math inline">\(\bar{x}\)</span>, with weights inversely proportional to their respective variances. The posterior variance <span class="math inline">\(\sigma_n^2\)</span> is smaller than both the prior and the sampling variance, reflecting the reduction in uncertainty after observing the data.</p>
</section>
<section id="bayesian-inference-with-normal-normal" class="level3" data-number="12.2.4">
<h3 data-number="12.2.4" class="anchored" data-anchor-id="bayesian-inference-with-normal-normal"><span class="header-section-number">12.2.4</span> Bayesian Inference with Normal-Normal</h3>
<p>The Normal-Normal model provides a straightforward way to update our beliefs about the mean of a normal distribution. The posterior distribution summarizes our updated beliefs after incorporating the data. We can obtain point estimates (e.g., posterior mean <span class="math inline">\(\mu_n\)</span>) and credible intervals to quantify uncertainty.</p>
</section>
<section id="python-implementation-with-pymc-1" class="level3" data-number="12.2.5">
<h3 data-number="12.2.5" class="anchored" data-anchor-id="python-implementation-with-pymc-1"><span class="header-section-number">12.2.5</span> Python Implementation with PyMC</h3>
<div id="fa9f7cf6" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed data</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>mu_prior <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>sigma_prior <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span> <span class="co"># Known standard deviation</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> pm.Normal(<span class="st">"mu"</span>, mu<span class="op">=</span>mu_prior, sigma<span class="op">=</span>sigma_prior)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> pm.Normal(<span class="st">"y"</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>data)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(trace, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 4 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-5-output-5.png" width="546" height="440" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code implements the Normal-Normal model in PyMC. The posterior distribution of <span class="math inline">\(\mu\)</span> is visualized.</p>
</section>
<section id="handling-unknown-variance" class="level3" data-number="12.2.6">
<h3 data-number="12.2.6" class="anchored" data-anchor-id="handling-unknown-variance"><span class="header-section-number">12.2.6</span> Handling Unknown Variance</h3>
<p>When the variance <span class="math inline">\(\sigma^2\)</span> is unknown, we need to introduce a prior for it. A common choice is the Inverse-Gamma distribution, which is conjugate to the normal likelihood for the variance. This makes the model more realistic but also more complex analytically. The complete model with unknown variance would require using techniques like MCMC sampling, which PyMC handles efficiently:</p>
<div id="ee48c2e6" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed data</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Priors</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> pm.Normal(<span class="st">"mu"</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)  <span class="co"># Weak prior on mu</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pm.HalfCauchy(<span class="st">"sigma"</span>, beta<span class="op">=</span><span class="dv">5</span>) <span class="co"># Prior on sigma</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> pm.Normal(<span class="st">"y"</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>data)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">1000</span>, cores<span class="op">=</span><span class="dv">1</span>) <span class="co"># adjust cores as needed</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(trace, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Sequential sampling (2 chains in 1 job)
NUTS: [mu, sigma]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 2 chains for 1_000 tune and 10_000 draw iterations (2_000 + 20_000 draws total) took 7 seconds.
We recommend running at least 4 chains for robust computation of convergence diagnostics</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-6-output-5.png" width="1219" height="496" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This example uses a Half-Cauchy prior for <span class="math inline">\(\sigma\)</span>, a common and relatively non-informative prior for scale parameters. Remember to adjust the number of cores (<code>cores=</code>) based on your system’s capabilities. The Half-Cauchy prior is used to avoid problems that can arise with using an Inverse Gamma prior with improper priors. It is important to be mindful of the effect that your chosen prior has on the posterior results.</p>
</section>
</section>
<section id="dirichlet-multinomial-model" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="dirichlet-multinomial-model"><span class="header-section-number">12.3</span> Dirichlet-Multinomial Model</h2>
<section id="the-multinomial-likelihood" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="the-multinomial-likelihood"><span class="header-section-number">12.3.1</span> The Multinomial Likelihood</h3>
<p>The multinomial distribution is a generalization of the binomial distribution to more than two outcomes. Suppose we have <span class="math inline">\(k\)</span> categories and perform <span class="math inline">\(n\)</span> independent trials. Let <span class="math inline">\(x_i\)</span> be the number of trials resulting in category <span class="math inline">\(i\)</span>, where <span class="math inline">\(\sum_{i=1}^{k} x_i = n\)</span>. Let <span class="math inline">\(\theta_i\)</span> be the probability of a trial resulting in category <span class="math inline">\(i\)</span>, where <span class="math inline">\(\sum_{i=1}^{k} \theta_i = 1\)</span>. The multinomial likelihood is given by:</p>
<p><span class="math inline">\(p(x_1, ..., x_k | \theta_1, ..., \theta_k, n) = \binom{n}{x_1, ..., x_k} \prod_{i=1}^{k} \theta_i^{x_i}\)</span></p>
<p>where <span class="math inline">\(\binom{n}{x_1, ..., x_k} = \frac{n!}{x_1! ... x_k!}\)</span> is the multinomial coefficient.</p>
</section>
<section id="the-dirichlet-prior" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="the-dirichlet-prior"><span class="header-section-number">12.3.2</span> The Dirichlet Prior</h3>
<p>The Dirichlet distribution is a conjugate prior for the multinomial distribution. It is defined over the <span class="math inline">\(k\)</span>-dimensional probability simplex, i.e., the set of vectors <span class="math inline">\(\theta = (\theta_1, ..., \theta_k)\)</span> such that <span class="math inline">\(\theta_i \ge 0\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(\sum_{i=1}^{k} \theta_i = 1\)</span>. The Dirichlet distribution is parameterized by a vector of <span class="math inline">\(k\)</span> positive concentration parameters, <span class="math inline">\(\alpha = (\alpha_1, ..., \alpha_k)\)</span>:</p>
<p><span class="math inline">\(p(\theta_1, ..., \theta_k | \alpha_1, ..., \alpha_k) = \frac{1}{B(\alpha)} \prod_{i=1}^{k} \theta_i^{\alpha_i - 1}\)</span></p>
<p>where <span class="math inline">\(B(\alpha) = \frac{\prod_{i=1}^{k} \Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^{k} \alpha_i)}\)</span> is the multivariate Beta function. The <span class="math inline">\(\alpha_i\)</span> parameters influence the shape of the distribution. Larger values of <span class="math inline">\(\alpha_i\)</span> lead to higher probability density around <span class="math inline">\(\theta_i = \frac{\alpha_i}{\sum_{j=1}^{k} \alpha_j}\)</span>.</p>
</section>
<section id="posterior-distribution-derivation-2" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="posterior-distribution-derivation-2"><span class="header-section-number">12.3.3</span> Posterior Distribution Derivation</h3>
<p>Since the Dirichlet is conjugate to the multinomial, the posterior distribution is also a Dirichlet distribution. The posterior parameters are updated by simply adding the observed counts to the prior parameters:</p>
<p><span class="math inline">\(p(\theta_1, ..., \theta_k | x_1, ..., x_k, \alpha_1, ..., \alpha_k) = Dirichlet(\alpha_1 + x_1, ..., \alpha_k + x_k)\)</span></p>
</section>
<section id="bayesian-inference-with-dirichlet-multinomial" class="level3" data-number="12.3.4">
<h3 data-number="12.3.4" class="anchored" data-anchor-id="bayesian-inference-with-dirichlet-multinomial"><span class="header-section-number">12.3.4</span> Bayesian Inference with Dirichlet-Multinomial</h3>
<p>The Dirichlet-Multinomial model allows us to perform Bayesian inference on the parameters of a multinomial distribution. Starting with a Dirichlet prior reflecting our prior beliefs about the category probabilities, we update our beliefs using the observed counts to obtain a posterior Dirichlet distribution. We can use this posterior to make inferences, such as calculating credible intervals for each <span class="math inline">\(\theta_i\)</span> or predicting the counts for future trials.</p>
</section>
<section id="python-implementation-with-pymc-2" class="level3" data-number="12.3.5">
<h3 data-number="12.3.5" class="anchored" data-anchor-id="python-implementation-with-pymc-2"><span class="header-section-number">12.3.5</span> Python Implementation with PyMC</h3>
<div id="c80f12f6" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed data (counts for each category)</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>observed_counts <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="bu">len</span>(observed_counts)  <span class="co"># Number of categories</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters (concentration parameters) - weakly informative prior</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>alpha_prior <span class="op">=</span> np.ones(k)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior distribution</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> pm.Dirichlet(<span class="st">"theta"</span>, a<span class="op">=</span>alpha_prior)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> pm.Multinomial(<span class="st">"y"</span>, n<span class="op">=</span><span class="bu">sum</span>(observed_counts), p<span class="op">=</span>theta, observed<span class="op">=</span>observed_counts)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(trace, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/pytensor/link/c/cmodule.py:2959: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.
This usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.
Alternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.
For more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library
  warnings.warn(
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [theta]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 7 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-7-output-5.png" width="2415" height="517" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code implements the Dirichlet-Multinomial model in PyMC. Remember to install PyMC (<code>pip install pymc</code>). The posterior distributions for each <span class="math inline">\(\theta_i\)</span> are shown.</p>
</section>
<section id="applications-in-text-mining-and-other-fields" class="level3" data-number="12.3.6">
<h3 data-number="12.3.6" class="anchored" data-anchor-id="applications-in-text-mining-and-other-fields"><span class="header-section-number">12.3.6</span> Applications in Text Mining and other fields</h3>
<p>The Dirichlet-Multinomial model finds widespread applications:</p>
<ul>
<li><p><strong>Text mining:</strong> Modeling the distribution of words in documents. Each category represents a word, and the <span class="math inline">\(\theta_i\)</span> represent the probability of each word appearing in a document. The Dirichlet prior helps smooth the word probabilities, preventing zero probabilities for unseen words (especially helpful with large vocabularies).</p></li>
<li><p><strong>Topic modeling:</strong> Identifying latent topics in a collection of documents. Each topic is a multinomial distribution over words, and the Dirichlet prior is used to control the sparsity of topic distributions.</p></li>
<li><p><strong>Recommender systems:</strong> Modeling user preferences over items. Each item is a category, and the <span class="math inline">\(\theta_i\)</span> represent the probability that a user will choose a given item.</p></li>
<li><p><strong>Genetics:</strong> Modeling allele frequencies in populations. Each category is an allele, and <span class="math inline">\(\theta_i\)</span> is the frequency of that allele.</p></li>
<li><p><strong>Ecology:</strong> Species abundance in an ecosystem, where each category is a species.</p></li>
</ul>
<p>The flexibility and analytical tractability of the Dirichlet-Multinomial model make it a valuable tool in various fields involving categorical data analysis.</p>
</section>
</section>
<section id="beyond-the-basic-models" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="beyond-the-basic-models"><span class="header-section-number">12.4</span> Beyond the Basic Models</h2>
<section id="other-conjugate-prior-pairs" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="other-conjugate-prior-pairs"><span class="header-section-number">12.4.1</span> Other Conjugate Prior Pairs</h3>
<p>While the Beta-Binomial, Normal-Normal, and Dirichlet-Multinomial models are frequently used, many other conjugate prior pairs exist, each tailored to a specific likelihood function. Here are a few examples:</p>
<ul>
<li><p><strong>Gamma-Poisson:</strong> The Gamma distribution is a conjugate prior for the Poisson likelihood. If <span class="math inline">\(X \sim Poisson(\lambda)\)</span>, and we use a Gamma prior for <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(p(\lambda|\alpha, \beta) = Gamma(\alpha, \beta)\)</span>, then the posterior is also a Gamma distribution with updated parameters.</p></li>
<li><p><strong>Inverse Gamma-Normal (for variance):</strong> The Inverse Gamma distribution is a conjugate prior for the variance of a normal distribution when the mean is known. If <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> and <span class="math inline">\(\mu\)</span> is known, using an Inverse Gamma prior for <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(p(\sigma^2 | \alpha, \beta) = InvGamma(\alpha, \beta)\)</span>, yields an Inverse Gamma posterior.</p></li>
<li><p><strong>Normal-Inverse Gamma:</strong> This is a bivariate conjugate prior for the mean and variance of a normal distribution. The prior is a combination of a normal distribution for the mean (conditional on the variance) and an inverse gamma distribution for the variance.</p></li>
</ul>
<p>These examples highlight the versatility of conjugate priors. The availability of a conjugate prior greatly simplifies Bayesian inference, but it’s essential to select the appropriate pair that matches the likelihood function and reflects your prior knowledge accurately.</p>
</section>
<section id="choosing-appropriate-conjugate-priors" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="choosing-appropriate-conjugate-priors"><span class="header-section-number">12.4.2</span> Choosing Appropriate Conjugate Priors</h3>
<p>Selecting the right conjugate prior involves many considerations:</p>
<ol type="1">
<li><p><strong>Likelihood Function:</strong> The choice of prior is fundamentally determined by the likelihood function of your data. Only certain prior distributions form conjugate pairs with specific likelihoods.</p></li>
<li><p><strong>Prior Knowledge:</strong> The prior should reflect your prior beliefs or existing knowledge about the parameter. If you have strong prior information, choose a prior that incorporates this information effectively. If you have little or no prior information, consider weakly informative priors that avoid unduly influencing the posterior.</p></li>
<li><p><strong>Computational Tractability:</strong> While all conjugate priors lead to analytically tractable posteriors, some might be easier to work with than others depending on the complexity of the expressions involved.</p></li>
<li><p><strong>Interpretability:</strong> The parameters of the chosen prior should be easily interpretable in the context of your problem. This allows for easier communication and understanding of the Bayesian analysis.</p></li>
</ol>
<p>Choosing an inappropriate conjugate prior can lead to inaccurate or misleading results. It is essential to carefully consider the implications of the prior choice and to justify its selection.</p>
</section>
<section id="approximations-when-conjugate-priors-are-unavailable" class="level3" data-number="12.4.3">
<h3 data-number="12.4.3" class="anchored" data-anchor-id="approximations-when-conjugate-priors-are-unavailable"><span class="header-section-number">12.4.3</span> Approximations when Conjugate Priors are Unavailable</h3>
<p>In many practical situations, no conjugate prior exists for the given likelihood. In such cases, many approximation techniques can be employed:</p>
<ol type="1">
<li><p><strong>Laplace Approximation:</strong> This method approximates the posterior distribution with a normal distribution centered at the mode of the posterior density. It’s relatively simple but might not be accurate if the posterior is highly skewed or multimodal.</p></li>
<li><p><strong>Variational Inference:</strong> This technique approximates the true posterior with a simpler, tractable distribution from a specified family (e.g., a mean-field approximation). It offers flexibility but can be computationally intensive.</p></li>
<li><p><strong>Markov Chain Monte Carlo (MCMC):</strong> MCMC methods, such as Hamiltonian Monte Carlo (HMC) or Metropolis-Hastings, are powerful tools for sampling from complex, high-dimensional posterior distributions. They are computationally intensive but generally provide accurate approximations. Libraries like PyMC automate these methods.</p></li>
</ol>
<div id="6219742a" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Example using PyMC for a non-conjugate scenario (illustrative)</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Assume a likelihood that does not have a simple conjugate prior</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate some data (Example: a mixture of Gaussians)</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>data1 <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>data2 <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">5</span>, scale<span class="op">=</span><span class="fl">0.5</span>, size<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.concatenate((data1, data2))</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Priors (Non-conjugate Example)</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> pm.Normal(<span class="st">"mu"</span>, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> pm.HalfCauchy(<span class="st">"sigma"</span>, beta<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood (Example: mixture of gaussians, not directly conjugate)</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> pm.Normal(<span class="st">"y"</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>data)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample the posterior using MCMC</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(trace, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu, sigma]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 4 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-8-output-5.png" width="1229" height="496" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This PyMC code demonstrates how to handle a non-conjugate scenario by using MCMC sampling. The example employs a simple normal likelihood, but the principle extends to more complex non-conjugate situations. Note that the choice of priors is essential even in non-conjugate settings. The selection process should still consider prior knowledge and avoid overly strong or overly weak priors.</p>
</section>
</section>
<section id="case-studies-and-applications" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="case-studies-and-applications"><span class="header-section-number">12.5</span> Case Studies and Applications</h2>
<section id="ab-testing-with-beta-binomial" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="ab-testing-with-beta-binomial"><span class="header-section-number">12.5.1</span> A/B Testing with Beta-Binomial</h3>
<p>A/B testing is a common method for comparing two versions of a webpage, advertisement, or other item to determine which performs better. The Beta-Binomial model provides a powerful Bayesian framework for analyzing A/B test results.</p>
<p>Let’s say we have two versions (A and B) of a webpage, and we want to determine which has a higher conversion rate (e.g., the proportion of visitors who make a purchase). We can model the conversion rate for each version using a Beta-Binomial model:</p>
<ul>
<li><p><strong>Version A:</strong> Let <span class="math inline">\(\theta_A\)</span> be the conversion rate for version A. We place a Beta prior on <span class="math inline">\(\theta_A\)</span>, <span class="math inline">\(p(\theta_A|\alpha_{A0}, \beta_{A0}) = Beta(\alpha_{A0}, \beta_{A0})\)</span>. After observing <span class="math inline">\(k_A\)</span> conversions out of <span class="math inline">\(n_A\)</span> visitors, the posterior is <span class="math inline">\(p(\theta_A|k_A, n_A, \alpha_{A0}, \beta_{A0}) = Beta(\alpha_{A0} + k_A, \beta_{A0} + n_A - k_A)\)</span>.</p></li>
<li><p><strong>Version B:</strong> Similarly, for version B, we have <span class="math inline">\(\theta_B\)</span>, a Beta prior <span class="math inline">\(p(\theta_B|\alpha_{B0}, \beta_{B0})\)</span>, and a posterior <span class="math inline">\(p(\theta_B|k_B, n_B, \alpha_{B0}, \beta_{B0}) = Beta(\alpha_{B0} + k_B, \beta_{B0} + n_B - k_B)\)</span>.</p></li>
</ul>
<p>We can then compare the posterior distributions of <span class="math inline">\(\theta_A\)</span> and <span class="math inline">\(\theta_B\)</span> to determine which version has a higher conversion rate. We might compute the probability that <span class="math inline">\(\theta_A &gt; \theta_B\)</span> using posterior samples.</p>
<div id="7ffd37db" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># A/B testing data</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>kA <span class="op">=</span> <span class="dv">15</span>  <span class="co"># Conversions for version A</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>nA <span class="op">=</span> <span class="dv">100</span> <span class="co"># Visitors for version A</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>kB <span class="op">=</span> <span class="dv">20</span>  <span class="co"># Conversions for version B</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>nB <span class="op">=</span> <span class="dv">100</span> <span class="co"># Visitors for version B</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Weakly informative priors</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>alpha_prior <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>beta_prior <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Priors</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    theta_A <span class="op">=</span> pm.Beta(<span class="st">"theta_A"</span>, alpha<span class="op">=</span>alpha_prior, beta<span class="op">=</span>beta_prior)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    theta_B <span class="op">=</span> pm.Beta(<span class="st">"theta_B"</span>, alpha<span class="op">=</span>alpha_prior, beta<span class="op">=</span>beta_prior)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihoods</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    obs_A <span class="op">=</span> pm.Binomial(<span class="st">"obs_A"</span>, p<span class="op">=</span>theta_A, n<span class="op">=</span>nA, observed<span class="op">=</span>kA)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    obs_B <span class="op">=</span> pm.Binomial(<span class="st">"obs_B"</span>, p<span class="op">=</span>theta_B, n<span class="op">=</span>nB, observed<span class="op">=</span>kB)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(trace, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability that theta_A &gt; theta_B</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>prob_A_gt_B <span class="op">=</span> np.mean(trace.posterior[<span class="st">"theta_A"</span>] <span class="op">&gt;</span> trace.posterior[<span class="st">"theta_B"</span>])</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(theta_A &gt; theta_B) = </span><span class="sc">{</span>prob_A_gt_B<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [theta_A, theta_B]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 4 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-9-output-5.png" width="1219" height="496" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>P(theta_A &gt; theta_B) = &lt;xarray.DataArray ()&gt; Size: 8B
array(0.1827)</code></pre>
</div>
</div>
</section>
<section id="estimating-population-means-with-normal-normal" class="level3" data-number="12.5.2">
<h3 data-number="12.5.2" class="anchored" data-anchor-id="estimating-population-means-with-normal-normal"><span class="header-section-number">12.5.2</span> Estimating Population Means with Normal-Normal</h3>
<p>Suppose we want to estimate the average height of adult women in a certain city. We collect a random sample of <span class="math inline">\(n\)</span> heights, <span class="math inline">\(x_1, x_2, ..., x_n\)</span>. We can model the heights using a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We use a normal prior for <span class="math inline">\(\mu\)</span>, <span class="math inline">\(p(\mu|\mu_0, \sigma_0^2) = N(\mu_0, \sigma_0^2)\)</span>, reflecting our prior belief about the average height. The posterior distribution is also normal, with parameters derived as described in the “Normal-Normal Model” section.</p>
<div id="df92147b" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample heights (simulated data)</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>heights <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">165</span>, scale<span class="op">=</span><span class="dv">5</span>, size<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters (weak prior)</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>mu_prior <span class="op">=</span> <span class="dv">170</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>sigma_prior <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Known standard deviation</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prior</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> pm.Normal(<span class="st">"mu"</span>, mu<span class="op">=</span>mu_prior, sigma<span class="op">=</span>sigma_prior)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> pm.Normal(<span class="st">"y"</span>, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>heights)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(<span class="dv">10000</span>, tune<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>pm.summary(trace)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>pm.plot_posterior(trace, hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [mu]</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/home/leopard/development/QuantumTraderX/venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 4 seconds.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="conjugate-priors_files/figure-html/cell-10-output-5.png" width="540" height="440" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This code estimates the population mean height using the Normal-Normal model.</p>
</section>
<section id="topic-modeling-with-dirichlet-multinomial" class="level3" data-number="12.5.3">
<h3 data-number="12.5.3" class="anchored" data-anchor-id="topic-modeling-with-dirichlet-multinomial"><span class="header-section-number">12.5.3</span> Topic Modeling with Dirichlet-Multinomial</h3>
<p>Topic modeling aims to discover underlying thematic structures (topics) in a collection of documents. The Latent Dirichlet Allocation (LDA) model is a probabilistic approach that utilizes the Dirichlet-Multinomial framework.</p>
<p>In LDA:</p>
<ul>
<li>Each document is represented as a mixture of topics.</li>
<li>Each topic is a probability distribution over words.</li>
<li>The distribution of topics in a document follows a Dirichlet distribution.</li>
<li>The distribution of words given a topic follows a multinomial distribution.</li>
</ul>
<p>The Dirichlet-Multinomial model forms the core of this generative process. Inference in LDA typically involves MCMC methods, which are computationally intensive. Libraries like <code>gensim</code> provide efficient implementations. Here’s a conceptual outline using PyMC, but it won’t be a fully functional LDA implementation due to the high dimensionality and computational challenges of this model:</p>
<div id="ff214679" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: This is a simplified conceptual illustration and NOT a full LDA implementation</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Full LDA implementation requires specialized libraries like gensim</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified Example:  Assume we have 2 topics and 3 words</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of documents</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>n_docs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of words per document (simplified example, varies in real LDA)</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>n_words <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co">#Simulate some data (This is a placeholder. Real data requires preprocessing)</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>word_counts <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">10</span>, size<span class="op">=</span>(n_docs, <span class="dv">3</span>))</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Priors (Weak priors)</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> pm.HalfCauchy(<span class="st">"alpha"</span>, beta<span class="op">=</span><span class="dv">1</span>, shape<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Prior for topic distribution</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> pm.Dirichlet(<span class="st">"beta"</span>, a<span class="op">=</span>np.ones(<span class="dv">3</span>), shape<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Prior for word distribution per topic</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Likelihood (This is a placeholder, actual LDA has more complex structure)</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ...  (The likelihood would require modeling topic assignment and word generation) ...</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior sampling (Would require complex samplers for full LDA)</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (Sampling process would be much more involved for full LDA) ...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This conceptual example illustrates the role of Dirichlet and Multinomial distributions within LDA. For real-world topic modeling, using specialized libraries such as <code>gensim</code> is strongly recommended due to the computational complexity of LDA inference. These libraries often employ optimized variational inference or Gibbs sampling for efficient posterior approximation.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../parts/bayesian-inference/parameter-estimation.html" class="pagination-link" aria-label="Introduction to Parameter Estimation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Parameter Estimation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../parts/bayesian-inference/prior-selection.html" class="pagination-link" aria-label="Prior Selection">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Prior Selection</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>