{"title":"Implementation with PyMC3","markdown":{"headingText":"Implementation with PyMC3","containsRefs":false,"markdown":"\nThis chapter demonstrates how to implement Bayesian inference using PyMC3, a powerful probabilistic programming library in Python. We will walk through a concrete example, illustrating each step of the process.  Assume we are trying to model the relationship between advertising expenditure ($X$) and sales ($Y$).\n\n\n### Defining Variables and Distributions\n\nFirst, we need to define the variables in our model. We'll assume a linear relationship between advertising expenditure and sales, with normally distributed noise.  In PyMC3, this is achieved by defining stochastic variables representing our model parameters and data.\n\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sample data (replace with your actual data)\nnp.random.seed(42)\nX = np.linspace(0, 10, 20)\ntrue_slope = 2.5\ntrue_intercept = 5\nY = true_slope * X + true_intercept + np.random.normal(0, 2, 20)\n\n# Plotting the data\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(8,6))\nplt.scatter(X, Y, label='Observed Data')\nplt.xlabel('Advertising Expenditure (X)')\nplt.ylabel('Sales (Y)')\nplt.title('Observed Data Points')\nplt.legend()\nplt.show()\n\nwith pm.Model() as model:\n    # Priors for the slope and intercept\n    slope = pm.Normal(\"slope\", mu=0, sigma=10)  \n    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n\n    # Likelihood\n    sigma = pm.HalfNormal(\"sigma\", sigma=5) # Prior for the standard deviation\n    mu = slope * X + intercept\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=Y)\n```\n\nHere, `slope` and `intercept` are our model parameters. We've assigned them normal prior distributions with mean 0 and standard deviation 10, reflecting our initial uncertainty.  `sigma` represents the standard deviation of the noise in our model, and is given a HalfNormal prior to ensure it's positive. `y_obs` represents the observed data, and is assigned a normal likelihood distribution. The `observed=Y` argument links the model to our data.\n\n\n### Specifying Prior Distributions\n\nThe choice of prior distributions is crucial.  Above, we used normal priors for the slope and intercept. The choice reflects our belief that the true values are centered around zero, with a relatively large uncertainty (sigma = 10). The HalfNormal prior for sigma ensures a positive standard deviation. The selection of prior distributions often relies on prior knowledge, or may employ uninformative (weakly informative) priors if prior knowledge is lacking.  For example, more informative priors might be:\n\n* **Slope:**  `pm.Normal(\"slope\", mu=2, sigma=2)` (if we suspect a positive slope around 2)\n* **Intercept:** `pm.Normal(\"intercept\", mu=5, sigma=1)` (if we expect an intercept around 5)\n\n\n### Building the Likelihood Function\n\nThe likelihood function specifies how likely our observed data is, given the model parameters.  In our linear regression example, we assume the data follows a normal distribution:\n\n$p(Y|X, \\text{slope}, \\text{intercept}, \\sigma) = \\mathcal{N}(Y | \\text{slope} \\cdot X + \\text{intercept}, \\sigma)$\n\nThis is represented in the PyMC3 code above by the line `y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=Y)`.  The `observed=Y` argument connects the likelihood function to our actual data.\n\n\n### Deterministic Transformations\n\nSometimes, we might need to define deterministic transformations of our variables.  For instance, we could calculate the $R^2$ value as a deterministic function of the model parameters:\n\n```\\{python}\n#| echo: true\nwith model:\n    y_pred = slope * X + intercept\n    residuals = Y - y_pred\n    ss_res = np.sum(residuals**2)\n    ss_tot = np.sum((Y-np.mean(Y))**2)\n    R2 = pm.Deterministic(\"R2\", 1 - (ss_res / ss_tot))\n```\n\nThis adds a new deterministic variable `R2` to our model.\n\n\n### Model Compilation and Summary\n\nFinally, we compile the model and draw samples using Markov Chain Monte Carlo (MCMC):\n\n\n```\\{python}\n#| echo: true\nwith model:\n    trace = pm.sample(draws=4000, tune=1000, cores=1) #Adjust cores based on your system\n\npm.summary(trace)\npm.traceplot(trace)\nplt.show()\n```\n\n`pm.sample` performs the MCMC sampling.  `draws` specifies the number of samples to generate, and `tune` represents the number of tuning samples (used to adapt the sampler). `pm.summary` provides a summary of the posterior distributions, and `pm.traceplot` visualizes the MCMC chains.\n\nThe `pm.summary` output shows the mean, standard deviation, credible intervals, and other statistics for the posterior distributions of our model parameters. The `pm.traceplot` shows the trace plots of the sampled parameters, allowing for visual assessment of convergence.  If the chains appear well-mixed and have converged, the results are reliable.  Note that you might need to adjust the number of draws and tune samples based on the complexity of your model and data.\n\n```{mermaid}\ngraph LR\nA[Prior Distributions] --> B(Model Specification);\nC[Data] --> B;\nB --> D{MCMC Sampling};\nD --> E[Posterior Distributions];\nE --> F[Inference & Prediction];\n```\n\n\n## Implementation with PyMC3\n\nThis chapter delves into the sampling methods employed by PyMC3 to perform Bayesian inference.  Understanding these methods is crucial for effectively using PyMC3 and interpreting its results.\n\n\n### Introduction to Markov Chain Monte Carlo (MCMC)\n\nBayesian inference aims to obtain the posterior distribution $p(\\theta|D)$, where $\\theta$ represents the model parameters and $D$ is the observed data.  Often, this posterior is intractable analytically.  Markov Chain Monte Carlo (MCMC) methods provide a computational solution by constructing a Markov chain whose stationary distribution is the target posterior distribution.  By simulating this chain for a sufficiently long time, we can obtain samples that approximate the posterior.  These samples then allow us to estimate posterior quantities of interest, such as means, credible intervals, and other summary statistics.\n\n\n### Metropolis-Hastings Algorithm\n\nThe Metropolis-Hastings algorithm is a fundamental MCMC method. It works by iteratively proposing new parameter values and accepting or rejecting them based on a probability that depends on the ratio of the posterior density at the proposed and current values.\n\nThe algorithm proceeds as follows:\n\n1. **Initialization:** Start with an initial guess for the parameters $\\theta^{(0)}$.\n\n2. **Proposal:** Generate a proposed state $\\theta^*$ from a proposal distribution $q(\\theta^* | \\theta^{(t)})$, where $\\theta^{(t)}$ is the current state. Common choices for $q$ include Gaussian distributions.\n\n3. **Acceptance:** Calculate the acceptance probability:\n\n   $\\alpha = \\min\\left(1, \\frac{p(\\theta^*|D)q(\\theta^{(t)}|\\theta^*)}{p(\\theta^{(t)}|D)q(\\theta^*|\\theta^{(t)})}\\right)$\n\n   where $p(\\theta|D)$ is the target posterior distribution.\n\n4. **Update:** Generate a uniform random number $u \\sim U(0, 1)$.\n   * If $u < \\alpha$, accept the proposal and set $\\theta^{(t+1)} = \\theta^*$.\n   * Otherwise, reject the proposal and set $\\theta^{(t+1)} = \\theta^{(t)}$.\n\n5. **Iteration:** Repeat steps 2-4 for a large number of iterations. The samples $\\theta^{(t)}$ after a burn-in period (initial iterations discarded) approximate the target posterior.\n\n\n### Hamiltonian Monte Carlo (HMC)\n\nHMC is a more advanced MCMC method that leverages Hamiltonian dynamics to efficiently explore the target posterior distribution.  It's particularly useful for high-dimensional problems where simple methods like Metropolis-Hastings can struggle. HMC introduces auxiliary momentum variables and simulates the Hamiltonian dynamics of the system to generate proposals that are more likely to be accepted and explore the parameter space more effectively. The details of the Hamiltonian dynamics are beyond the scope of this brief introduction, but it essentially utilizes the gradient of the log-posterior to guide the sampling process.\n\n\n### No-U-Turn Sampler (NUTS)\n\nNUTS is an extension of HMC that automatically tunes the parameters of HMC, making it very robust and widely applicable.  It avoids the manual tuning of HMC's parameters (like step size and number of steps) by adaptively determining the appropriate trajectory length in the Hamiltonian dynamics. This adaptive nature makes NUTS a popular and often default choice in PyMC3.\n\n\n### Choosing an Appropriate Sampler\n\nPyMC3 offers several samplers.  NUTS is a good default choice for many problems due to its automatic tuning.  However, for simpler models or when specific properties are needed, other samplers might be more suitable.\n\n\n### Sampling Strategies and Tuning Parameters\n\nEffective MCMC requires careful consideration of sampling strategies and tuning parameters.  Here are some key aspects:\n\n* **Burn-in:**  Discard initial samples (burn-in period) to allow the Markov chain to converge to the stationary distribution.  PyMC3 handles this automatically but inspecting the traceplots is crucial to ensure sufficient burn-in.\n\n* **Number of Samples:**  The number of samples directly impacts accuracy.  More samples generally improve accuracy, but increase computation time.\n\n* **Thinning:**  To reduce autocorrelation between samples, thinning is sometimes employed. This involves selecting only every *k*th sample from the chain.  PyMC3 can handle this automatically based on effective sample size calculations.\n\n* **Parallel Sampling:**  PyMC3 can leverage multiple cores to run multiple chains in parallel.  This speeds up sampling, especially for complex models.\n\n* **Diagnostics:** Monitor convergence using traceplots (visual inspection for convergence and mixing) and Gelman-Rubin statistics (for comparing multiple chains).\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\n\n# Example: Simple Bayesian linear regression using NUTS\nnp.random.seed(42)\nX = np.linspace(0,10, 20)\nY = 2*X + 1 + np.random.normal(0,1,20)\n\nwith pm.Model() as model:\n    slope = pm.Normal(\"slope\", mu=0, sigma=10)\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n    mu = slope * X + intercept\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=Y)\n    \n    trace = pm.sample(draws=2000, tune=1000, cores=1, target_accept=0.95)  #NUTS is default\n\npm.traceplot(trace)\npm.summary(trace)\nplt.show()\n```\n\nThe `target_accept` parameter in `pm.sample` influences the acceptance rate of the NUTS sampler; values around 0.8-0.95 are usually good.  Experimentation and careful diagnostic checking are key to achieving reliable results.\n\n\n```{mermaid}\ngraph LR\nA[Problem Definition] --> B(Model Specification);\nB --> C[Sampler Selection];\nC --> D{MCMC Sampling (NUTS, HMC, etc.)};\nD --> E[Convergence Diagnostics];\nE -- Converged --> F[Posterior Inference];\nE -- Not Converged --> G[Adjust Parameters/Sampler];\nG --> D;\n```\n\n\n## Implementation with PyMC3\n\nThis chapter focuses on analyzing the results of PyMC3's MCMC sampling and assessing the quality of the generated samples.\n\n\n### Convergence Diagnostics\n\nBefore making any inferences based on the posterior samples, it is crucial to ensure that the Markov chains have converged to the target distribution.  Convergence diagnostics help assess whether the sampler has adequately explored the posterior and whether the samples are representative of the true posterior distribution.  Failure to check convergence can lead to inaccurate and misleading conclusions.\n\n\n### Trace Plots and Autocorrelation\n\nTrace plots visualize the sampled values of each parameter over the iterations of the MCMC algorithm. They show the evolution of the Markov chain.  Ideally, the trace should appear as a \"fuzzy caterpillar,\" indicating that the chain has explored the entire parameter space and not stuck in a particular region.  Autocorrelation plots show the correlation between samples separated by different lags (time differences).  High autocorrelation indicates that consecutive samples are strongly dependent, suggesting slow mixing and potentially insufficient exploration of the posterior.\n\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Example data (replace with your own)\nnp.random.seed(42)\ndata = np.random.normal(loc=5, scale=2, size=100)\n\nwith pm.Model() as model:\n    mu = pm.Normal('mu', mu=0, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=5)\n    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)\n    trace = pm.sample(1000, tune=1000)\n\npm.traceplot(trace);\nplt.show()\n\npm.autocorrplot(trace);\nplt.show()\n```\n\n\n### Gelman-Rubin Statistic (R-hat)\n\nThe Gelman-Rubin statistic ($\\hat{R}$) is a powerful convergence diagnostic that compares the variance within multiple Markov chains to the variance between them.  The statistic is calculated as:\n\n$\\hat{R} = \\frac{\\hat{Var}(\\theta)}{W}$\n\nwhere $\\hat{Var}(\\theta)$ is the estimated variance of the posterior distribution, and $W$ is the average of the within-chain variances.  A value of $\\hat{R}$ close to 1 (typically less than 1.1) indicates good convergence, suggesting that the chains have reached a similar distribution.  Values significantly greater than 1 suggest that the chains haven't converged and further sampling is needed.\n\n\n```\\{python}\n#| echo: true\npm.summary(trace) # R-hat is included in the summary\n```\n\nThe PyMC3 `pm.summary` function automatically calculates and reports $\\hat{R}$ for each parameter.\n\n\n### Effective Sample Size (ESS)\n\nThe effective sample size (ESS) measures the number of independent samples effectively obtained from the MCMC run, considering autocorrelation between samples.  A low ESS relative to the total number of samples indicates high autocorrelation and potentially poor mixing.  Ideally, we want a high ESS, indicating that the samples provide a good representation of the posterior distribution despite the correlation between samples.\n\n\n```\\{python}\n#| echo: true\npm.summary(trace) # ESS is included in the summary\n```\n\nPyMC3's `pm.summary` also reports the ESS for each parameter.\n\n\n### Assessing Mixing and Stationarity\n\nMixing refers to how well the MCMC chain explores the entire parameter space. Good mixing is characterized by rapid transitions between different regions of the posterior distribution.  Stationarity means that the Markov chain has reached its stationary distribution – the target posterior distribution – and is no longer changing significantly.  We assess mixing visually through trace plots and autocorrelation plots.  Stationarity is assessed using $\\hat{R}$ and by checking whether the trace plots show stability and lack of long-term trends.  A combination of visual inspection and quantitative diagnostics like $\\hat{R}$ and ESS is crucial for a complete assessment of convergence.\n\n\n\n```{mermaid}\ngraph LR\nA[MCMC Sampling] --> B(Trace Plots);\nA --> C(Autocorrelation Plots);\nB --> D[Gelman-Rubin Statistic (R-hat)];\nC --> D;\nB --> E(Effective Sample Size (ESS));\nC --> E;\nD -- R-hat ≈ 1 & ESS high --> F[Convergence Achieved];\nD -- R-hat > 1.1 or ESS low --> G[Sampling Issues];\nG --> H[Increase Samples/Tune Parameters];\nH --> A;\n```\n\n\n\n## Implementation with PyMC3\n\nThis chapter demonstrates how to visualize and interpret the posterior distributions obtained using PyMC3, allowing for meaningful conclusions based on the Bayesian analysis.\n\n\n### Visualizing Posterior Distributions\n\nVisualizing the posterior distributions is crucial for understanding the uncertainty associated with the model parameters.  PyMC3 offers tools to create various plots that aid in this visualization.\n\n\n### Histograms and Density Plots\n\nHistograms and kernel density estimates (KDEs) provide a visual representation of the marginal posterior distributions for each parameter.  Histograms show the frequency of samples within specified bins, while KDEs provide a smoother estimate of the probability density function.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example data (replace with your own)\nnp.random.seed(42)\ndata = np.random.normal(loc=5, scale=2, size=100)\n\nwith pm.Model() as model:\n    mu = pm.Normal('mu', mu=0, sigma=10)\n    sigma = pm.HalfNormal('sigma', sigma=5)\n    y = pm.Normal('y', mu=mu, sigma=sigma, observed=data)\n    trace = pm.sample(1000, tune=1000)\n\npm.plot_posterior(trace, credible_interval=0.95);\nplt.show()\n\n#Alternative using seaborn\nsns.displot(trace['mu'], kind='kde', fill=True);\nplt.title(\"Posterior Density of Mu\");\nplt.show()\n\nsns.histplot(trace['mu'], kde=True);\nplt.title('Posterior Histogram of Mu');\nplt.show()\n```\n\n\n### Credible Intervals\n\nCredible intervals represent the range of values within which a parameter is likely to fall with a specified probability.  For example, a 95% credible interval means that there is a 95% probability that the true parameter value lies within that interval.  PyMC3's `pm.summary` function provides credible intervals by default.\n\n\n```\\{python}\n#| echo: true\npm.summary(trace)\n```\n\n\n\n### Posterior Predictive Checks\n\nPosterior predictive checks assess the goodness of fit of the model.  They involve generating new data from the posterior predictive distribution, $p(\\tilde{y}|y)$, and comparing these simulated data to the observed data. Discrepancies might indicate that the model is not adequately capturing some aspect of the data-generating process.\n\n\n```\\{python}\n#| echo: true\nwith model:\n    ppc = pm.sample_posterior_predictive(trace)\n\nplt.figure(figsize=(10, 5))\nsns.kdeplot(data, label='Observed Data')\nsns.kdeplot(ppc['y'].mean(axis=0), label='Posterior Predictive')\nplt.legend()\nplt.title(\"Posterior Predictive Check\")\nplt.show()\n\n```\n\nThe above generates simulated data from the posterior and plots its distribution along with the observed data distribution, allowing for visual comparison.\n\n\n### Interpreting Posterior Samples\n\nThe posterior samples provide a comprehensive picture of the uncertainty associated with the model parameters.  We can summarize these samples by calculating their mean, median, standard deviation, and credible intervals. These provide point estimates and measures of uncertainty.\n\n\n### Model Comparison and Selection\n\nWhen multiple models are considered, model comparison techniques are necessary to select the best-fitting model.  Common approaches include:\n\n* **Log-pointwise predictive density (LPPD):** This measures the average log-likelihood of the observed data under the posterior predictive distribution.  Higher LPPD values indicate better model fit.\n* **Watanabe-Akaike Information Criterion (WAIC):**  WAIC is a model selection criterion that accounts for model complexity and data fit.  Lower WAIC values suggest better models.\n* **Leave-one-out cross-validation (LOO-CV):**  LOO-CV assesses the predictive performance by iteratively leaving out one data point and predicting its value based on the remaining data.  Lower LOO-CV scores imply better predictive accuracy.\n\n\nPyMC3 provides functions (`pm.waic`, `pm.loo`) to calculate WAIC and LOO-CV scores.  Model comparison involves calculating these scores for competing models and selecting the model with the highest LPPD or the lowest WAIC/LOO-CV score.\n\n\n```{mermaid}\ngraph LR\nA[Posterior Samples] --> B(Summary Statistics);\nA --> C(Histograms/Density Plots);\nA --> D(Credible Intervals);\nA --> E(Posterior Predictive Checks);\nF[Multiple Models] --> G(Model Comparison Metrics);\nG --> H[Model Selection];\n```\n\n\n\n## Implementation with PyMC3\n\nThis chapter explores more advanced topics and capabilities within the PyMC3 framework, expanding its application beyond the basic examples.\n\n\n### Handling Missing Data\n\nBayesian methods are particularly well-suited for handling missing data.  Instead of discarding observations with missing values or using imputation techniques, we can directly incorporate the missing data mechanism into the model.  In PyMC3, this is done by treating the missing values as latent variables with appropriate prior distributions. The model then jointly estimates the model parameters and the missing data points.  The choice of prior distribution for the missing data often depends on the nature of the data and the assumed missing data mechanism.  For example, if the missing data is assumed to be Missing At Random (MAR), a suitable prior can be chosen based on the observed data.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\n\n# Example with missing data\nnp.random.seed(42)\nX = np.linspace(0, 10, 20)\ntrue_slope = 2.5\ntrue_intercept = 5\nY = true_slope * X + true_intercept + np.random.normal(0, 2, 20)\n\n# Introduce some missing data\nY[::3] = np.nan\n\nwith pm.Model() as model:\n    slope = pm.Normal(\"slope\", mu=0, sigma=10)\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n    mu = slope * X + intercept\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=Y)\n    trace = pm.sample(draws=4000, tune=1000)\n\npm.summary(trace)\n```\n\n\n### Hierarchical Models\n\nHierarchical models are powerful tools for analyzing data with a nested structure, such as data collected from multiple groups or individuals. They allow for sharing of information across groups, improving estimation efficiency and reducing uncertainty, especially when data for individual groups is limited. These models posit that parameters at a lower level (e.g., individual-level parameters) are drawn from a higher-level distribution (e.g., group-level distribution).  The higher-level distribution encapsulates the variation between groups.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\n\n# Example: Hierarchical model for multiple groups\nJ = 4  # Number of groups\nN = 10 # Observations per group\n\n#Simulate Data\ngroup_means = np.random.normal(0, 2, J)\ndata = [np.random.normal(mu, 1, N) for mu in group_means]\n\nwith pm.Model() as hierarchical_model:\n    mu_global = pm.Normal(\"mu_global\", mu=0, sigma=10) #Global mean (hyperparameter)\n    sigma_global = pm.HalfNormal(\"sigma_global\", sigma=5) # Global deviation (hyperparameter)\n    group_means = pm.Normal(\"group_means\", mu=mu_global, sigma=sigma_global, shape=J)\n    obs = pm.Normal(\"obs\", mu=group_means[None,:], sigma=1, observed=data, shape=(J,N))\n    trace_hierarchical = pm.sample(draws=4000, tune=1000)\n\npm.summary(trace_hierarchical)\n```\n\n\n### Model Extensions and Custom Distributions\n\nPyMC3 allows for flexibility in specifying custom distributions and extending the model beyond built-in functionalities. This is useful when dealing with data that doesn’t fit standard distributions or when incorporating domain-specific knowledge into the model.  You can define custom distributions by extending the `pm.Distribution` class.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport scipy.stats as stats\n\n# Example custom distribution\nclass MyCustomDist(pm.Distribution):\n    def __init__(self, alpha, beta, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alpha = alpha\n        self.beta = beta\n\n    def logp(self, x):\n        return stats.beta.logpdf(x, self.alpha, self.beta)\n\nwith pm.Model() as model:\n    custom_param = MyCustomDist('custom_param', alpha=2, beta=5)\n    trace = pm.sample(1000, tune=1000)\n\npm.summary(trace)\n```\n\n\n### PyMC3 and External Libraries\n\nPyMC3 can be integrated with other libraries to enhance its capabilities.  For example:\n* **Theano:** PyMC3 uses Theano (or Aesara) for symbolic differentiation and optimized computation, enabling efficient sampling even for complex models.\n* **NumPy:** NumPy is used for numerical computation throughout PyMC3.\n* **SciPy:** SciPy's special functions and statistical distributions are often integrated within custom distributions.\n* **ArviZ:** ArviZ is a powerful library for visualizing and analyzing Bayesian inference results, complementing PyMC3's output.\n\n\n```{mermaid}\ngraph LR\nA[Data] --> B(PyMC3 Model);\nB --> C[Sampling (NUTS, HMC)];\nC --> D[Posterior Analysis (ArviZ)];\nE[External Libraries (NumPy, SciPy, Theano)] --> B;\nB --> F[Predictions/Inferences];\n```\n\n\n\n\n## Implementation with PyMC3\n\nThis chapter presents several case studies to illustrate the practical application of PyMC3 in various scenarios.\n\n\n### Example 1: Simple Linear Regression\n\nWe revisit the simple linear regression example, focusing on the Bayesian approach using PyMC3.  We assume a linear relationship between an independent variable $X$ and a dependent variable $Y$, with additive Gaussian noise.  The model can be written as:\n\n$Y_i = \\alpha + \\beta X_i + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma)$\n\nHere, $\\alpha$ is the intercept, $\\beta$ is the slope, and $\\sigma$ is the standard deviation of the noise.  We'll use weakly informative priors for these parameters.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\n# Sample data\nnp.random.seed(42)\nX = np.linspace(0, 10, 20)\nY = 2*X + 1 + np.random.normal(0, 1, 20)\n\nwith pm.Model() as model:\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n    beta = pm.Normal(\"beta\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n    mu = alpha + beta * X\n    Y_obs = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=Y)\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\naz.plot_trace(trace);\nplt.show()\naz.summary(trace)\n```\n\nThe code defines the model, samples from the posterior, and visualizes the results using `arviz`.  The summary shows the posterior estimates for α, β, and σ, along with their credible intervals.\n\n\n### Example 2: Bayesian A/B Testing\n\nA/B testing compares two versions (A and B) of something (e.g., a website, an ad) to determine which performs better. In a Bayesian framework, we model the conversion rates (success probabilities) for each version as independent Beta distributions. We then update these distributions using the observed data (number of successes and failures for each version).\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport arviz as az\n\n# Observed data (number of successes and failures)\nsuccesses_A = 50\nfailures_A = 50\nsuccesses_B = 60\nfailures_B = 40\n\nwith pm.Model() as model:\n    p_A = pm.Beta(\"p_A\", alpha=1, beta=1) #Prior for conversion rate of A\n    p_B = pm.Beta(\"p_B\", alpha=1, beta=1) #Prior for conversion rate of B\n    obs_A = pm.Binomial(\"obs_A\", p=p_A, n=successes_A + failures_A, observed=successes_A)\n    obs_B = pm.Binomial(\"obs_B\", p=p_B, n=successes_B + failures_B, observed=successes_B)\n    diff_of_means = pm.Deterministic(\"diff_of_means\", p_B - p_A)\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\naz.plot_posterior(trace, var_names=['p_A', 'p_B', 'diff_of_means'], hdi_prob=0.95);\nplt.show()\naz.summary(trace)\n```\n\nThe code defines the prior and likelihood for each group, samples the posterior, and visualizes the posterior distributions of the conversion rates for A and B, as well as their difference.\n\n\n### Example 3: Time Series Analysis\n\nBayesian methods are also applicable to time series analysis.  Here, we'll consider a simple autoregressive model of order 1 (AR(1)) to model a time series with temporal dependence.  The model is defined as:\n\n$y_t = \\phi y_{t-1} + \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma)$\n\nHere, $y_t$ is the value of the time series at time t, and $\\phi$ is the autoregressive coefficient.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\n# Simulate AR(1) time series\nnp.random.seed(42)\nphi_true = 0.7\nsigma_true = 1\nT = 100\ny = np.zeros(T)\nfor i in range(1, T):\n    y[i] = phi_true * y[i-1] + np.random.normal(0, sigma_true)\n\nwith pm.Model() as model:\n    phi = pm.Uniform(\"phi\", lower=-1, upper=1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n    y_ = pm.AR1(\"y_\", k=1, rho=phi, sigma=sigma, observed=y) # AR1 model in PyMC3\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\naz.plot_trace(trace); plt.show()\naz.summary(trace)\n```\n\nThis code simulates an AR(1) process, fits a Bayesian AR(1) model using PyMC3, and visualizes the posterior distribution of the parameters.  The posterior distribution of `phi` provides information on the strength of the temporal dependence in the data.\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"implementation-with-pymc3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"implementation-with-pymc3.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}