{"title":"Introduction to MCMC","markdown":{"headingText":"Introduction to MCMC","containsRefs":false,"markdown":"\nMarkov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from probability distributions.  They are particularly useful in Bayesian inference, where we often encounter complex posterior distributions that are intractable to sample from directly.  Instead of directly calculating the posterior, MCMC methods construct a Markov chain whose stationary distribution is the target posterior distribution.  By running this chain for a sufficiently long time, we can obtain samples that approximate draws from the posterior. This allows us to estimate posterior expectations, credible intervals, and other quantities of interest.  The power of MCMC lies in its ability to handle high-dimensional and complex probability distributions that are otherwise impossible to sample from using simpler methods.  Different MCMC algorithms vary in their efficiency and effectiveness depending on the characteristics of the target distribution.\n\n\n### Markov Chains and Bayesian Inference\n\nA Markov chain is a stochastic process $\\{X_t\\}_{t=0}^{\\infty}$ with the Markov property: the future state depends only on the present state, not on the past.  Formally, this means that the conditional probability of transitioning to state $x$ at time $t+1$ depends only on the current state $x_t$:\n\n$P(X_{t+1} = x | X_t = x_t, X_{t-1} = x_{t-1}, \\dots, X_0 = x_0) = P(X_{t+1} = x | X_t = x_t)$\n\nThis conditional probability is often represented by a transition kernel, $K(x, x') = P(X_{t+1} = x' | X_t = x)$.  In the context of Bayesian inference, we aim to sample from the posterior distribution $p(\\theta|y)$, where $\\theta$ are the model parameters and $y$ is the observed data. We design a Markov chain such that its stationary distribution is precisely this posterior distribution.  The chain is initialized at some starting point $\\theta_0$ and iteratively updated using the transition kernel. After a sufficient number of iterations (the \"burn-in\" period), the samples generated approximate draws from the target posterior.\n\n\n### Detailed Balance and Stationarity\n\nA crucial concept in MCMC is *detailed balance*. A Markov chain satisfies detailed balance with respect to a distribution $\\pi(\\theta)$ if:\n\n$\\pi(x) K(x, x') = \\pi(x') K(x', x) \\quad \\forall x, x'$\n\nThis condition states that the probability flux from state $x$ to $x'$ is equal to the probability flux from $x'$ to $x$. If detailed balance holds, then $\\pi(\\theta)$ is the *stationary distribution* of the Markov chain.  This means that if the chain reaches its stationary distribution, the probability of being in any state $x$ is $\\pi(x)$.  Many MCMC algorithms are designed to satisfy detailed balance, guaranteeing that the chain converges to the desired target distribution.  The importance of detailed balance is that it ensures that the stationary distribution is unique and independent of the starting point of the chain.\n\n### Convergence Diagnostics\n\nDetermining when an MCMC chain has converged to its stationary distribution is crucial for reliable inference.  Several diagnostic methods exist to assess convergence:\n\n* **Visual inspection of trace plots:**  Plotting the sampled values of each parameter against iteration number can reveal trends or slow mixing.  If the chain hasn't converged, you might see trends and lack of randomness in the plot.\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example trace plot\nnp.random.seed(42)\nsamples = np.cumsum(np.random.randn(1000))  #Example non-converged chain\nplt.plot(samples)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Parameter Value\")\nplt.title(\"Trace Plot\")\nplt.show()\n\nsamples = np.random.randn(1000) #Example converged chain\nplt.plot(samples)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Parameter Value\")\nplt.title(\"Trace Plot\")\nplt.show()\n\n```\n\n* **Autocorrelation function:** Measures the correlation between samples separated by a certain lag. High autocorrelation indicates slow mixing and a lack of independence between samples.\n\n```\\{python}\n#| echo: true\nfrom statsmodels.tsa.stattools import acf\nimport matplotlib.pyplot as plt\n\n# Example autocorrelation plot\nacf_values = acf(samples)\nplt.plot(acf_values)\nplt.xlabel(\"Lag\")\nplt.ylabel(\"Autocorrelation\")\nplt.title(\"Autocorrelation Function\")\nplt.show()\n\n```\n\n\n* **Gelman-Rubin diagnostic:**  Runs multiple chains in parallel with different starting points.  The diagnostic calculates the ratio of between-chain variance to within-chain variance.  A value close to 1 suggests convergence.  Values significantly greater than 1 indicate that the chains haven't converged to the same distribution.\n\n\n* **Effective sample size (ESS):** Accounts for autocorrelation between samples. A lower ESS than the actual number of samples indicates that the autocorrelation is high, and the effective information content of the sample is reduced.\n\n\nIt is important to use multiple convergence diagnostics and visually inspect the trace plots before drawing any conclusions about the MCMC output.  It's common practice to discard the initial samples (burn-in period) before using the remaining samples for inference.  The choice of burn-in period and the assessment of convergence are often subjective and require careful consideration.  Software packages like PyMC3 and Stan provide tools for implementing and monitoring MCMC algorithms.\n\n\n## Metropolis-Hastings Algorithm\n\nThe Metropolis-Hastings algorithm is a widely used MCMC method for sampling from a probability distribution.  It's particularly valuable when the target distribution is complex and direct sampling is infeasible.  It cleverly constructs a Markov chain that asymptotically converges to the desired target distribution by cleverly accepting or rejecting proposed moves.\n\n\n### Algorithm Description and Intuition\n\nThe algorithm iteratively generates samples as follows:\n\n1. **Initialization:** Start with an initial value $\\theta^{(0)}$.\n\n2. **Proposal:** Given the current state $\\theta^{(t)}$, propose a new state $\\theta^*$ using a proposal distribution $q(\\theta^* | \\theta^{(t)})$.  This proposal distribution is chosen by the user and should be easy to sample from.\n\n3. **Acceptance:** Accept the proposed state $\\theta^*$ with probability:\n\n   $\\alpha(\\theta^* | \\theta^{(t)}) = \\min \\left( 1, \\frac{\\pi(\\theta^*) q(\\theta^{(t)} | \\theta^*)}{\\pi(\\theta^{(t)}) q(\\theta^* | \\theta^{(t)})} \\right)$\n\n   where $\\pi(\\theta)$ is the target distribution (e.g., the posterior distribution in Bayesian inference).\n\n4. **Update:** If the proposed state is accepted, set $\\theta^{(t+1)} = \\theta^*$. Otherwise, set $\\theta^{(t+1)} = \\theta^{(t)}$.\n\n5. **Iteration:** Repeat steps 2-4 for a large number of iterations.\n\n\nThe intuition behind the acceptance probability is to favor moves that increase the probability mass of the target distribution. If the proposed move increases the probability ($\\pi(\\theta^*) > \\pi(\\theta^{(t)})$), it is always accepted. If the proposed move decreases the probability, it is accepted with a probability proportional to the ratio of the probability densities.  The proposal distribution $q$ plays a crucial role in the efficiency of the algorithm.\n\n\n### Proposal Distributions\n\nThe choice of proposal distribution significantly impacts the efficiency of the Metropolis-Hastings algorithm.  A good proposal distribution should:\n\n* Be easy to sample from.\n* Have sufficient spread to explore the target distribution effectively.\n* Not be too broad to have low acceptance rates.\n\nCommon choices include:\n\n* **Gaussian random walk:** $\\theta^* = \\theta^{(t)} + \\epsilon$, where $\\epsilon \\sim N(0, \\Sigma)$.  $\\Sigma$ is a covariance matrix that needs tuning.\n\n* **Uniform distribution:**  $\\theta^* \\sim U(\\theta^{(t)} - \\delta, \\theta^{(t)} + \\delta)$. $\\delta$ is a parameter to tune.\n\nThe optimal proposal distribution depends on the characteristics of the target distribution.  A poorly chosen proposal distribution can lead to slow mixing and inefficient exploration of the parameter space.\n\n\n\n### Acceptance Rate and Tuning\n\nThe acceptance rate, the proportion of proposed states that are accepted, is an important metric for assessing the efficiency of the Metropolis-Hastings algorithm. A very low acceptance rate indicates that the proposal distribution is too broad, while a very high acceptance rate suggests that it's too narrow.  A reasonable acceptance rate is often considered to be between 0.2 and 0.5, although this can vary depending on the dimensionality of the problem.  Tuning the proposal distribution (e.g., adjusting the variance of a Gaussian proposal) is often necessary to achieve a good acceptance rate.\n\n\n### Example: Metropolis-Hastings for a Gaussian Posterior\n\nLet's assume a simple Bayesian model where the posterior distribution is a Gaussian: $\\pi(\\theta) = N(\\mu, \\sigma^2)$. We can use a Gaussian random walk proposal distribution: $\\theta^* = \\theta^{(t)} + \\epsilon$, with $\\epsilon \\sim N(0, \\tau^2)$. The acceptance probability is:\n\n$\\alpha(\\theta^* | \\theta^{(t)}) = \\min \\left( 1, \\frac{\\pi(\\theta^*)}{\\pi(\\theta^{(t)})} \\right) = \\min \\left( 1, \\exp \\left( -\\frac{(\\theta^* - \\mu)^2}{2\\sigma^2} + \\frac{(\\theta^{(t)} - \\mu)^2}{2\\sigma^2} \\right) \\right)$\n\n\n### Python Implementation\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef metropolis_hastings(target_pdf, proposal_pdf, initial_state, n_iterations, proposal_params):\n    \"\"\"\n    Metropolis-Hastings algorithm.\n\n    Args:\n        target_pdf: The target probability density function (PDF).\n        proposal_pdf: The proposal PDF (e.g., a Gaussian).\n        initial_state: The starting point of the chain.\n        n_iterations: Number of iterations.\n        proposal_params: Parameters for proposal distribution (e.g., mean, variance).\n\n    Returns:\n        A NumPy array of samples from the target distribution.\n    \"\"\"\n\n    samples = [initial_state]\n    current_state = initial_state\n    accepted = 0\n\n    for i in range(n_iterations):\n        # Proposal\n        proposed_state = proposal_pdf(current_state, proposal_params)\n\n        # Acceptance probability\n        acceptance_prob = min(1, target_pdf(proposed_state) / target_pdf(current_state))\n\n        # Acceptance/Rejection\n        if np.random.rand() < acceptance_prob:\n            current_state = proposed_state\n            accepted += 1\n        samples.append(current_state)\n\n    print(f\"Acceptance rate: {accepted / n_iterations}\")\n    return np.array(samples)\n\n\n\n# Target distribution (Gaussian)\ndef target_gaussian(theta, mu=0, sigma=1):\n    return np.exp(-(theta - mu)**2 / (2 * sigma**2)) / (sigma * np.sqrt(2 * np.pi))\n\n# Gaussian random walk proposal\ndef gaussian_proposal(current_state, params):\n    return current_state + np.random.normal(0, params['sigma'])\n\n# Parameters\nmu = 0\nsigma = 1\ninitial_state = 0\nn_iterations = 10000\nproposal_params = {'sigma': 0.5}  # Tune this parameter\n\n# Run Metropolis-Hastings\nsamples = metropolis_hastings(lambda x: target_gaussian(x, mu, sigma), gaussian_proposal, initial_state, n_iterations, proposal_params)\n\n# Plot results\nplt.hist(samples[1000:], bins=50, density=True) #burn-in of 1000 samples\nplt.title('Metropolis-Hastings Samples')\nplt.xlabel('θ')\nplt.ylabel('Density')\nplt.show()\n\n```\n\nThis code provides a basic implementation of the Metropolis-Hastings algorithm for a Gaussian target distribution.  You can adapt it for other target distributions by changing the `target_pdf` and `proposal_pdf` functions.  Remember to tune the parameters of the proposal distribution to achieve a reasonable acceptance rate.\n\n\n## Gibbs Sampling\n\nGibbs sampling is a special case of the Metropolis-Hastings algorithm where the proposal distributions are chosen in a way that guarantees acceptance at every step. This makes it a particularly efficient MCMC method when the full conditional distributions of the parameters are easy to sample from.\n\n\n### Conditional Distributions\n\nGibbs sampling relies on the ability to sample from the *full conditional distributions* of the parameters.  Suppose we have a joint distribution $p(\\theta_1, \\theta_2, \\dots, \\theta_k)$. The full conditional distribution for parameter $\\theta_i$ is the conditional distribution of $\\theta_i$ given all other parameters:\n\n$p(\\theta_i | \\theta_1, \\dots, \\theta_{i-1}, \\theta_{i+1}, \\dots, \\theta_k)$\n\n\nIf we can easily sample from these full conditional distributions, Gibbs sampling offers a straightforward and efficient way to sample from the joint distribution.\n\n\n### Algorithm Description\n\nThe Gibbs sampling algorithm iteratively samples from the full conditional distributions of each parameter, one at a time.  Specifically:\n\n1. **Initialization:** Start with initial values for all parameters: $\\theta_1^{(0)}, \\theta_2^{(0)}, \\dots, \\theta_k^{(0)}$.\n\n2. **Iteration:** For iteration $t = 1, 2, \\dots$:\n   * Sample $\\theta_1^{(t)} \\sim p(\\theta_1 | \\theta_2^{(t-1)}, \\theta_3^{(t-1)}, \\dots, \\theta_k^{(t-1)})$\n   * Sample $\\theta_2^{(t)} \\sim p(\\theta_2 | \\theta_1^{(t)}, \\theta_3^{(t-1)}, \\dots, \\theta_k^{(t-1)})$\n   * ...\n   * Sample $\\theta_k^{(t)} \\sim p(\\theta_k | \\theta_1^{(t)}, \\theta_2^{(t)}, \\dots, \\theta_{k-1}^{(t)})$\n\n3. **Continuation:** Repeat step 2 for a large number of iterations.  After a sufficient burn-in period, the samples approximate draws from the joint distribution $p(\\theta_1, \\theta_2, \\dots, \\theta_k)$.\n\n\nNotice that each parameter is updated sequentially, conditioning on the most recently sampled values of the other parameters.  This creates a Markov chain whose stationary distribution is the joint distribution of interest.\n\n\n### Advantages and Disadvantages\n\n**Advantages:**\n\n* **Simplicity:** Relatively easy to implement if full conditional distributions are known and easy to sample from.\n* **Efficiency:** Can be very efficient, especially for distributions with relatively simple conditional distributions.\n* **Guaranteed acceptance:**  Unlike Metropolis-Hastings, every proposed sample is accepted, leading to a higher effective sample size.\n\n**Disadvantages:**\n\n* **Conditional distributions:** Requires the ability to sample from the full conditional distributions.  This may not always be possible.\n* **Slow mixing:** Can suffer from slow mixing if the parameters are highly correlated.  In such cases, other MCMC methods might be more efficient.\n\n\n### Example: Gibbs Sampling for a Bivariate Gaussian\n\nConsider a bivariate Gaussian distribution:\n\n$ \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix} \\sim N \\left( \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix} \\right) $\n\nThe full conditional distributions are also Gaussian:\n\n$\\theta_1 | \\theta_2 \\sim N\\left( \\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2}(\\theta_2 - \\mu_2), \\sigma_1^2(1 - \\rho^2) \\right)$\n\n$\\theta_2 | \\theta_1 \\sim N\\left( \\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1}(\\theta_1 - \\mu_1), \\sigma_2^2(1 - \\rho^2) \\right)$\n\n\nWe can easily sample from these conditional distributions using standard methods.\n\n\n### Python Implementation\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef gibbs_sampling(mu, sigma, rho, n_iterations, initial_state):\n    \"\"\"\n    Gibbs sampling for a bivariate Gaussian distribution.\n    \"\"\"\n    theta1 = [initial_state[0]]\n    theta2 = [initial_state[1]]\n\n    for i in range(n_iterations):\n        # Sample theta1 given theta2\n        theta1_new = np.random.normal(mu[0] + rho * (sigma[0]/sigma[1]) * (theta2[-1] - mu[1]), sigma[0] * np.sqrt(1 - rho**2))\n        theta1.append(theta1_new)\n\n        # Sample theta2 given theta1\n        theta2_new = np.random.normal(mu[1] + rho * (sigma[1]/sigma[0]) * (theta1[-1] - mu[0]), sigma[1] * np.sqrt(1 - rho**2))\n        theta2.append(theta2_new)\n\n    return np.array(theta1), np.array(theta2)\n\n\n# Parameters\nmu = np.array([0, 0])\nsigma = np.array([1, 2])\nrho = 0.8\nn_iterations = 10000\ninitial_state = np.array([0, 0])\n\n\ntheta1, theta2 = gibbs_sampling(mu, sigma, rho, n_iterations, initial_state)\n\n# Plot the samples\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.plot(theta1[1000:]) #burn-in of 1000 samples\nplt.title('Trace Plot of θ1')\nplt.subplot(1,2,2)\nplt.plot(theta2[1000:]) #burn-in of 1000 samples\nplt.title('Trace Plot of θ2')\nplt.show()\n\nplt.figure(figsize=(6,6))\nplt.scatter(theta1[1000:],theta2[1000:]) #burn-in of 1000 samples\nplt.title('Scatter plot of θ1 vs θ2')\nplt.xlabel('θ1')\nplt.ylabel('θ2')\nplt.show()\n\n```\n\nThis code implements Gibbs sampling for a bivariate Gaussian distribution.  The trace plots show the sampled values over time, and the scatter plot visualizes the joint distribution of the samples.  Remember to adjust the `n_iterations` and burn-in period as necessary for convergence.  This example can be easily expanded to higher-dimensional problems if you can derive the full conditional distributions.\n\n\n## Hamiltonian Monte Carlo (HMC)\n\nHamiltonian Monte Carlo (HMC) is an advanced MCMC algorithm that addresses some of the limitations of simpler methods like Metropolis-Hastings and Gibbs sampling.  It leverages Hamiltonian dynamics to propose more efficient moves in high-dimensional spaces, leading to better exploration of the target distribution and reduced autocorrelation between samples.\n\n\n### Hamiltonian Dynamics\n\nHMC draws inspiration from Hamiltonian mechanics, a framework in physics that describes the evolution of a system using a Hamiltonian function.  In the context of MCMC, the Hamiltonian is defined as:\n\n$H(\\theta, p) = U(\\theta) + K(p)$\n\nwhere:\n\n* $\\theta$ represents the model parameters (position).\n* $p$ represents auxiliary momentum variables (momentum).\n* $U(\\theta) = -\\log(\\pi(\\theta))$ is the potential energy, related to the negative log-likelihood of the target distribution $\\pi(\\theta)$.\n* $K(p) = \\frac{1}{2}p^T M^{-1} p$ is the kinetic energy, where $M$ is a mass matrix (often chosen as the identity matrix for simplicity).\n\n\nHamiltonian dynamics govern the evolution of the system through Hamilton's equations:\n\n$\\frac{d\\theta}{dt} = \\frac{\\partial H}{\\partial p} = M^{-1}p$\n\n$\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial \\theta} = -\\nabla U(\\theta)$\n\n\nThese equations describe how the position and momentum evolve over time.  The key idea in HMC is to use this evolution to propose new states for the Markov chain.\n\n\n### Leapfrog Integration\n\nSolving Hamilton's equations analytically is often impossible.  Instead, HMC employs numerical integration techniques, most commonly the leapfrog method. The leapfrog integrator updates the position and momentum using half-steps:\n\n$p(t + \\frac{\\epsilon}{2}) = p(t) - \\frac{\\epsilon}{2} \\nabla U(\\theta(t))$\n\n$\\theta(t + \\epsilon) = \\theta(t) + \\epsilon M^{-1} p(t + \\frac{\\epsilon}{2})$\n\n$p(t + \\epsilon) = p(t + \\frac{\\epsilon}{2}) - \\frac{\\epsilon}{2} \\nabla U(\\theta(t + \\epsilon))$\n\nwhere $\\epsilon$ is the step size. This process is repeated for a specified number of steps to generate a proposed state.\n\n\n### Algorithm Description\n\nThe HMC algorithm combines Hamiltonian dynamics and the Metropolis-Hastings acceptance criterion:\n\n1. **Initialization:** Start with initial values for parameters $\\theta^{(0)}$ and sample momentum $p \\sim N(0, M)$.\n\n2. **Hamiltonian dynamics:** Use the leapfrog integrator to simulate Hamiltonian dynamics for $L$ steps with step size $\\epsilon$, obtaining a proposed state $(\\theta^*, p^*)$.\n\n3. **Acceptance:** Accept the proposed state $(\\theta^*, p^*)$ with probability:\n\n   $\\alpha = \\min \\left( 1, \\exp(-H(\\theta^*, p^*) + H(\\theta^{(t)}, p)) \\right)$\n\n4. **Update:** If accepted, set $\\theta^{(t+1)} = \\theta^*$. Otherwise, set $\\theta^{(t+1)} = \\theta^{(t)}$.\n\n5. **Iteration:** Repeat steps 2-4 for a sufficient number of iterations.\n\n\nThe leapfrog integration introduces a small amount of error, but this is corrected by the Metropolis acceptance step, ensuring that the algorithm correctly targets the desired distribution.\n\n\n### Tuning Parameters: Step Size and Number of Leapfrog Steps\n\nThe efficiency of HMC depends crucially on the choice of step size ($\\epsilon$) and the number of leapfrog steps ($L$).\n\n* **Step size ($\\epsilon$):**  A small step size reduces numerical error but increases computational cost. A large step size can lead to inaccurate proposals and low acceptance rates.\n\n* **Number of leapfrog steps ($L$):**  A small number of steps leads to short proposals, which might not explore the parameter space effectively.  A large number of steps increases computational cost.\n\n\nFinding optimal values often requires experimentation.  Adaptive HMC methods aim to automatically tune these parameters during the sampling process.\n\n\n### Advantages over Metropolis-Hastings and Gibbs Sampling\n\n* **Efficient exploration:** HMC typically explores the target distribution more efficiently than Metropolis-Hastings, especially in high-dimensional spaces, by making larger, more informed proposals.\n\n* **Reduced autocorrelation:**  HMC generates samples with lower autocorrelation than random-walk Metropolis-Hastings, leading to a higher effective sample size for the same number of iterations.\n\n* **No conditional distributions needed:** Unlike Gibbs sampling, HMC does not require knowledge or sampling from the full conditional distributions.\n\n\nHowever, HMC can be more computationally expensive per iteration than simpler methods.\n\n\n### Example: HMC for a Gaussian Mixture Model\n\nImplementing HMC for a Gaussian mixture model requires a bit more setup than the previous examples. We will use PyMC3, a powerful probabilistic programming library that handles HMC automatically.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\n# Simulate data from a Gaussian mixture model\nnp.random.seed(42)\nN = 500\nmu1 = [-2, 0]\nmu2 = [2, 0]\nsigma1 = [[1, 0.5], [0.5, 1]]\nsigma2 = [[1, -0.5], [-0.5, 1]]\ndata1 = np.random.multivariate_normal(mu1, sigma1, N//2)\ndata2 = np.random.multivariate_normal(mu2, sigma2, N//2)\ndata = np.concatenate((data1, data2))\n\n\nwith pm.Model() as model:\n    # Priors\n    mu = pm.MvNormal(\"mu\", mu=np.zeros(2), cov=np.eye(2), shape=2)\n    sigma = pm.LKJCholeskyCov(\"sigma\", eta=2, n=2, sd_dist=pm.HalfCauchy.dist(2.5))\n    w = pm.Dirichlet(\"w\", a=np.ones(2))\n    \n    # Likelihood\n    y = pm.Mixture(\"y\", w=w, comp_dists=[pm.MvNormal.dist(mu=mu[i], cov=pm.Deterministic(\"cov\"+str(i),pm.math.matrix_dot(sigma,sigma.T)) ) for i in range(2)], observed=data)\n\n    # Inference (HMC)\n    trace = pm.sample(1000, tune=1000, target_accept=0.8) #tune to find good acceptance rate\n\n\naz.plot_trace(trace);\nplt.show()\n\n```\n\nThis PyMC3 code defines a Gaussian Mixture Model and performs inference using HMC. PyMC3 automatically handles the Hamiltonian dynamics and tuning of parameters. The `az.plot_trace` function provides visual diagnostics of the convergence.  Remember to install PyMC3: `pip install pymc3 arviz`.  Stan is another powerful option for HMC, but its syntax is different.  Both offer significant advantages over manual HMC implementation.\n\n\n\n## Comparing MCMC Algorithms\n\nChoosing the right MCMC algorithm for a specific problem is crucial for efficient and accurate Bayesian inference.  Different algorithms have strengths and weaknesses concerning efficiency, convergence rates, and computational cost.\n\n\n### Efficiency and Convergence Rates\n\nThe efficiency of an MCMC algorithm is often measured by its convergence rate and the effective sample size (ESS).  The convergence rate refers to how quickly the Markov chain approaches its stationary distribution.  A faster convergence rate means that fewer iterations are needed to obtain reliable samples.  ESS quantifies the number of effectively independent samples generated by the algorithm, accounting for autocorrelation between samples. A higher ESS for a given number of iterations indicates greater efficiency.\n\n* **Metropolis-Hastings:** Its efficiency strongly depends on the proposal distribution.  Poorly tuned proposals can lead to slow convergence and low ESS. Random walk Metropolis can be particularly inefficient in high dimensions.\n\n* **Gibbs Sampling:**  Can be highly efficient when the full conditional distributions are easy to sample from. However, it suffers from slow mixing if the parameters are highly correlated.\n\n* **Hamiltonian Monte Carlo (HMC):**  Generally more efficient than Metropolis-Hastings and Gibbs sampling in high-dimensional problems due to its ability to make larger, more informed proposals. However, it is computationally more expensive per iteration.\n\n\n### Computational Cost\n\nThe computational cost of an MCMC algorithm depends on several factors, including:\n\n* **Number of iterations:** Algorithms with faster convergence rates require fewer iterations, reducing the overall computational cost.\n\n* **Cost per iteration:** HMC, for example, is generally more computationally expensive per iteration than Metropolis-Hastings due to the numerical integration involved.\n\n* **Dimensionality:**  The computational cost often increases with the dimensionality of the parameter space.\n\n\nThe choice of algorithm should consider the balance between computational cost and the desired accuracy and efficiency.\n\n\n### Choosing the Right Algorithm for a Specific Problem\n\nThe optimal MCMC algorithm depends on several factors related to the target distribution and computational resources:\n\n* **Target distribution:**  If the full conditional distributions are easy to sample from, Gibbs sampling can be highly efficient. If the target is high-dimensional and complex, HMC is often preferred.  If the target is simple, Metropolis-Hastings with a well-tuned proposal distribution might suffice.\n\n* **Dimensionality:**  HMC generally performs better in high-dimensional settings.  Metropolis-Hastings and Gibbs sampling can be inefficient in high dimensions if not carefully implemented.\n\n* **Computational resources:**  HMC is more computationally expensive per iteration. If computational resources are limited, Metropolis-Hastings or Gibbs sampling might be a more practical choice.\n\n\nIn practice, it's often beneficial to experiment with multiple algorithms and compare their performance using convergence diagnostics and ESS.  The following table summarizes some guidelines:\n\n\n| Algorithm          | Advantages                                      | Disadvantages                                  | Best Suited For                               |\n|----------------------|-------------------------------------------------|-----------------------------------------------|-----------------------------------------------|\n| Metropolis-Hastings | Simple to implement, versatile                  | Can be inefficient in high dimensions, proposal tuning required | Low-dimensional problems, simple target distributions |\n| Gibbs Sampling      | Efficient if conditional distributions are easy to sample from | Requires sampling from full conditional distributions, slow mixing with high correlation | Problems with easy-to-sample conditional distributions |\n| HMC                 | Efficient in high dimensions, reduced autocorrelation | More computationally expensive per iteration, requires tuning | High-dimensional problems, complex target distributions |\n\n\nThere's no universally \"best\" algorithm. The optimal choice often involves experimentation and careful consideration of the problem's specific characteristics.  Software packages like PyMC3 and Stan offer a range of algorithms and automatic tuning capabilities, simplifying the process of choosing and implementing an appropriate MCMC method.\n\n\n```\\{python}\n#| echo: true\n#Illustrative comparison (not a real-world benchmark)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Simulate effective sample sizes for different algorithms\nmh_ess = np.random.poisson(1000, 100) #Example ESS for Metropolis-Hastings\ngibbs_ess = np.random.poisson(1500, 100) #Example ESS for Gibbs Sampling\nhmc_ess = np.random.poisson(2000,100) #Example ESS for HMC\n\n\nplt.boxplot([mh_ess, gibbs_ess, hmc_ess], labels=['Metropolis-Hastings', 'Gibbs', 'HMC'])\nplt.ylabel('Effective Sample Size (ESS)')\nplt.title('Illustrative Comparison of ESS for Different Algorithms')\nplt.show()\n```\n\nThis code provides a simplified illustration of how effective sample sizes might differ between algorithms. Real-world comparisons require more thorough benchmarking on specific problems.  Remember that the actual performance depends heavily on the problem's specifics and the tuning of the algorithms.\n\n\n## Advanced Topics in MCMC\n\nThis section briefly explores some advanced techniques to improve the efficiency and applicability of MCMC methods.\n\n\n### Parallel MCMC\n\nRunning multiple MCMC chains in parallel offers several advantages:\n\n* **Faster convergence assessment:**  Comparing multiple chains helps assess convergence more reliably than with a single chain. The Gelman-Rubin diagnostic, for example, relies on parallel chains.\n\n* **Increased effective sample size:** Combining samples from multiple chains increases the overall effective sample size, improving the precision of posterior estimates.\n\n* **Reduced autocorrelation:**  Properly parallelized chains can reduce autocorrelation between samples, further enhancing efficiency.\n\n\nParallel tempering is a sophisticated parallel MCMC approach. It runs multiple chains at different temperatures, allowing chains at higher temperatures (with more exploration) to occasionally swap states with chains at lower temperatures (with more exploitation).  This improves exploration of the target distribution, especially for multimodal distributions.\n\n\n```{mermaid}\ngraph LR\n    A[Chain 1 (Low Temperature)] --> B(Swap);\n    C[Chain 2 (Medium Temperature)] --> B;\n    D[Chain 3 (High Temperature)] --> B;\n    B --> A;\n    B --> C;\n    B --> D;\n```\n\nThis mermaid diagram illustrates the swapping of states between chains in parallel tempering.\n\n\n### Adaptive MCMC\n\nAdaptive MCMC methods adjust the algorithm's parameters (e.g., step size in HMC, proposal variance in Metropolis-Hastings) during the sampling process. This adaptation aims to optimize the algorithm's performance based on the observed behavior of the chain.  Adaptive methods can improve efficiency by dynamically tuning parameters to maintain a target acceptance rate or reduce autocorrelation.  However, careful design is needed to ensure that the adaptation process doesn't compromise the algorithm's convergence properties.  Common adaptive MCMC methods include:\n\n* **Adaptive Metropolis:** Adapts the proposal distribution's covariance matrix based on past samples.\n\n* **Adaptive Hamiltonian Monte Carlo:**  Adapts the step size and number of leapfrog steps based on the acceptance rate.\n\n\nCareful implementation and monitoring are critical for adaptive methods to guarantee convergence to the correct stationary distribution.\n\n\n### Dealing with High-Dimensional Problems\n\nHigh-dimensional problems pose significant challenges for MCMC due to the \"curse of dimensionality\":  the volume of the parameter space increases exponentially with the number of dimensions, making it increasingly difficult to explore the target distribution effectively.  Strategies for tackling high-dimensional problems include:\n\n* **Dimensionality reduction:**  Techniques like principal component analysis (PCA) can reduce the dimensionality before applying MCMC.  This reduces the computational cost and improves exploration.\n\n* **Variable selection:**  If some parameters are less important, they can be marginalized out or fixed to reduce the effective dimensionality.\n\n* **HMC and its variants:**  HMC and its variants, such as the No-U-Turn Sampler (NUTS), are designed to handle high-dimensional problems more efficiently than simpler methods. NUTS automatically determines the number of leapfrog steps, adapting to the curvature of the target distribution.\n\n* **Parallel MCMC:** Running multiple chains in parallel allows for more efficient exploration of the high-dimensional space.\n\n\nEfficiently sampling from high-dimensional distributions often requires a combination of strategies tailored to the specific problem.  Advanced techniques such as Hamiltonian Monte Carlo with sophisticated adaptation schemes, or parallel tempering, are often necessary.  The choice of prior distributions can also affect the efficiency of sampling in high dimensions.  Carefully considering the prior specification is crucial to avoid overly diffuse priors that lead to slower convergence.\n\n\n```\\{python}\n#| echo: true\n#Illustrative example of dimensionality reduction (PCA) before MCMC (requires more detailed implementation)\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Simulate high-dimensional data\nnp.random.seed(42)\ndata = np.random.randn(1000, 10)  # 1000 samples, 10 dimensions\n\n# Apply PCA to reduce dimensionality\npca = PCA(n_components=2) # Reduce to 2 dimensions\nreduced_data = pca.fit_transform(data)\n\n#Apply MCMC to the reduced data (MCMC implementation would be added here)\n#...\n\n```\n\nThis code snippet demonstrates dimensionality reduction using PCA before applying MCMC.  The actual MCMC implementation would need to be added, applied to the `reduced_data`.  Remember that dimensionality reduction might lead to information loss, and the choice of the number of components is a crucial decision.  This example requires additional code (for instance, a call to a chosen MCMC algorithm) to perform the MCMC steps.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"mcmc-algorithms.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"mcmc-algorithms.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}