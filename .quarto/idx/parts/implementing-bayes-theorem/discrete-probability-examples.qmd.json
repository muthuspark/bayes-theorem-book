{"title":"Introduction to Discrete Probability","markdown":{"headingText":"Introduction to Discrete Probability","containsRefs":false,"markdown":"\n### What is Discrete Probability?\n\nDiscrete probability deals with events that have a finite or countably infinite number of possible outcomes.  Unlike continuous probability, where outcomes can take on any value within a range (e.g., height, weight), discrete probability focuses on distinct, separate outcomes.  For example, the number of heads when flipping a coin three times (0, 1, 2, or 3) is a discrete random variable.  Other examples include the number of cars passing a certain point on a highway in an hour, or the number of defects in a batch of manufactured items.  The key characteristic is that we can count the possible outcomes.\n\n### Probability Mass Functions (PMFs)\n\nA Probability Mass Function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value.  We denote the PMF of a discrete random variable $X$ as $P(X=x)$, where $x$ represents a specific value the random variable can take.  The PMF must satisfy two conditions:\n\n1. $P(X=x) \\ge 0$ for all $x$. (Probabilities are non-negative)\n2. $\\sum_{x} P(X=x) = 1$ (The sum of probabilities over all possible outcomes is 1)\n\nLet's consider an example: rolling a fair six-sided die.  The random variable $X$ represents the outcome of the roll. The PMF is:\n\n$P(X=x) = \\begin{cases} \\frac{1}{6} & x \\in \\{1, 2, 3, 4, 5, 6\\} \\\\ 0 & \\text{otherwise} \\end{cases}$\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(1, 7)\ny = np.full(6, 1/6)\n\nplt.stem(x, y)\nplt.xlabel(\"Outcome (x)\")\nplt.ylabel(\"Probability P(X=x)\")\nplt.title(\"PMF of a Fair Six-Sided Die\")\nplt.xticks(x)\nplt.show()\n```\n\n### Cumulative Distribution Functions (CDFs)\n\nThe Cumulative Distribution Function (CDF) of a discrete random variable $X$, denoted as $F(x)$, gives the probability that $X$ is less than or equal to a specific value $x$.  Formally:\n\n$F(x) = P(X \\le x) = \\sum_{i \\le x} P(X=i)$\n\nFor our six-sided die example, the CDF is:\n\n$F(x) = \\begin{cases} 0 & x < 1 \\\\ \\frac{1}{6} & 1 \\le x < 2 \\\\ \\frac{2}{6} & 2 \\le x < 3 \\\\ \\frac{3}{6} & 3 \\le x < 4 \\\\ \\frac{4}{6} & 4 \\le x < 5 \\\\ \\frac{5}{6} & 5 \\le x < 6 \\\\ 1 & x \\ge 6 \\end{cases}$\n\n\n```\\{python}\n#| echo: true\nx = np.arange(1, 7)\ny = np.cumsum(np.full(6, 1/6))\n\nplt.step(x, y)\nplt.xlabel(\"Outcome (x)\")\nplt.ylabel(\"Cumulative Probability F(x)\")\nplt.title(\"CDF of a Fair Six-Sided Die\")\nplt.xticks(x)\nplt.show()\n```\n\n### Expectation and Variance\n\nThe *expectation* (or expected value) of a discrete random variable $X$, denoted as $E[X]$ or $\\mu$, is the average value of $X$ weighted by its probabilities:\n\n$E[X] = \\mu = \\sum_{x} x \\cdot P(X=x)$\n\nFor the six-sided die:\n\n$E[X] = 1(\\frac{1}{6}) + 2(\\frac{1}{6}) + 3(\\frac{1}{6}) + 4(\\frac{1}{6}) + 5(\\frac{1}{6}) + 6(\\frac{1}{6}) = 3.5$\n\nThe *variance*, denoted as $Var(X)$ or $\\sigma^2$, measures the spread or dispersion of the random variable around its mean:\n\n$Var(X) = E[(X - \\mu)^2] = \\sum_{x} (x - \\mu)^2 \\cdot P(X=x)$\n\nAlternatively, it can be calculated as:\n\n$Var(X) = E[X^2] - (E[X])^2$\n\nwhere $E[X^2] = \\sum_{x} x^2 \\cdot P(X=x)$\n\n\nFor the six-sided die:\n\n$E[X^2] = 1^2(\\frac{1}{6}) + 2^2(\\frac{1}{6}) + 3^2(\\frac{1}{6}) + 4^2(\\frac{1}{6}) + 5^2(\\frac{1}{6}) + 6^2(\\frac{1}{6}) = \\frac{91}{6}$\n\n$Var(X) = \\frac{91}{6} - (3.5)^2 = \\frac{35}{12} \\approx 2.92$\n\n```\\{python}\n#| echo: true\nmu = np.mean(x)\nvariance = np.var(x)\n\nprint(f\"Expectation (mu): {mu}\")\nprint(f\"Variance: {variance}\")\n\n```\n\n\n## Binomial Distribution\n\n### The Binomial Experiment\n\nA binomial experiment is a statistical experiment that satisfies the following conditions:\n\n1. **Fixed number of trials:** The experiment consists of a fixed number, $n$, of identical trials.\n2. **Independent trials:** The outcome of each trial is independent of the outcomes of other trials.\n3. **Two outcomes:** Each trial has only two possible outcomes, often called \"success\" and \"failure\".\n4. **Constant probability of success:** The probability of success, denoted by $p$, is constant for each trial. The probability of failure is then $1-p$, often denoted as $q$.\n\nExamples include flipping a coin n times, testing n light bulbs for defects, or surveying n people about their preference for a product.\n\n### Binomial PMF and CDF\n\nThe probability mass function (PMF) of a binomial random variable $X$, representing the number of successes in $n$ trials, is given by:\n\n$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$  for  $k = 0, 1, 2, ..., n$\n\nwhere $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the binomial coefficient, representing the number of ways to choose $k$ successes from $n$ trials.\n\nThe cumulative distribution function (CDF) is:\n\n$F(k) = P(X \\le k) = \\sum_{i=0}^{k} \\binom{n}{i} p^i (1-p)^{n-i}$\n\n\n### Calculating Binomial Probabilities with Python\n\nPython's `scipy.stats` module provides functions for working with the binomial distribution.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n# Parameters\nn = 10  # Number of trials\np = 0.3 # Probability of success\n\n# PMF\nk = np.arange(0, n + 1)\npmf = binom.pmf(k, n, p)\n\n#CDF\ncdf = binom.cdf(k,n,p)\n\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.stem(k, pmf)\nplt.title('Binomial PMF (n=10, p=0.3)')\nplt.xlabel('Number of Successes (k)')\nplt.ylabel('Probability P(X=k)')\n\nplt.subplot(1, 2, 2)\nplt.step(k, cdf)\nplt.title('Binomial CDF (n=10, p=0.3)')\nplt.xlabel('Number of Successes (k)')\nplt.ylabel('Cumulative Probability P(X≤k)')\n\nplt.tight_layout()\nplt.show()\n\n\n#Example calculation: Probability of exactly 3 successes\nprobability_3_successes = binom.pmf(3, n, p)\nprint(f\"Probability of exactly 3 successes: {probability_3_successes}\")\n\n#Example calculation: Probability of at most 3 successes\nprobability_at_most_3_successes = binom.cdf(3, n, p)\nprint(f\"Probability of at most 3 successes: {probability_at_most_3_successes}\")\n```\n\n### Expectation and Variance of the Binomial Distribution\n\nThe expectation (mean) and variance of a binomial distribution are:\n\n$E[X] = np$\n$Var(X) = np(1-p)$\n\nFor our example (n=10, p=0.3):\n\n$E[X] = 10 \\times 0.3 = 3$\n$Var(X) = 10 \\times 0.3 \\times (1 - 0.3) = 2.1$\n\n\n```\\{python}\n#| echo: true\nmean = n * p\nvariance = n * p * (1 - p)\nprint(f\"Expectation: {mean}\")\nprint(f\"Variance: {variance}\")\n```\n\n### Example: A/B Testing with the Binomial Distribution\n\nA/B testing is a common application of the binomial distribution. Suppose we have two versions of a website (A and B) and we want to see which one has a higher conversion rate (e.g., users clicking a \"buy\" button).  We randomly assign users to either version A or version B and track the number of conversions in each group. We can use the binomial distribution to model the number of conversions in each group and perform hypothesis testing to determine if there's a statistically significant difference between the conversion rates.\n\n\n### Bayesian Approach to Binomial Distribution\n\nIn a Bayesian approach to the binomial distribution, we treat the probability of success, $p$, as a random variable itself, rather than a fixed constant. We start with a prior distribution for $p$ (reflecting our initial beliefs about the likely value of $p$) and update this prior based on observed data using Bayes' theorem.  A common prior for $p$ is the Beta distribution, which is conjugate to the binomial distribution (meaning the posterior distribution is also a Beta distribution).\n\nLet's say we observe $k$ successes in $n$ trials.  If the prior for $p$ is a Beta distribution with parameters $\\alpha$ and $\\beta$, denoted as $Beta(\\alpha, \\beta)$, then the posterior distribution for $p$ is:\n\n$P(p | k, n) \\sim Beta(\\alpha + k, \\beta + n - k)$\n\n```\\{python}\n#| echo: true\nfrom scipy.stats import beta\nimport matplotlib.pyplot as plt\n\n#Prior distribution parameters\nalpha_prior = 1\nbeta_prior = 1\n\n# Observed data\nk = 6 #Number of successes\nn = 10 #Number of trials\n\n# Posterior distribution parameters\nalpha_posterior = alpha_prior + k\nbeta_posterior = beta_prior + n - k\n\n#Plot prior and posterior distributions\n\nx = np.linspace(0, 1, 100)\nprior = beta.pdf(x, alpha_prior, beta_prior)\nposterior = beta.pdf(x, alpha_posterior, beta_posterior)\n\nplt.plot(x, prior, label='Prior Distribution')\nplt.plot(x, posterior, label='Posterior Distribution')\nplt.xlabel('Probability of Success (p)')\nplt.ylabel('Density')\nplt.title('Prior and Posterior Distributions for p')\nplt.legend()\nplt.show()\n```\n\nThis posterior distribution summarizes our updated belief about the true probability of success after observing the data. We can calculate credible intervals from this posterior distribution to quantify the uncertainty about p.\n\n\n## Poisson Distribution\n\n### Understanding Poisson Processes\n\nA Poisson process models the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known average rate and independently of the time since the last event.  The key characteristics of a Poisson process are:\n\n1. **Events are independent:** The occurrence of one event doesn't affect the probability of another event occurring.\n2. **Constant average rate:** Events occur at a constant average rate, denoted by λ (lambda).\n3. **Events are random:** The probability of an event occurring in a small time interval is proportional to the length of the interval.\n\n\nExamples include:\n\n* The number of cars passing a certain point on a highway in an hour.\n* The number of customers arriving at a store in a day.\n* The number of typos on a page of a book.\n\n\n### Poisson PMF and CDF\n\nThe probability mass function (PMF) of a Poisson random variable X, representing the number of events in a given interval, is:\n\n$P(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}$ for $k = 0, 1, 2, ...$\n\nwhere:\n\n* $k$ is the number of events\n* λ is the average rate of events\n* $e$ is the base of the natural logarithm (approximately 2.71828)\n\n\nThe cumulative distribution function (CDF) is:\n\n$F(k) = P(X \\le k) = \\sum_{i=0}^{k} \\frac{e^{-\\lambda}\\lambda^i}{i!}$\n\n\n### Calculating Poisson Probabilities with Python\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\n\n# Parameter\nlam = 5  # Average rate of events\n\n# PMF\nk = np.arange(0, 15)  # Range of events\npmf = poisson.pmf(k, lam)\n\n# CDF\ncdf = poisson.cdf(k, lam)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.stem(k, pmf)\nplt.title('Poisson PMF (λ=5)')\nplt.xlabel('Number of Events (k)')\nplt.ylabel('Probability P(X=k)')\n\nplt.subplot(1, 2, 2)\nplt.step(k, cdf)\nplt.title('Poisson CDF (λ=5)')\nplt.xlabel('Number of Events (k)')\nplt.ylabel('Cumulative Probability P(X≤k)')\n\nplt.tight_layout()\nplt.show()\n\n# Example calculation: Probability of exactly 3 events\nprobability_3_events = poisson.pmf(3, lam)\nprint(f\"Probability of exactly 3 events: {probability_3_events}\")\n\n# Example calculation: Probability of at most 3 events\nprobability_at_most_3_events = poisson.cdf(3, lam)\nprint(f\"Probability of at most 3 events: {probability_at_most_3_events}\")\n```\n\n\n### Expectation and Variance of the Poisson Distribution\n\nThe expectation (mean) and variance of a Poisson distribution are both equal to λ:\n\n$E[X] = λ$\n$Var(X) = λ$\n\n\n```\\{python}\n#| echo: true\nmean = lam\nvariance = lam\nprint(f\"Expectation: {mean}\")\nprint(f\"Variance: {variance}\")\n```\n\n### Example: Modeling Website Traffic with the Poisson Distribution\n\nThe number of visitors to a website in a given hour can often be modeled using a Poisson distribution.  If the average hourly rate of visitors is known, we can use the Poisson distribution to calculate the probability of a certain number of visitors in that hour. This can be useful for planning server capacity or predicting website load.\n\n### Bayesian Approach to Poisson Distribution\n\nIn a Bayesian approach to the Poisson distribution, we treat the rate parameter λ as a random variable. A common prior for λ is the Gamma distribution, which is conjugate to the Poisson distribution.\n\nIf the prior for λ is a Gamma distribution with shape parameter α and rate parameter β, denoted as $Gamma(α, β)$, and we observe k events, then the posterior distribution for λ is:\n\n$P(λ | k) \\sim Gamma(α + k, β + 1)$\n\n```\\{python}\n#| echo: true\nfrom scipy.stats import gamma\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Prior parameters\nalpha_prior = 2\nbeta_prior = 1\n\n#Observed data\nk = 8 #Number of events\n\n\n# Posterior parameters\nalpha_posterior = alpha_prior + k\nbeta_posterior = beta_prior + 1\n\n#Plot prior and posterior distributions\nx = np.linspace(0, 20, 100) # Adjust range as needed\nprior = gamma.pdf(x, alpha_prior, scale=1/beta_prior)\nposterior = gamma.pdf(x, alpha_posterior, scale=1/beta_posterior)\n\nplt.plot(x, prior, label='Prior Distribution')\nplt.plot(x, posterior, label='Posterior Distribution')\nplt.xlabel('Rate Parameter (λ)')\nplt.ylabel('Density')\nplt.title('Prior and Posterior Distributions for λ')\nplt.legend()\nplt.show()\n```\n\nThe posterior distribution represents our updated belief about the true rate parameter λ after observing the data.  We can use this posterior to make inferences about future events or to quantify uncertainty about λ.\n\n\n## Categorical Data and Probability\n\n### Representing Categorical Data\n\nCategorical data represents observations that can be assigned to categories or groups.  These categories are often qualitative (e.g., colors, types of fruit) rather than quantitative (numerical).  Categorical data can be nominal (unordered, like colors) or ordinal (ordered, like education levels: high school, bachelor's, master's).  We often represent categorical data using frequency counts or proportions for each category.\n\n### Probability Mass Functions for Categorical Data\n\nThe probability mass function (PMF) for categorical data assigns a probability to each category.  Let $C_1, C_2, ..., C_k$ be the categories, and let $P(C_i)$ be the probability of observing category $C_i$.  The PMF satisfies:\n\n* $P(C_i) \\ge 0$ for all $i$\n* $\\sum_{i=1}^{k} P(C_i) = 1$\n\nFor example, if we have categories \"Red,\" \"Green,\" and \"Blue,\" the PMF might be:\n\n$P(\\text{Red}) = 0.4$\n$P(\\text{Green}) = 0.3$\n$P(\\text{Blue}) = 0.3$\n\n\n### Multinomial Distribution\n\nThe multinomial distribution is a generalization of the binomial distribution to more than two categories.  It models the probabilities of observing counts for each category in a fixed number of independent trials, where each trial has the same set of possible outcomes (categories) with constant probabilities.\n\nLet $X_i$ be the count of observations in category $C_i$, and let $n$ be the total number of trials. The probability mass function of the multinomial distribution is:\n\n$P(X_1 = x_1, X_2 = x_2, ..., X_k = x_k) = \\frac{n!}{x_1!x_2!...x_k!} \\prod_{i=1}^{k} p_i^{x_i}$\n\nwhere:\n\n* $x_i$ is the observed count in category $C_i$\n* $p_i$ is the probability of observing category $C_i$\n* $\\sum_{i=1}^{k} x_i = n$\n* $\\sum_{i=1}^{k} p_i = 1$\n\n\n### Calculating Probabilities with Categorical Data in Python\n\n```\\{python}\n#| echo: true\nimport numpy as np\nfrom scipy.stats import multinomial\n\n#Probabilities for each category\np = np.array([0.4, 0.3, 0.3]) #Red, Green, Blue\n\n# Number of trials\nn = 10\n\n# Example: probability of observing 4 Red, 3 Green, 3 Blue\nx = np.array([4, 3, 3])\nprobability = multinomial.pmf(x, n, p)\nprint(f\"Probability: {probability}\")\n\n\n#Visualizing the distribution (challenging for >2 categories) -  A bar chart showing the probabilities.\ncategories = ['Red', 'Green', 'Blue']\nplt.bar(categories, p)\nplt.ylabel('Probability')\nplt.title('Probability Mass Function for Colors')\nplt.show()\n\n```\n\n### Example: Analyzing Survey Results\n\nImagine a survey asking respondents to choose their favorite ice cream flavor from chocolate, vanilla, and strawberry. The survey results can be analyzed using the multinomial distribution to determine the probability of observing specific combinations of flavor choices.\n\n### Bayesian Approach to Categorical Data\n\nThe Bayesian approach to categorical data involves placing prior distributions on the category probabilities ($p_i$).  A common choice is the Dirichlet distribution, which is the conjugate prior to the multinomial distribution. The Dirichlet distribution is parameterized by a vector of concentration parameters,  $\\alpha = (\\alpha_1, \\alpha_2, ..., \\alpha_k)$, where $\\alpha_i > 0$ for all i.\n\nIf the prior is $Dir(\\alpha)$, and we observe counts $x = (x_1, x_2, ..., x_k)$, then the posterior distribution is:\n\n$P(p | x) \\sim Dir(\\alpha + x)$\n\n```\\{python}\n#| echo: true\nfrom scipy.stats import dirichlet\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Prior parameters (symmetric Dirichlet, implies prior belief of equal probabilities)\nalpha_prior = np.array([1, 1, 1])  # For Red, Green, Blue\n\n\n#Observed data\nx = np.array([4,3,3]) #Counts of Red, Green, Blue\n\n# Posterior parameters\nalpha_posterior = alpha_prior + x\n\n#Plotting the posterior - this requires more sophisticated visualization for >2 categories\n#This example demonstrates 2D projection for simplicity.  More complex visualization methods are needed for higher dimensions.\n#We can only effectively visualize the first two categories.\n\nsamples = dirichlet.rvs(alpha_posterior, size=10000) #Generating samples to visualize the 2D distribution\nplt.scatter(samples[:, 0], samples[:,1], alpha=0.2)\nplt.xlabel('Probability of Red')\nplt.ylabel('Probability of Green')\nplt.title('Posterior Distribution of Probabilities')\nplt.show()\n```\n\nThis posterior Dirichlet distribution represents our updated beliefs about the category probabilities after considering the observed data.  We can draw samples from this posterior to estimate credible intervals for each category probability.  More advanced visualization techniques would be needed to represent the full higher-dimensional posterior if there were more categories.\n\n\n## Applying Bayes' Theorem to Discrete Distributions\n\nThis section demonstrates how to apply Bayes' Theorem to update our beliefs about the parameters of various discrete probability distributions after observing data. We'll use Python to perform the calculations and visualize the results.\n\n\n### Bayes' Theorem with Binomial Data\n\nLet's say we have a binomial distribution with parameters $n$ (number of trials) and $p$ (probability of success).  We want to infer the value of $p$ given some observed data. We'll use a Beta distribution as the prior for $p$ because it's conjugate to the binomial likelihood, simplifying calculations.\n\nBayes' Theorem states:\n\n$P(p|data) = \\frac{P(data|p)P(p)}{P(data)}$\n\n* **Prior:** $P(p) \\sim Beta(\\alpha, \\beta)$\n* **Likelihood:** $P(data|p) \\sim Binomial(n, p)$\n* **Posterior:** $P(p|data) \\sim Beta(\\alpha + k, \\beta + n - k)$ where k is the number of successes observed.\n* **Evidence:** $P(data)$ is a normalizing constant that ensures the posterior integrates to 1.\n\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, binom\n\n# Prior parameters\nalpha_prior = 2\nbeta_prior = 2\n\n# Observed data\nn = 10\nk = 7\n\n# Posterior parameters\nalpha_posterior = alpha_prior + k\nbeta_posterior = beta_prior + n - k\n\n# Plot prior and posterior\nx = np.linspace(0, 1, 100)\nprior = beta.pdf(x, alpha_prior, beta_prior)\nposterior = beta.pdf(x, alpha_posterior, beta_posterior)\n\nplt.plot(x, prior, label='Prior Distribution')\nplt.plot(x, posterior, label='Posterior Distribution')\nplt.xlabel('Probability of Success (p)')\nplt.ylabel('Density')\nplt.title('Prior and Posterior Distributions for p')\nplt.legend()\nplt.show()\n```\n\n### Bayes' Theorem with Poisson Data\n\nFor a Poisson distribution with rate parameter λ, we can use a Gamma distribution as the conjugate prior.\n\n* **Prior:** $P(λ) \\sim Gamma(α, β)$\n* **Likelihood:** $P(data|λ) \\sim Poisson(λ)$\n* **Posterior:** $P(λ|data) \\sim Gamma(α + k, β + n)$ where k is the total number of events observed over n intervals.\n\n\n```\\{python}\n#| echo: true\nfrom scipy.stats import gamma, poisson\n\n# Prior parameters\nalpha_prior = 2\nbeta_prior = 1\n\n# Observed data (assuming we observed 8 events over 2 intervals)\nk = 8  \nn = 2\n\n# Posterior parameters\nalpha_posterior = alpha_prior + k\nbeta_posterior = beta_prior + n\n\n# Plot prior and posterior\nx = np.linspace(0, 15, 100) # adjust the range as necessary\nprior = gamma.pdf(x, alpha_prior, scale=1/beta_prior)\nposterior = gamma.pdf(x, alpha_posterior, scale=1/beta_posterior)\n\nplt.plot(x, prior, label='Prior Distribution')\nplt.plot(x, posterior, label='Posterior Distribution')\nplt.xlabel('Rate Parameter (λ)')\nplt.ylabel('Density')\nplt.title('Prior and Posterior Distributions for λ')\nplt.legend()\nplt.show()\n\n```\n\n\n### Bayes' Theorem with Categorical Data\n\nFor categorical data, we use the multinomial distribution for the likelihood and the Dirichlet distribution for the prior on the category probabilities.\n\n* **Prior:** $P(p) \\sim Dirichlet(\\alpha)$ where α is a vector of concentration parameters.\n* **Likelihood:** $P(data|p) \\sim Multinomial(n, p)$\n* **Posterior:** $P(p|data) \\sim Dirichlet(\\alpha + x)$ where x is a vector of observed counts for each category.\n\n\n\n```\\{python}\n#| echo: true\nfrom scipy.stats import dirichlet\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Prior parameters (symmetric Dirichlet)\nalpha_prior = np.array([1, 1, 1])  # For 3 categories\n\n# Observed data\nx = np.array([4, 3, 3])\n\n# Posterior parameters\nalpha_posterior = alpha_prior + x\n\n#Plotting the posterior distribution (2D projection for visualization).\nsamples = dirichlet.rvs(alpha_posterior, size=10000)\nplt.scatter(samples[:, 0], samples[:,1], alpha=0.2)\nplt.xlabel('Probability of Category 1')\nplt.ylabel('Probability of Category 2')\nplt.title('Posterior Distribution of Probabilities')\nplt.show()\n\n```\n\n### Prior and Posterior Distributions\n\nIn all the above examples, the prior distribution reflects our initial beliefs about the parameter(s) before observing any data. The posterior distribution, calculated using Bayes' theorem, represents our updated beliefs after incorporating the observed data.  The shift from prior to posterior shows how the data has influenced our understanding of the parameter.\n\n\n### Illustrative Examples with Python Code\n\nThe code snippets above provide illustrative examples of applying Bayes' theorem to update beliefs about parameters of binomial, Poisson, and multinomial distributions.  They highlight the use of conjugate priors to simplify calculations and the power of Bayesian methods in updating our understanding of probability distributions in light of new evidence.  Remember to adjust prior parameters to reflect your initial beliefs and data accordingly.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"discrete-probability-examples.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"discrete-probability-examples.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}