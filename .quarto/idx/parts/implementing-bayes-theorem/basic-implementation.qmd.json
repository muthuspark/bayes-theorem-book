{"title":"Basic Implementation","markdown":{"headingText":"Basic Implementation","containsRefs":false,"markdown":"\n### A Coin Toss Example\n\nLet's start with a simple example: tossing a fair coin.  We want to calculate the probability of getting heads ($H$) given that we've already observed one head in two tosses.  Intuitively, we might think the probability remains 0.5, but Bayes' Theorem allows us to formally express and calculate this.\n\nWe can define the following:\n\n* $P(H)$: Prior probability of getting heads (0.5 for a fair coin).\n* $P(T)$: Prior probability of getting tails (0.5 for a fair coin).\n* $A$: Event of observing one head in two tosses.\n\nWe want to find $P(H|A)$, the probability of getting heads given event A.  Using Bayes' Theorem:\n\n$P(H|A) = \\frac{P(A|H)P(H)}{P(A)}$\n\nCalculating the components:\n\n* $P(H) = 0.5$\n* $P(A|H)$: Probability of observing one head in two tosses given the first toss was heads. This is the probability of getting one head and one tail in the remaining toss, which is $\\binom{1}{1}(0.5)^1(0.5)^1 = 0.5$.\n* $P(A)$: Probability of observing one head in two tosses.  This can happen in two ways: HT or TH.  Therefore, $P(A) = P(HT) + P(TH) = (0.5)^2 + (0.5)^2 = 0.5$.\n\nPlugging these values into Bayes' Theorem:\n\n$P(H|A) = \\frac{0.5 \\times 0.5}{0.5} = 0.5$\n\nAs expected, the posterior probability remains 0.5.  The prior information didn't change the probability of getting heads on the next toss.\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\n\n# Prior probabilities\nprior_H = 0.5\nprior_T = 0.5\n\n# Likelihood (probability of observing one head in two tosses given heads on the first toss)\nlikelihood_H = 0.5\n\n# Probability of observing one head in two tosses\nprob_A = 0.5\n\n# Posterior probability\nposterior_H = (likelihood_H * prior_H) / prob_A\n\nprint(f\"Posterior probability of getting heads: {posterior_H}\")\n\n# Visualization (simple bar chart)\nplt.bar(['Prior', 'Posterior'], [prior_H, posterior_H], color=['skyblue', 'coral'])\nplt.ylabel(\"Probability\")\nplt.title(\"Prior vs. Posterior Probability of Heads\")\nplt.show()\n```\n\n### The Monty Hall Problem\n\nThe Monty Hall problem is a classic example illustrating the impact of new information on prior beliefs.  A contestant chooses one of three doors. Behind one door is a car, and behind the others are goats.  After the contestant chooses a door, Monty Hall (the host), who knows where the car is, opens one of the *unchosen* doors to reveal a goat.  The contestant is then given the option to switch doors. Should they?\n\nBayes' Theorem helps clarify the situation. Let's define:\n\n* $C_i$:  The car is behind door $i$ (i = 1, 2, 3).  $P(C_i) = \\frac{1}{3}$ for each $i$.\n* $M_j$: Monty opens door $j$.\n* $D_k$: The contestant initially chooses door $k$.\n\nLet's assume the contestant chooses door 1 ($D_1$).  Monty opens door 3 revealing a goat ($M_3$). We want to calculate the probability that the car is behind door 2 ($P(C_2|D_1, M_3)$) given the information.  Applying Bayes' theorem:\n\n\n$P(C_2|D_1, M_3) = \\frac{P(M_3|C_2, D_1)P(C_2)}{P(M_3|D_1)}$\n\n\n* $P(C_2) = \\frac{1}{3}$ (prior probability)\n* $P(M_3|C_2, D_1) = 1$ (Monty *must* open door 3 if the car is behind door 2 and the contestant chose door 1)\n* $P(M_3|D_1) = P(M_3|C_2, D_1)P(C_2) + P(M_3|C_3, D_1)P(C_3) = 1 \\times \\frac{1}{3} + \\frac{1}{2} \\times \\frac{1}{3} = \\frac{1}{2}$ (This is the probability Monty opens door 3 given the contestant chose door 1.  It considers both possibilities of the car being behind door 2 or 3).\n\nTherefore:\n\n$P(C_2|D_1, M_3) = \\frac{1 \\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}$\n\nThis shows that switching doors doubles the probability of winning the car.\n\n### Bayes' Theorem with Dice\n\nLet's consider two six-sided dice, one fair (die A) and one loaded (die B).  Die B has a probability of $\\frac{1}{2}$ of rolling a 6 and $\\frac{1}{10}$ for each of the other numbers (1-5). We roll one of the dice (we don't know which one) and observe a 6. What's the probability it was die B?\n\nLet:\n\n* $A$: Event that we choose die A. $P(A) = 0.5$\n* $B$: Event that we choose die B. $P(B) = 0.5$\n* $S_6$: Event of rolling a 6.\n\nWe want to find $P(B|S_6)$.  Using Bayes' Theorem:\n\n$P(B|S_6) = \\frac{P(S_6|B)P(B)}{P(S_6)}$\n\n* $P(B) = 0.5$\n* $P(S_6|B) = 0.5$ (Probability of rolling a 6 given die B)\n* $P(S_6) = P(S_6|A)P(A) + P(S_6|B)P(B) = \\frac{1}{6} \\times 0.5 + 0.5 \\times 0.5 = \\frac{1}{12} + \\frac{1}{4} = \\frac{1}{3}$ (Law of Total Probability)\n\nTherefore:\n\n$P(B|S_6) = \\frac{0.5 \\times 0.5}{\\frac{1}{3}} = \\frac{3}{4} = 0.75$\n\nThe probability that it was die B given we rolled a 6 is 0.75.\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Prior probabilities\nprior_A = 0.5\nprior_B = 0.5\n\n# Likelihoods\nlikelihood_6_A = 1/6\nlikelihood_6_B = 0.5\n\n# Probability of rolling a 6\nprob_6 = (likelihood_6_A * prior_A) + (likelihood_6_B * prior_B)\n\n# Posterior probability\nposterior_B = (likelihood_6_B * prior_B) / prob_6\n\nprint(f\"Posterior probability that it was die B: {posterior_B}\")\n\n\n#Visualization\nlabels = 'Die A', 'Die B'\nprior = [prior_A, prior_B]\nposterior = [1 - posterior_B, posterior_B]\n\nwidth = 0.35       \n\nfig, ax = plt.subplots()\nrects1 = ax.bar(np.arange(len(labels)) - width/2, prior, width, label='Prior')\nrects2 = ax.bar(np.arange(len(labels)) + width/2, posterior, width, label='Posterior')\n\n\nax.set_ylabel('Probabilities')\nax.set_title('Prior vs Posterior Probabilities')\nax.set_xticks(np.arange(len(labels)))\nax.set_xticklabels(labels)\nax.legend()\n\nfig.tight_layout()\nplt.show()\n```\n\n```{mermaid}\ngraph LR\nA[Prior: Choose Die A (0.5)] --> |Roll a 6| C{P(6|A) = 1/6};\nB[Prior: Choose Die B (0.5)] --> |Roll a 6| C{P(6|B) = 0.5};\nC --> D[P(6) = 1/3];\nD --> E[Posterior: Die B (0.75)];\n```\n\n\n## Leveraging NumPy for Efficient Calculations\n\n### NumPy Arrays and Bayes' Theorem\n\nNumPy, Python's numerical computing library, provides significant advantages when working with Bayes' Theorem, especially when dealing with larger datasets or more complex scenarios.  Its core data structure, the NumPy array, allows for vectorized operations, making calculations significantly faster than using standard Python lists and loops.  This efficiency becomes crucial when dealing with high-dimensional probability distributions or numerous data points.\n\nInstead of calculating probabilities element-by-element, NumPy enables us to perform operations across entire arrays simultaneously. This vectorization dramatically improves performance, particularly for large datasets where the computational cost of iterative loops can be prohibitive.\n\n### Calculating Probabilities with NumPy\n\nLet's revisit the coin toss example from the previous section, but now using NumPy.  Suppose we have 1000 coin tosses, and we want to calculate the probability of getting heads given that we've already observed a certain number of heads in the first 500 tosses.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate 1000 coin tosses (0 for tails, 1 for heads)\ntosses = np.random.randint(0, 2, 1000)\n\n# First 500 tosses\nfirst_500 = tosses[:500]\n\n# Number of heads in the first 500 tosses\nnum_heads = np.sum(first_500)\n\n# Prior probability (assuming a fair coin)\nprior_heads = 0.5\n\n# Likelihood (probability of observing the remaining tosses given the first 500)\n#  We'll simplify this for demonstration; a more rigorous approach would involve a binomial distribution.\n#  This simplified approach assumes independence between tosses.\n\nlikelihood_heads = num_heads / 500  #Simplified likelihood\n\n# Posterior probability using NumPy\nposterior_heads = (likelihood_heads * prior_heads) / ((likelihood_heads * prior_heads) + ((1-likelihood_heads) * (1-prior_heads)))\n\nprint(f\"Number of heads in first 500 tosses: {num_heads}\")\nprint(f\"Posterior probability of heads (NumPy): {posterior_heads}\")\n\n#Visualization\nplt.bar(['Prior', 'Posterior'], [prior_heads, posterior_heads], color=['skyblue', 'coral'])\nplt.ylabel(\"Probability\")\nplt.title(\"Prior vs. Posterior Probability of Heads (NumPy)\")\nplt.show()\n```\n\nThis code demonstrates the efficiency of NumPy: calculations that would involve explicit looping in standard Python are now handled efficiently by NumPy's vectorized operations.\n\n### Handling Large Datasets with NumPy\n\n\nWhen dealing with very large datasets, NumPy's memory efficiency and optimized functions become even more critical. Consider a scenario with millions of data points and multiple features.  Using standard Python lists and loops would be extremely slow and could easily exhaust available memory. NumPy's arrays, however, are designed for efficient storage and manipulation of large numerical data.\n\nLet's illustrate with a simplified example of Bayesian classification:\n\n```\\{python}\n#| echo: true\nimport numpy as np\n\n# Simulate a large dataset (1 million data points, 2 features)\ndata = np.random.rand(1000000, 2)\nlabels = np.random.randint(0, 2, 1000000)  # 0 or 1 labels\n\n#Let's assume a simple Gaussian Naive Bayes for demonstration. We will avoid explicit calculation of probabilities for brevity.  In a real-world scenario, you would use a library like scikit-learn for this.\n\n\n#In a real application you'd use a library like scikit-learn for efficient calculation of probabilities and posterior predictions.\n\n#Example of calculating means for each class using NumPy\nclass0_indices = labels == 0\nclass1_indices = labels ==1\nmean_class0 = np.mean(data[class0_indices,:], axis=0)\nmean_class1 = np.mean(data[class1_indices,:], axis=0)\n\n\nprint(f\"Mean of features for class 0: {mean_class0}\")\nprint(f\"Mean of features for class 1: {mean_class1}\")\n\n```\n\nThis example showcases how NumPy handles the large dataset efficiently.  In a real-world application, you would incorporate more sophisticated Bayesian methods (like those found in libraries such as scikit-learn), but the foundation of efficient data handling remains NumPy arrays.  Note that for true Bayesian classification on large datasets, libraries like scikit-learn are highly recommended due to their optimized implementations.\n\n\n```{mermaid}\ngraph LR\nA[Large Dataset (Millions of points)] --> B(NumPy Array);\nB --> C[Efficient Storage];\nB --> D[Vectorized Operations];\nD --> E[Fast Probability Calculations];\nE --> F[Bayesian Classification];\n```\n\n\n## Visualizing Results with Matplotlib\n\nMatplotlib is a powerful Python library for creating static, interactive, and animated visualizations.  It's invaluable for understanding and communicating the results of Bayesian calculations.  Visualizing probability distributions, particularly the evolution from prior to posterior, is crucial for intuitive grasping of Bayes' Theorem's impact.\n\n\n### Creating Histograms and Probability Distributions\n\nBefore diving into Bayesian visualizations, let's review how to create histograms and probability distributions with Matplotlib.  Histograms are excellent for displaying the frequency distribution of data, while various plotting functions allow for visualizing probability density functions (PDFs).\n\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some sample data\ndata = np.random.normal(loc=0, scale=1, size=1000)  # Normal distribution\n\n# Create a histogram\nplt.hist(data, bins=30, density=True, alpha=0.7, color='skyblue', label='Histogram')\n\n# Overlay a probability density function (PDF)\nx = np.linspace(-4, 4, 100)\ny = (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2) #PDF of standard normal distribution\nplt.plot(x, y, 'r-', label='PDF')\n\n\nplt.xlabel('Value')\nplt.ylabel('Frequency/Density')\nplt.title('Histogram and Probability Density Function')\nplt.legend()\nplt.show()\n\n```\n\nThis code generates a histogram of the sample data and overlays the theoretical probability density function of the standard normal distribution for comparison.\n\n\n### Visualizing Prior and Posterior Distributions\n\nVisualizing the prior and posterior distributions is key to understanding how evidence updates our beliefs.  We can use Matplotlib to plot both distributions on the same graph, clearly showcasing the shift in probability mass after incorporating new evidence.\n\nLet's consider a simple example where we have a prior belief about the mean of a normal distribution, and we then observe some data points.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Prior distribution parameters\nprior_mean = 0\nprior_std = 1\n\n# Observed data (example)\ndata = np.array([0.5, 1.2, 0.8, 1.0])\n\n# Posterior distribution (simplified calculation - in practice, use conjugate priors for easier calculations)\n\nposterior_mean = np.mean(data)\nposterior_std = 1 # Simplified for demonstration. A proper calculation would incorporate the prior variance and data variance.\n\n\n#Plot prior and posterior\nx = np.linspace(-3,3,100)\nplt.plot(x, norm.pdf(x, prior_mean, prior_std), label='Prior Distribution')\nplt.plot(x, norm.pdf(x, posterior_mean, posterior_std), label='Posterior Distribution')\nplt.xlabel('Mean')\nplt.ylabel('Probability Density')\nplt.title('Prior and Posterior Distributions')\nplt.legend()\nplt.show()\n```\n\nThis code plots both the prior and posterior distributions, showing how the posterior is centered closer to the observed data, reflecting the updated belief.  Remember that a proper posterior calculation would involve more complex formulas incorporating prior and data variances.  This example simplifies the calculation for illustrative purposes.\n\n\n### Illustrating the Impact of Evidence\n\nMultiple plots can demonstrate how increasing evidence gradually refines our belief. By plotting posterior distributions for progressively more data, we can visualize the convergence towards a more precise estimate.\n\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Prior distribution\nprior_mean = 0\nprior_std = 1\n\n# Generate data in batches\ndata_batches = [np.random.normal(loc=0.5, scale=1, size=i) for i in [10, 50, 100, 500]]\n\n\n#plotting multiple posteriors (simplified calculations - use conjugate priors for proper Bayesian inference in real-world problems)\nx = np.linspace(-3, 3, 100)\nplt.plot(x, norm.pdf(x, prior_mean, prior_std), label='Prior')\n\nfor i, data in enumerate(data_batches):\n    posterior_mean = np.mean(data)\n    posterior_std = 1 #Simplified for demonstration.  A true calculation would incorporate prior and data variance\n    plt.plot(x, norm.pdf(x, posterior_mean, posterior_std), label=f'Posterior (n={len(data)})')\n\nplt.xlabel('Mean')\nplt.ylabel('Probability Density')\nplt.title('Impact of Increasing Evidence')\nplt.legend()\nplt.show()\n\n```\n\nThis illustrates how the posterior distribution becomes increasingly narrow and centered around the true mean (0.5 in this case) as more data is observed, showcasing the effect of accumulating evidence in Bayesian inference.  Again, simplified posterior calculations are used here; more sophisticated techniques are necessary for precise results in real-world applications.\n\n```{mermaid}\ngraph LR\nA[Prior Distribution] --> B(Evidence);\nB --> C[Posterior Distribution 1];\nC --> D(More Evidence);\nD --> E[Posterior Distribution 2];\nE --> F(More Evidence);\nF --> G[Posterior Distribution 3];\nG --> H[Convergence];\n\n```\n\n\n## Putting it all Together: A Comprehensive Example\n\nThis section combines the techniques discussed earlier to solve a more realistic problem using Bayes' Theorem with Python, NumPy, and Matplotlib.\n\n### Problem Definition and Data Representation\n\nLet's consider a medical diagnostic scenario. We have a test for a rare disease. The prior probability of having the disease ($P(D)$) is 0.01 (1% prevalence). The test has the following characteristics:\n\n* **Sensitivity:** $P(T+|D) = 0.95$ (Probability of a positive test given the disease is present)\n* **Specificity:** $P(T-|¬D) = 0.90$ (Probability of a negative test given the disease is absent)\n\nWe want to determine the probability of having the disease ($P(D|T+)$) given a positive test result.  We'll simulate data to represent a larger population.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\n\n# Parameters\nprior_prob_disease = 0.01\nsensitivity = 0.95\nspecificity = 0.90\n\n# Simulate a population\npopulation_size = 100000\ndisease_status = bernoulli.rvs(prior_prob_disease, size=population_size)  # 1 for disease, 0 for no disease\n\n# Simulate test results\ntest_results = np.zeros(population_size)\ntest_results[disease_status == 1] = bernoulli.rvs(sensitivity, size=np.sum(disease_status))\ntest_results[disease_status == 0] = 1 - bernoulli.rvs(1-specificity, size=np.sum(1-disease_status)) # 1 for positive, 0 for negative\n\n#Count positive tests\npositive_tests = np.sum(test_results==1)\n\n```\n\n\n### Applying Bayes' Theorem using NumPy\n\nWe'll now use Bayes' Theorem to calculate the posterior probability of having the disease given a positive test result:\n\n$P(D|T+) = \\frac{P(T+|D)P(D)}{P(T+)}$\n\nWe need to calculate $P(T+)$, the probability of a positive test result, using the law of total probability:\n\n$P(T+) = P(T+|D)P(D) + P(T+|¬D)P(¬D)$\n\nWhere $P(T+|¬D) = 1 - P(T-|¬D) = 1 - specificity$.  And $P(¬D) = 1 - P(D)$.\n\n```\\{python}\n#| echo: true\n# Calculate P(T+) using NumPy\nprob_positive_test = (sensitivity * prior_prob_disease) + ((1 - specificity) * (1 - prior_prob_disease))\n\n# Calculate P(D|T+) using NumPy\nposterior_prob_disease = (sensitivity * prior_prob_disease) / prob_positive_test\n\n\nprint(f\"Probability of positive test: {prob_positive_test}\")\nprint(f\"Posterior probability of disease given positive test: {posterior_prob_disease}\")\n\n#Alternatively using simulation counts:\nposterior_prob_disease_sim = np.sum((disease_status==1) & (test_results==1)) / positive_tests\nprint(f\"Posterior probability of disease given positive test (simulation): {posterior_prob_disease_sim}\")\n\n\n```\n\n### Visualizing Results and Interpretation\n\nLet's visualize the prior and posterior distributions using Matplotlib.  Since we're dealing with probabilities, we can represent them as simple bar charts.\n\n```\\{python}\n#| echo: true\nplt.bar(['Prior', 'Posterior'], [prior_prob_disease, posterior_prob_disease], color=['skyblue', 'coral'])\nplt.ylabel('Probability')\nplt.title('Prior vs. Posterior Probability of Disease')\nplt.show()\n```\n\nThe chart clearly shows that although the test is quite accurate (high sensitivity and specificity), the posterior probability of having the disease given a positive test result is still relatively low due to the low prior probability. This highlights the importance of considering prior probabilities when interpreting test results, especially for rare diseases.  Even with a positive test, further investigations might be necessary.\n\n\n```{mermaid}\ngraph LR\nA[Prior P(D) = 0.01] --> B(Positive Test Result);\nB --> C[Posterior P(D|T+) ≈ 0.09];\nsubgraph \"\"\nA -- P(T+|D) = 0.95 --> B;\nA -- P(¬D) = 0.99 --> D;\nD -- P(T+|¬D) = 0.1 --> B;\nend\n```\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"basic-implementation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"basic-implementation.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}