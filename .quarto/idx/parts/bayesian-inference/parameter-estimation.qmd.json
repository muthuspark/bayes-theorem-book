{"title":"Introduction to Parameter Estimation","markdown":{"headingText":"Introduction to Parameter Estimation","containsRefs":false,"markdown":"\nParameter estimation is a fundamental problem in statistics, aiming to determine the values of unknown parameters in a statistical model based on observed data.  This chapter explores parameter estimation through the lens of Bayes' Theorem, contrasting it with frequentist approaches. We will learn how Bayes' Theorem allows us to incorporate prior knowledge and update our beliefs about parameters as we gather more data.\n\n### Frequentist vs. Bayesian Approaches\n\nFrequentist and Bayesian approaches to parameter estimation differ fundamentally in their interpretation of probability.\n\n* **Frequentist Approach:**  Frequentists view probability as the long-run frequency of an event.  Parameter estimation focuses on point estimates (e.g., maximum likelihood estimate) and confidence intervals, which are constructed based on the sampling distribution of the estimator.  The true parameter is considered fixed, and the uncertainty is solely attributed to the variability of the data.  For example, maximum likelihood estimation (MLE) seeks to find the parameter values that maximize the likelihood function, $L(\\theta|x) = P(x|\\theta)$, where $x$ is the observed data and $\\theta$ is the parameter.\n\n* **Bayesian Approach:** Bayesians view probability as a degree of belief.  The unknown parameter $\\theta$ is treated as a random variable with a probability distribution.  The Bayesian approach uses Bayes' Theorem to update the prior distribution (our initial belief about $\\theta$) based on the observed data to obtain the posterior distribution.  This posterior distribution represents our updated belief about $\\theta$ after observing the data.\n\n\n### The Role of Bayes' Theorem\n\nBayes' Theorem provides the mathematical framework for updating our beliefs about parameters in light of new data. The theorem states:\n\n$P(\\theta|x) = \\frac{P(x|\\theta)P(\\theta)}{P(x)}$\n\nwhere:\n\n* $P(\\theta|x)$ is the posterior distribution of $\\theta$ given the data $x$. This is what we want to estimate.\n* $P(x|\\theta)$ is the likelihood function, representing the probability of observing the data given a specific value of $\\theta$.\n* $P(\\theta)$ is the prior distribution of $\\theta$, representing our initial belief about the parameter before observing the data.\n* $P(x)$ is the marginal likelihood (or evidence), which acts as a normalizing constant.  It can often be calculated as: $P(x) = \\int P(x|\\theta)P(\\theta)d\\theta$.\n\nIn practice, we often work with the proportional relationship:\n\n$P(\\theta|x) \\propto P(x|\\theta)P(\\theta)$\n\nThis means that the posterior distribution is proportional to the product of the likelihood and the prior.\n\n\n### Prior and Posterior Distributions\n\nThe choice of prior distribution reflects our prior knowledge or beliefs about the parameter.  A non-informative prior expresses a lack of strong prior knowledge, while an informative prior incorporates existing information. The posterior distribution is then obtained by combining the prior and the likelihood.\n\nLet's illustrate this with a simple example using Python.  Suppose we are estimating the mean $\\mu$ of a normal distribution with known variance $\\sigma^2$.  We'll assume a normal prior for $\\mu$:\n\n$\\mu \\sim N(\\mu_0, \\sigma_0^2)$\n\nand observe data $x_1, x_2, ..., x_n$ which are i.i.d. from $N(\\mu, \\sigma^2)$. The likelihood is:\n\n$P(x|\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2})$\n\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Prior parameters\nmu_0 = 0  \nsigma_0 = 1\n\n# Likelihood parameters\nsigma = 1\ndata = np.random.normal(loc=2, scale=sigma, size=10) # Observed data, generating from a distribution with mean 2\n\n# Calculate posterior parameters\nn = len(data)\nmu_n = (mu_0 / sigma_0**2 + np.sum(data) / sigma**2) / (1 / sigma_0**2 + n / sigma**2)\nsigma_n = np.sqrt(1 / (1 / sigma_0**2 + n / sigma**2))\n\n# Plot prior and posterior\nx = np.linspace(-5, 5, 100)\nplt.plot(x, norm.pdf(x, mu_0, sigma_0), label='Prior')\nplt.plot(x, norm.pdf(x, mu_n, sigma_n), label='Posterior')\nplt.xlabel('μ')\nplt.ylabel('Density')\nplt.legend()\nplt.title('Prior and Posterior Distributions of μ')\nplt.show()\n\n```\n\nThis code generates a plot showing how the prior distribution is updated to the posterior distribution after observing the data.  Note that the posterior is a compromise between the prior and the information from the data.  As we observe more data, the influence of the prior diminishes, and the posterior becomes increasingly dominated by the data likelihood.\n\n\n```{mermaid}\ngraph LR\nA[Prior Distribution] --> B(Bayes' Theorem);\nC[Likelihood Function] --> B;\nB --> D[Posterior Distribution];\n```\n\nThis diagram illustrates how Bayes' Theorem combines the prior and likelihood to produce the posterior distribution.\n\n\n## Point Estimation\n\nPoint estimation aims to provide a single best guess for the unknown parameter(s) of a statistical model.  In the Bayesian framework, this is often done by summarizing the posterior distribution.  We'll explore two common approaches: Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation.\n\n### Maximum Likelihood Estimation (MLE)\n\nMaximum Likelihood Estimation (MLE) is a frequentist approach.  It finds the parameter value that maximizes the likelihood function, which is the probability of observing the data given the parameter value.  Formally, we want to find $\\hat{\\theta}_{MLE}$ such that:\n\n$\\hat{\\theta}_{MLE} = \\arg \\max_{\\theta} L(\\theta|x) = \\arg \\max_{\\theta} P(x|\\theta)$\n\nwhere $L(\\theta|x)$ is the likelihood function, $x$ represents the observed data, and $\\theta$ is the parameter we are estimating.  Often, it's easier to work with the log-likelihood, $\\log L(\\theta|x)$, since it simplifies calculations and doesn't change the location of the maximum.\n\n\n### Maximum A Posteriori (MAP) Estimation\n\nMaximum A Posteriori (MAP) estimation is a Bayesian approach.  It finds the mode of the posterior distribution, i.e., the parameter value that maximizes the posterior probability.  Formally, we want to find $\\hat{\\theta}_{MAP}$ such that:\n\n$\\hat{\\theta}_{MAP} = \\arg \\max_{\\theta} P(\\theta|x) = \\arg \\max_{\\theta} \\frac{P(x|\\theta)P(\\theta)}{P(x)} = \\arg \\max_{\\theta} P(x|\\theta)P(\\theta)$\n\nSince $P(x)$ is independent of $\\theta$, we can ignore it in the maximization.  Therefore, the MAP estimate is the parameter value that maximizes the product of the likelihood and the prior.\n\n\n### Comparing MLE and MAP\n\n| Feature        | MLE                               | MAP                                   |\n|----------------|------------------------------------|----------------------------------------|\n| Approach       | Frequentist                       | Bayesian                              |\n| Goal           | Maximize likelihood               | Maximize posterior probability          |\n| Prior          | Implicitly assumes uniform prior   | Explicitly uses a prior distribution   |\n| Computation    | Often simpler                     | Can be more complex, depending on prior |\n| Interpretation | Point estimate, no uncertainty     | Point estimate, reflects prior belief |\n\n\nAs the number of data points increases, the influence of the prior diminishes, and the MAP estimate often converges to the MLE estimate.  However, with limited data, the prior can significantly affect the MAP estimate.\n\n\n### Python Implementation of MLE and MAP\n\nLet's revisit the example of estimating the mean ($\\mu$) of a normal distribution with known variance ($\\sigma^2$). We'll assume a normal prior for $\\mu$.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Data\ndata = np.array([1.5, 2.1, 1.8, 2.3, 1.9])\nsigma = 0.5  # Known variance\n\n# MLE\nmle = np.mean(data)\n\n# MAP (assuming a normal prior)\nmu_0 = 2   # Prior mean\nsigma_0 = 1 # Prior standard deviation\nmu_map = (np.sum(data) / (sigma**2) + mu_0 / (sigma_0**2)) / (len(data) / (sigma**2) + 1 / (sigma_0**2))\n\n# Plotting\nx = np.linspace(0, 3, 100)\nplt.hist(data, density=True, alpha=0.6, label='Data Histogram')\nplt.plot(x, norm.pdf(x, mle, sigma/np.sqrt(len(data))), label='MLE')\nplt.plot(x, norm.pdf(x, mu_map, np.sqrt(1/(len(data)/sigma**2 + 1/sigma_0**2))), label='MAP')\nplt.xlabel('μ')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\nprint(f\"MLE: {mle}\")\nprint(f\"MAP: {mu_map}\")\n```\n\nThis code calculates both the MLE and MAP estimates for $\\mu$ and visualizes them against a histogram of the data. You can observe how MLE and MAP might differ when the prior has an effect, especially with a small sample size. As the sample size increases the MLE and MAP estimators should converge.\n\n\n```{mermaid}\ngraph LR\nA[Data] --> B(Likelihood Function);\nC[Prior Distribution] --> D(Bayes Theorem);\nB --> D;\nD --> E[Posterior Distribution];\nF[MAP: mode of Posterior] --> E;\nG[MLE: maximum of Likelihood] --> B;\n```\n\nThis diagram shows how both MLE and MAP approaches relate to the likelihood function and, in the case of MAP, the prior and posterior distributions.\n\n\n## Credible Intervals\n\nWhile point estimates provide a single value for an unknown parameter, credible intervals offer a range of plausible values, reflecting the uncertainty in the estimate.  Credible intervals are a key feature of Bayesian inference.\n\n### Definition and Interpretation\n\nA $100(1-\\alpha)\\%$ credible interval for a parameter $\\theta$ is an interval $[a, b]$ such that:\n\n$P(a \\le \\theta \\le b | x) = 1 - \\alpha$\n\nwhere $x$ represents the observed data.  This means that the probability that the true value of $\\theta$ lies within the interval $[a, b]$, given the observed data, is $1-\\alpha$.  The interpretation is fundamentally probabilistic: there's a $1-\\alpha$ probability that the true parameter value is within the credible interval.  This is different from the frequentist confidence interval, which has a frequentist interpretation about the procedure rather than a statement about a single interval.\n\n### Calculating Credible Intervals\n\nCalculating credible intervals depends on the form of the posterior distribution.  If the posterior is easily integrable, we can find the interval directly. If not, we can use numerical methods such as Markov Chain Monte Carlo (MCMC) sampling techniques (covered in later chapters) to obtain samples from the posterior and estimate the credible interval from these samples.\n\nFor simple cases, if we have the cumulative distribution function (CDF) of the posterior distribution, $F(\\theta|x)$, we can find the credible interval $[a, b]$ by solving:\n\n$F(a|x) = \\frac{\\alpha}{2}$  and  $F(b|x) = 1 - \\frac{\\alpha}{2}$\n\n\n### Equal-tailed vs. Highest Posterior Density (HPD) Intervals\n\nThere are different ways to construct credible intervals:\n\n* **Equal-tailed intervals:** These intervals are defined by the equations above. They are simple to calculate but might not be the shortest interval containing $1-\\alpha$ probability mass.\n\n* **Highest Posterior Density (HPD) intervals:** These intervals contain the values of $\\theta$ with the highest posterior density.  They are always the shortest intervals containing $1-\\alpha$ probability mass.  Finding HPD intervals often requires numerical optimization techniques.\n\n\n### Python Implementation of Credible Intervals\n\nLet's demonstrate calculating equal-tailed credible intervals using the previous example of estimating the mean of a normal distribution.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Posterior parameters (from previous example, assuming we have posterior distribution)\nmu_n = 2.0  #Posterior mean\nsigma_n = 0.2 #Posterior standard deviation\nalpha = 0.05 # 95% Credible Interval\n\n#Calculate quantiles\nlower_bound = norm.ppf(alpha/2, loc=mu_n, scale=sigma_n)\nupper_bound = norm.ppf(1 - alpha/2, loc=mu_n, scale=sigma_n)\n\n# Plotting\nx = np.linspace(mu_n - 3*sigma_n, mu_n + 3*sigma_n, 100)\nplt.plot(x, norm.pdf(x, mu_n, sigma_n), label='Posterior Distribution')\nplt.fill_between(x, 0, norm.pdf(x, mu_n, sigma_n), where=(x >= lower_bound) & (x <= upper_bound), color='skyblue', alpha=0.5, label=f'{1-alpha:.0%} Credible Interval')\nplt.xlabel('μ')\nplt.ylabel('Density')\nplt.legend()\nplt.title('Credible Interval')\nplt.show()\n\nprint(f\"95% Credible Interval: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n\n```\n\nThis code calculates and plots a 95% equal-tailed credible interval for the posterior distribution of $\\mu$.\n\n\n### Choosing the Credible Interval Level\n\nThe choice of the credible interval level (e.g., 95%, 99%) depends on the context and the desired level of certainty.  A higher credible interval level implies a wider interval, reflecting greater uncertainty. A 95% credible interval is commonly used, but other levels might be appropriate depending on the application's risk tolerance.  There isn't a universally optimal level.\n\n```{mermaid}\ngraph LR\nA[Posterior Distribution] --> B(CDF);\nB --> C{Find quantiles};\nC --> D[Credible Interval];\n```\nThis diagram shows how to obtain a credible interval from the posterior distribution via the CDF.\n\n\n## Advanced Topics in Parameter Estimation\n\nThis section briefly introduces some more advanced topics in Bayesian parameter estimation, providing a foundation for further exploration.\n\n### Bayesian Model Comparison\n\nOften, we have multiple competing models to explain the same data.  Bayesian model comparison provides a formal framework for selecting the best model.  The key concept is the *Bayes factor*, which is the ratio of the marginal likelihoods of two models:\n\n$B_{12} = \\frac{P(x|M_1)}{P(x|M_2)}$\n\nwhere $P(x|M_i)$ is the marginal likelihood of model $M_i$. A Bayes factor greater than 1 favors model $M_1$, while a Bayes factor less than 1 favors model $M_2$.  The marginal likelihood is often difficult to calculate analytically, requiring numerical methods like MCMC.  Another approach is to use model evidence.  The model with the higher model evidence is favored.  Model evidence is calculated by integrating the likelihood over the prior:\n\n$P(x|M) = \\int P(x|\\theta, M) P(\\theta|M) d\\theta$\n\nWhere $M$ denotes the model.\n\n\n### Hierarchical Models\n\nHierarchical models are useful when dealing with data from multiple related sources or groups.  They introduce parameters at different levels, allowing for sharing of information across groups.  For example, we might model the performance of students in different schools, allowing for school-specific effects while also borrowing strength across schools to estimate overall effects.  This can be represented by multi-level models.  A simple hierarchical model might look like:\n\n$\\theta_i \\sim N(\\mu, \\tau^2)$ (group-level parameters)\n$x_i \\sim N(\\theta_i, \\sigma^2)$ (individual observations)\n\nHere, $\\theta_i$ are group-level parameters,  $\\mu$ and $\\tau^2$ represent the hyperparameters for the group-level distribution, and $\\sigma^2$ is the variance of individual observations.\n\n\n### Dealing with High-Dimensional Data\n\nHigh-dimensional data (many parameters relative to the number of data points) pose challenges for Bayesian estimation.  Techniques like regularization (e.g., adding priors that shrink parameters towards zero) or dimensionality reduction are crucial to avoid overfitting and ensure stable posterior estimates.  Prior selection plays a critical role in high-dimensional settings.  Sparsity-inducing priors, like Laplace or horseshoe priors, are particularly useful in shrinking many parameters to exactly zero, effectively performing variable selection.\n\n\n### Computational Methods (MCMC)\n\nFor complex models, analytical solutions are often intractable. Markov Chain Monte Carlo (MCMC) methods provide a powerful approach to approximate the posterior distribution by generating a sample from it.   MCMC algorithms, such as Metropolis-Hastings and Gibbs sampling, construct a Markov chain whose stationary distribution is the target posterior.  By running the chain for a sufficient number of iterations, we can obtain a sample that accurately represents the posterior.  Libraries like PyMC3 provide tools for implementing MCMC in Python.\n\n```\\{python}\n#| echo: true\nimport pymc3 as pm\nimport numpy as np\n\n# Example: Simple linear regression with PyMC3\n\n# Data\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([2.1, 3.9, 6.2, 7.8, 10.1])\n\nwith pm.Model() as model:\n    # Priors\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n    slope = pm.Normal(\"slope\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=5)\n\n    # Likelihood\n    mu = intercept + slope * X\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y)\n\n    # Posterior sampling using MCMC\n    trace = pm.sample(1000, tune=1000) #tune helps the algorithm to converge faster\n\npm.traceplot(trace)\nplt.show()\n\npm.summary(trace)\n```\n\n\nThis code performs a simple linear regression using PyMC3, demonstrating MCMC sampling to obtain posterior estimates of the model parameters.  Note that `tune` is added to allow the sampler to find a good starting point for the MCMC chain. The trace plot visualizes the MCMC samples and helps assess convergence.  The summary provides statistics like the mean, standard deviation, and credible intervals for each parameter.  The use of priors such as `HalfNormal` helps to guide and constrain the MCMC algorithm.\n\n```{mermaid}\ngraph LR\nA[Prior] --> B(Likelihood);\nB --> C[Posterior];\nC --> D[MCMC Sampling];\nD --> E[Posterior Sample];\n```\n\nThis diagram illustrates the role of MCMC in approximating the posterior distribution.  The algorithm iteratively samples from the posterior distribution until the samples accurately represent the target distribution.\n\n\n## Case Studies\n\nThis section presents practical examples of Bayesian parameter estimation using Python.\n\n### Example: Estimating the Mean of a Normal Distribution\n\nLet's revisit the problem of estimating the mean ($\\mu$) of a normal distribution with known variance ($\\sigma^2$) from a sample of data $x_1, x_2, \\dots, x_n$. We assume a normal prior for $\\mu$:\n\n$\\mu \\sim N(\\mu_0, \\sigma_0^2)$\n\nThe likelihood is given by:\n\n$P(x|\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$\n\nThe posterior distribution, using Bayes' Theorem, is also a normal distribution:\n\n$\\mu | x \\sim N(\\mu_n, \\sigma_n^2)$\n\nwhere:\n\n$\\mu_n = \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{\\sum_{i=1}^n x_i}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}}$\n\n$\\sigma_n^2 = \\frac{1}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}}$\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Prior parameters\nmu_0 = 0\nsigma_0 = 1\n\n# Likelihood parameters (assuming known sigma)\nsigma = 1\n\n# Data\ndata = np.random.normal(loc=2, scale=sigma, size=10)\n\n# Posterior parameters\nn = len(data)\nmu_n = (mu_0 / sigma_0**2 + np.sum(data) / sigma**2) / (1 / sigma_0**2 + n / sigma**2)\nsigma_n = np.sqrt(1 / (1 / sigma_0**2 + n / sigma**2))\n\n# Plotting\nx = np.linspace(-1, 5, 100)\nplt.plot(x, norm.pdf(x, mu_0, sigma_0), label='Prior')\nplt.plot(x, norm.pdf(x, mu_n, sigma_n), label='Posterior')\nplt.hist(data, density=True, alpha=0.5, label='Data Histogram')\nplt.xlabel('μ')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\nprint(f\"Posterior Mean: {mu_n:.2f}\")\nprint(f\"Posterior Standard Deviation: {sigma_n:.2f}\")\n\n```\n\nThis code generates a plot showing the prior, posterior, and data histogram.\n\n\n### Example: Estimating the Parameter of a Binomial Distribution\n\nLet's estimate the success probability ($\\theta$) of a binomial distribution. We observe $k$ successes in $n$ trials.  We assume a Beta prior for $\\theta$:\n\n$\\theta \\sim Beta(\\alpha, \\beta)$\n\nThe likelihood is:\n\n$P(k|\\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}$\n\nThe posterior distribution is also a Beta distribution:\n\n$\\theta | k \\sim Beta(\\alpha + k, \\beta + n - k)$\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n# Prior parameters\nalpha = 1\nbeta = 1  #Uniform prior\n\n# Data\nn = 10\nk = 6\n\n# Posterior parameters\nalpha_post = alpha + k\nbeta_post = beta + n - k\n\n# Plotting\nx = np.linspace(0, 1, 100)\nplt.plot(x, beta.pdf(x, alpha, beta), label='Prior')\nplt.plot(x, beta.pdf(x, alpha_post, beta_post), label='Posterior')\nplt.xlabel('θ')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n```\n\nThis code shows how the prior Beta distribution is updated to the posterior Beta distribution after observing the binomial data.\n\n\n### Example: Bayesian Linear Regression\n\nBayesian linear regression models the relationship between a dependent variable $y$ and independent variables $X$ as:\n\n$y_i = X_i \\beta + \\epsilon_i$\n\nwhere $\\epsilon_i \\sim N(0, \\sigma^2)$.  We can assign priors to $\\beta$ and $\\sigma^2$ (e.g., normal and inverse gamma, respectively).  Inference is performed using MCMC sampling.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\n\n#Simulate some data\nnp.random.seed(42)\nX = np.linspace(0,10,100)\ntrue_slope = 2.5\ntrue_intercept = 1\ny_true = true_slope * X + true_intercept\ny = y_true + np.random.normal(0,1,100)\n\nwith pm.Model() as model:\n    #Priors\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=10)\n    slope = pm.Normal(\"slope\", mu=0, sigma=10)\n    sigma = pm.HalfNormal(\"sigma\", sigma=10)\n    \n    #Likelihood\n    mu = intercept + slope * X\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y)\n    \n    #MCMC\n    trace = pm.sample(2000, tune=1000)\n    \npm.traceplot(trace)\nplt.show()\npm.summary(trace)\n```\n\nThis uses PyMC3 to perform Bayesian linear regression, illustrating the use of MCMC for posterior inference. The trace plot visualizes the samples.  The summary shows the posterior means, standard deviations, and credible intervals for the intercept and slope.\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"parameter-estimation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"parameter-estimation.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}