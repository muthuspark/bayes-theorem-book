{"title":"Conjugate Priors","markdown":{"headingText":"Conjugate Priors","containsRefs":false,"markdown":"\n### Introduction to Conjugate Priors\n\nBayesian inference involves updating our prior beliefs about a parameter $\\theta$ given observed data $X$.  We represent our prior beliefs using a prior distribution $p(\\theta)$. After observing data, we update our beliefs using Bayes' theorem:\n\n$p(\\theta|X) = \\frac{p(X|\\theta)p(\\theta)}{p(X)}$\n\nwhere $p(\\theta|X)$ is the posterior distribution, $p(X|\\theta)$ is the likelihood function, and $p(X)$ is the marginal likelihood (often a normalizing constant).  Calculating the posterior distribution can be computationally challenging.  This is where conjugate priors come in handy.\n\nA conjugate prior is a prior distribution that, when combined with the likelihood function, results in a posterior distribution that belongs to the same family of distributions as the prior. This simplifies calculations significantly, as the posterior's parameters can be determined directly from the prior and the likelihood, without resorting to complex numerical integration or approximation techniques.  This elegance makes conjugate priors a powerful tool in Bayesian analysis, especially for pedagogical purposes and in situations where computational resources are limited.\n\n\n### What are Conjugate Priors?\n\nFormally, let $p(\\theta)$ be the prior distribution and $p(X|\\theta)$ be the likelihood function.  If the posterior distribution $p(\\theta|X)$ is in the same family of distributions as the prior $p(\\theta)$, then the prior is said to be conjugate to the likelihood.  The choice of conjugate prior depends heavily on the likelihood function.  Some common examples include:\n\n* **Beta distribution as a conjugate prior for the Bernoulli likelihood:**  If we model the probability of success in a Bernoulli trial with a parameter $\\theta$, then a Beta distribution, $Beta(\\alpha, \\beta)$, is a conjugate prior. The posterior will also be a Beta distribution with updated parameters.\n\n* **Normal distribution as a conjugate prior for the Normal likelihood:**  If we model data as coming from a Normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$, then a Normal distribution, $N(\\mu_0, \\sigma_0^2)$, is a conjugate prior for $\\mu$. The posterior will also be a Normal distribution.\n\n* **Gamma distribution as a conjugate prior for the Poisson likelihood:** If we model count data with a Poisson distribution, with parameter $\\lambda$, then a Gamma distribution, $Gamma(\\alpha, \\beta)$, is a conjugate prior. The posterior will also be a Gamma distribution.\n\n\n### Advantages of Using Conjugate Priors\n\n* **Analytical tractability:** The biggest advantage is the ability to derive the posterior distribution analytically. This avoids computationally intensive methods like Markov Chain Monte Carlo (MCMC), making inference faster and easier.\n\n* **Intuitive interpretation:** The parameters of the conjugate prior often have intuitive interpretations, making it easier to specify the prior and understand the results.  For example, the parameters of a Beta prior directly relate to our prior belief about the probability of success.\n\n* **Simplicity:** The mathematical expressions for updating the posterior are relatively simple and easy to implement.\n\n\n### Limitations of Conjugate Priors\n\n* **Limited flexibility:** The biggest limitation is the restriction to a specific family of distributions.  If the true prior belief doesn't closely resemble a conjugate prior, forcing the use of a conjugate prior can lead to inaccurate inferences.\n\n* **Oversimplification:** The assumption of conjugacy might oversimplify the true relationships in the data.\n\n* **Potential for misspecification:**  Improperly choosing the parameters of the conjugate prior can significantly bias the posterior, leading to erroneous conclusions.  Careful consideration is needed when selecting prior parameters.\n\n\n```{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n# Example: Beta prior and Bernoulli likelihood\n\n# Prior parameters\nalpha_prior = 2\nbeta_prior = 2\n\n# Observed data (number of successes and failures)\nsuccesses = 5\nfailures = 5\n\n# Posterior parameters\nalpha_posterior = alpha_prior + successes\nbeta_posterior = beta_prior + failures\n\n# Generate x values for plotting\nx = np.linspace(0, 1, 100)\n\n# Plot prior and posterior distributions\nplt.plot(x, beta.pdf(x, alpha_prior, beta_prior), label='Prior')\nplt.plot(x, beta.pdf(x, alpha_posterior, beta_posterior), label='Posterior')\nplt.xlabel('Î¸')\nplt.ylabel('Probability Density')\nplt.title('Beta Prior and Posterior')\nplt.legend()\nplt.show()\n```\n\nThis Python code demonstrates the update of a Beta prior given Bernoulli data, illustrating the conjugacy property visually.  The plot shows how the posterior distribution shifts based on the observed data.\n\n\n## Beta-Binomial Model\n\n### The Binomial Likelihood\n\nThe binomial distribution is a fundamental probability model for the number of successes in a fixed number of independent Bernoulli trials.  Each trial has a probability of success $\\theta$, where $0 \\le \\theta \\le 1$.  If we observe $k$ successes in $n$ trials, the likelihood function is given by:\n\n$p(k|\\theta, n) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}$\n\nwhere $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the binomial coefficient.  This likelihood describes the probability of observing $k$ successes given the parameters $n$ and $\\theta$.  In a Bayesian context, we treat $\\theta$ as a random variable, and the likelihood quantifies the plausibility of different values of $\\theta$ given the observed data.\n\n\n### The Beta Prior\n\nA natural choice for the prior distribution of $\\theta$ is the Beta distribution. The Beta distribution is defined on the interval [0, 1] and is parameterized by two positive shape parameters, $\\alpha$ and $\\beta$:\n\n$p(\\theta|\\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$\n\nwhere $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function, and $\\Gamma(\\cdot)$ is the gamma function. The parameters $\\alpha$ and $\\beta$ control the shape of the distribution.  A larger $\\alpha$ relative to $\\beta$ shifts the distribution towards larger values of $\\theta$, representing a prior belief that $\\theta$ is likely to be higher.  Conversely, a larger $\\beta$ relative to $\\alpha$ shifts the distribution towards smaller values of $\\theta$.  When $\\alpha = \\beta = 1$, the Beta distribution is uniform, representing a lack of prior information.\n\n\n### Posterior Distribution Derivation\n\nBecause the Beta distribution is a conjugate prior for the binomial likelihood, the posterior distribution is also a Beta distribution.  Applying Bayes' theorem:\n\n$p(\\theta|k, n, \\alpha, \\beta) \\propto p(k|\\theta, n) p(\\theta|\\alpha, \\beta)$\n\n$p(\\theta|k, n, \\alpha, \\beta) \\propto \\binom{n}{k} \\theta^k (1-\\theta)^{n-k} \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$\n\nIgnoring terms that don't depend on $\\theta$, we obtain:\n\n$p(\\theta|k, n, \\alpha, \\beta) \\propto \\theta^{k+\\alpha-1} (1-\\theta)^{n-k+\\beta-1}$\n\nThis is the kernel of a Beta distribution with parameters $\\alpha' = \\alpha + k$ and $\\beta' = \\beta + n - k$. Therefore, the posterior distribution is:\n\n$p(\\theta|k, n, \\alpha, \\beta) = Beta(\\alpha + k, \\beta + n - k)$\n\n\n### Bayesian Inference with Beta-Binomial\n\nThe Beta-Binomial model allows for straightforward Bayesian inference.  We start with a Beta prior reflecting our prior beliefs about $\\theta$. After observing data (number of successes $k$ in $n$ trials), we update our beliefs by calculating the posterior Beta distribution using the updated parameters $\\alpha' = \\alpha + k$ and $\\beta' = \\beta + n - k$.  We can then use the posterior distribution to make inferences about $\\theta$, such as calculating credible intervals or point estimates (e.g., mean or median).\n\n\n### Python Implementation with PyMC\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Observed data\nk = 6  # Number of successes\nn = 10 # Number of trials\n\n# Prior parameters\nalpha_prior = 1\nbeta_prior = 1\n\nwith pm.Model() as model:\n    # Prior distribution\n    theta = pm.Beta(\"theta\", alpha=alpha_prior, beta=beta_prior)\n\n    # Likelihood\n    y_obs = pm.Binomial(\"y_obs\", p=theta, n=n, observed=k)\n\n    # Posterior sampling\n    trace = pm.sample(10000, tune=1000)\n\n# Posterior analysis\npm.summary(trace)\npm.plot_posterior(trace, hdi_prob=0.95)\nplt.show()\n```\n\n```{python}\n# Posterior predictive distribution\nppc = pm.sample_posterior_predictive(trace, var_names=[\"y_obs\"], model=model)\nprint(ppc.keys())\nplt.hist(ppc.posterior_predictive['y_obs'], bins=range(n + 2))\nplt.xlabel(\"Number of successes\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Posterior Predictive Distribution\")\nplt.show()\n```\n\n\nThis PyMC code defines a Beta-Binomial model, samples from the posterior distribution using Hamiltonian Monte Carlo (HMC), and visualizes the results. The posterior plot shows the updated belief about  $\\theta$ after observing the data, and the posterior predictive distribution shows the probability of observing different numbers of successes in future experiments. Remember to install PyMC:  `pip install pymc`\n\n\n\n\n## Normal-Normal Model\n\n### The Normal Likelihood\n\nWe often model continuous data using the normal (Gaussian) distribution.  Suppose we have a sample of $n$ data points, $x_1, x_2, ..., x_n$, which are assumed to be independent and identically distributed (i.i.d.) from a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$. The likelihood function is given by:\n\n$p(X|\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2}{2\\sigma^2}\\right)$\n\nwhere $X = (x_1, x_2, ..., x_n)$.  This likelihood expresses the probability of observing the data given a specific value of $\\mu$ and the known $\\sigma^2$.\n\n\n### The Normal Prior\n\nA conjugate prior for the normal mean $\\mu$ when the variance $\\sigma^2$ is known is another normal distribution.  We specify a prior distribution for $\\mu$ as:\n\n$p(\\mu|\\mu_0, \\sigma_0^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2}\\right)$\n\nwhere $\\mu_0$ represents our prior belief about the mean, and $\\sigma_0^2$ represents the uncertainty in our prior belief.  A smaller $\\sigma_0^2$ indicates a stronger prior belief.\n\n\n### Posterior Distribution Derivation\n\nUsing Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior:\n\n$p(\\mu|X, \\mu_0, \\sigma_0^2, \\sigma^2) \\propto p(X|\\mu, \\sigma^2) p(\\mu|\\mu_0, \\sigma_0^2)$\n\nAfter some algebraic manipulation (completing the square), we find that the posterior distribution is also a normal distribution:\n\n$p(\\mu|X, \\mu_0, \\sigma_0^2, \\sigma^2) = N(\\mu_n, \\sigma_n^2)$\n\nwhere:\n\n$\\mu_n = \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{n\\bar{x}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}}$\n\n$\\sigma_n^2 = \\frac{1}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2}}$\n\nHere, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$ is the sample mean.  The posterior mean $\\mu_n$ is a weighted average of the prior mean $\\mu_0$ and the sample mean $\\bar{x}$, with weights inversely proportional to their respective variances. The posterior variance $\\sigma_n^2$ is smaller than both the prior and the sampling variance, reflecting the reduction in uncertainty after observing the data.\n\n\n### Bayesian Inference with Normal-Normal\n\nThe Normal-Normal model provides a straightforward way to update our beliefs about the mean of a normal distribution. The posterior distribution summarizes our updated beliefs after incorporating the data. We can obtain point estimates (e.g., posterior mean $\\mu_n$) and credible intervals to quantify uncertainty.\n\n\n### Python Implementation with PyMC\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Observed data\ndata = np.array([2, 3, 4, 5, 6])\nn = len(data)\n\n# Prior parameters\nmu_prior = 4\nsigma_prior = 2\nsigma = 1 # Known standard deviation\n\nwith pm.Model() as model:\n    # Prior\n    mu = pm.Normal(\"mu\", mu=mu_prior, sigma=sigma_prior)\n\n    # Likelihood\n    y = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=data)\n\n    # Posterior sampling\n    trace = pm.sample(10000, tune=1000)\n\npm.summary(trace)\npm.plot_posterior(trace, hdi_prob=0.95)\nplt.show()\n```\n\nThis code implements the Normal-Normal model in PyMC. The posterior distribution of $\\mu$ is visualized.\n\n\n### Handling Unknown Variance\n\nWhen the variance $\\sigma^2$ is unknown, we need to introduce a prior for it.  A common choice is the Inverse-Gamma distribution, which is conjugate to the normal likelihood for the variance.  This makes the model more realistic but also more complex analytically.  The complete model with unknown variance would require using techniques like MCMC sampling, which PyMC handles efficiently:\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Observed data\ndata = np.array([2, 3, 4, 5, 6])\nn = len(data)\n\nwith pm.Model() as model:\n    # Priors\n    mu = pm.Normal(\"mu\", mu=0, sigma=10)  # Weak prior on mu\n    sigma = pm.HalfCauchy(\"sigma\", beta=5) # Prior on sigma\n\n    # Likelihood\n    y = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=data)\n\n    # Posterior sampling\n    trace = pm.sample(10000, tune=1000, cores=1) # adjust cores as needed\n\npm.summary(trace)\npm.plot_posterior(trace, hdi_prob=0.95)\nplt.show()\n```\n\nThis example uses a Half-Cauchy prior for $\\sigma$, a common and relatively non-informative prior for scale parameters.  Remember to adjust the number of cores (`cores=`) based on your system's capabilities. The Half-Cauchy prior is used to avoid problems that can arise with using an Inverse Gamma prior with improper priors.  It is important to be mindful of the effect that your chosen prior has on the posterior results.\n\n\n## Dirichlet-Multinomial Model\n\n### The Multinomial Likelihood\n\nThe multinomial distribution is a generalization of the binomial distribution to more than two outcomes.  Suppose we have $k$ categories and perform $n$ independent trials. Let $x_i$ be the number of trials resulting in category $i$, where $\\sum_{i=1}^{k} x_i = n$.  Let $\\theta_i$ be the probability of a trial resulting in category $i$, where $\\sum_{i=1}^{k} \\theta_i = 1$.  The multinomial likelihood is given by:\n\n$p(x_1, ..., x_k | \\theta_1, ..., \\theta_k, n) = \\binom{n}{x_1, ..., x_k} \\prod_{i=1}^{k} \\theta_i^{x_i}$\n\nwhere $\\binom{n}{x_1, ..., x_k} = \\frac{n!}{x_1! ... x_k!}$ is the multinomial coefficient.\n\n\n### The Dirichlet Prior\n\nThe Dirichlet distribution is a conjugate prior for the multinomial distribution.  It is defined over the $k$-dimensional probability simplex, i.e., the set of vectors $\\theta = (\\theta_1, ..., \\theta_k)$ such that $\\theta_i \\ge 0$ for all $i$ and $\\sum_{i=1}^{k} \\theta_i = 1$.  The Dirichlet distribution is parameterized by a vector of $k$ positive concentration parameters, $\\alpha = (\\alpha_1, ..., \\alpha_k)$:\n\n$p(\\theta_1, ..., \\theta_k | \\alpha_1, ..., \\alpha_k) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^{k} \\theta_i^{\\alpha_i - 1}$\n\nwhere $B(\\alpha) = \\frac{\\prod_{i=1}^{k} \\Gamma(\\alpha_i)}{\\Gamma(\\sum_{i=1}^{k} \\alpha_i)}$ is the multivariate Beta function.  The $\\alpha_i$ parameters influence the shape of the distribution.  Larger values of $\\alpha_i$ lead to higher probability density around $\\theta_i = \\frac{\\alpha_i}{\\sum_{j=1}^{k} \\alpha_j}$.\n\n\n### Posterior Distribution Derivation\n\nSince the Dirichlet is conjugate to the multinomial, the posterior distribution is also a Dirichlet distribution.  The posterior parameters are updated by simply adding the observed counts to the prior parameters:\n\n$p(\\theta_1, ..., \\theta_k | x_1, ..., x_k, \\alpha_1, ..., \\alpha_k) = Dirichlet(\\alpha_1 + x_1, ..., \\alpha_k + x_k)$\n\n\n### Bayesian Inference with Dirichlet-Multinomial\n\nThe Dirichlet-Multinomial model allows us to perform Bayesian inference on the parameters of a multinomial distribution.  Starting with a Dirichlet prior reflecting our prior beliefs about the category probabilities, we update our beliefs using the observed counts to obtain a posterior Dirichlet distribution. We can use this posterior to make inferences, such as calculating credible intervals for each $\\theta_i$ or predicting the counts for future trials.\n\n\n### Python Implementation with PyMC\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\n\n# Observed data (counts for each category)\nobserved_counts = np.array([10, 20, 30, 40])\nk = len(observed_counts)  # Number of categories\n\n# Prior parameters (concentration parameters) - weakly informative prior\nalpha_prior = np.ones(k)\n\nwith pm.Model() as model:\n    # Prior distribution\n    theta = pm.Dirichlet(\"theta\", a=alpha_prior)\n\n    # Likelihood\n    y = pm.Multinomial(\"y\", n=sum(observed_counts), p=theta, observed=observed_counts)\n\n    # Posterior sampling\n    trace = pm.sample(10000, tune=1000)\n\npm.summary(trace)\npm.plot_posterior(trace, hdi_prob=0.95)\nplt.show()\n```\n\nThis code implements the Dirichlet-Multinomial model in PyMC.  Remember to install PyMC (`pip install pymc`). The posterior distributions for each $\\theta_i$ are shown.\n\n\n\n### Applications in Text Mining and other fields\n\nThe Dirichlet-Multinomial model finds widespread applications:\n\n* **Text mining:**  Modeling the distribution of words in documents.  Each category represents a word, and the $\\theta_i$ represent the probability of each word appearing in a document. The Dirichlet prior helps smooth the word probabilities, preventing zero probabilities for unseen words (especially helpful with large vocabularies).\n\n* **Topic modeling:**  Identifying latent topics in a collection of documents. Each topic is a multinomial distribution over words, and the Dirichlet prior is used to control the sparsity of topic distributions.\n\n* **Recommender systems:** Modeling user preferences over items. Each item is a category, and the $\\theta_i$ represent the probability that a user will choose a given item.\n\n* **Genetics:**  Modeling allele frequencies in populations.  Each category is an allele, and $\\theta_i$ is the frequency of that allele.\n\n* **Ecology:** Species abundance in an ecosystem, where each category is a species.\n\n\nThe flexibility and analytical tractability of the Dirichlet-Multinomial model make it a valuable tool in various fields involving categorical data analysis.\n\n\n## Beyond the Basic Models\n\n### Other Conjugate Prior Pairs\n\nWhile the Beta-Binomial, Normal-Normal, and Dirichlet-Multinomial models are frequently used, many other conjugate prior pairs exist, each tailored to a specific likelihood function.  Here are a few examples:\n\n* **Gamma-Poisson:** The Gamma distribution is a conjugate prior for the Poisson likelihood.  If $X \\sim Poisson(\\lambda)$, and we use a Gamma prior for $\\lambda$, $p(\\lambda|\\alpha, \\beta) = Gamma(\\alpha, \\beta)$, then the posterior is also a Gamma distribution with updated parameters.\n\n* **Inverse Gamma-Normal (for variance):** The Inverse Gamma distribution is a conjugate prior for the variance of a normal distribution when the mean is known. If $X \\sim N(\\mu, \\sigma^2)$ and $\\mu$ is known, using an Inverse Gamma prior for $\\sigma^2$,  $p(\\sigma^2 | \\alpha, \\beta) = InvGamma(\\alpha, \\beta)$, yields an Inverse Gamma posterior.\n\n* **Normal-Inverse Gamma:** This is a bivariate conjugate prior for the mean and variance of a normal distribution.  The prior is a combination of a normal distribution for the mean (conditional on the variance) and an inverse gamma distribution for the variance.\n\n\nThese examples highlight the versatility of conjugate priors. The availability of a conjugate prior greatly simplifies Bayesian inference, but it's essential to select the appropriate pair that matches the likelihood function and reflects your prior knowledge accurately.\n\n\n### Choosing Appropriate Conjugate Priors\n\nSelecting the right conjugate prior involves many considerations:\n\n1. **Likelihood Function:** The choice of prior is fundamentally determined by the likelihood function of your data.  Only certain prior distributions form conjugate pairs with specific likelihoods.\n\n2. **Prior Knowledge:** The prior should reflect your prior beliefs or existing knowledge about the parameter. If you have strong prior information, choose a prior that incorporates this information effectively. If you have little or no prior information, consider weakly informative priors that avoid unduly influencing the posterior.\n\n3. **Computational Tractability:** While all conjugate priors lead to analytically tractable posteriors, some might be easier to work with than others depending on the complexity of the expressions involved.\n\n4. **Interpretability:** The parameters of the chosen prior should be easily interpretable in the context of your problem.  This allows for easier communication and understanding of the Bayesian analysis.\n\n\nChoosing an inappropriate conjugate prior can lead to inaccurate or misleading results.  It is essential to carefully consider the implications of the prior choice and to justify its selection.\n\n\n### Approximations when Conjugate Priors are Unavailable\n\nIn many practical situations, no conjugate prior exists for the given likelihood. In such cases, many approximation techniques can be employed:\n\n1. **Laplace Approximation:**  This method approximates the posterior distribution with a normal distribution centered at the mode of the posterior density.  It's relatively simple but might not be accurate if the posterior is highly skewed or multimodal.\n\n2. **Variational Inference:** This technique approximates the true posterior with a simpler, tractable distribution from a specified family (e.g., a mean-field approximation). It offers flexibility but can be computationally intensive.\n\n3. **Markov Chain Monte Carlo (MCMC):**  MCMC methods, such as Hamiltonian Monte Carlo (HMC) or Metropolis-Hastings, are powerful tools for sampling from complex, high-dimensional posterior distributions.  They are computationally intensive but generally provide accurate approximations.  Libraries like PyMC automate these methods.\n\n```{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pymc as pm\n\n# Example using PyMC for a non-conjugate scenario (illustrative)\n# Assume a likelihood that does not have a simple conjugate prior\n\n# Simulate some data (Example: a mixture of Gaussians)\nnp.random.seed(0)\ndata1 = np.random.normal(loc=0, scale=1, size=50)\ndata2 = np.random.normal(loc=5, scale=0.5, size=50)\ndata = np.concatenate((data1, data2))\n\nwith pm.Model() as model:\n    # Priors (Non-conjugate Example)\n    mu = pm.Normal(\"mu\", mu=0, sigma=10)\n    sigma = pm.HalfCauchy(\"sigma\", beta=5)\n\n    # Likelihood (Example: mixture of gaussians, not directly conjugate)\n    y = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=data)\n\n    # Sample the posterior using MCMC\n    trace = pm.sample(10000, tune=1000)\n\npm.summary(trace)\npm.plot_posterior(trace, hdi_prob=0.95)\nplt.show()\n```\n\nThis PyMC code demonstrates how to handle a non-conjugate scenario by using MCMC sampling.  The example employs a simple normal likelihood, but the principle extends to more complex non-conjugate situations.  Note that the choice of priors is essential even in non-conjugate settings.  The selection process should still consider prior knowledge and avoid overly strong or overly weak priors.\n\n\n\n\n## Case Studies and Applications\n\n### A/B Testing with Beta-Binomial\n\nA/B testing is a common method for comparing two versions of a webpage, advertisement, or other item to determine which performs better.  The Beta-Binomial model provides a powerful Bayesian framework for analyzing A/B test results.\n\nLet's say we have two versions (A and B) of a webpage, and we want to determine which has a higher conversion rate (e.g., the proportion of visitors who make a purchase).  We can model the conversion rate for each version using a Beta-Binomial model:\n\n* **Version A:**  Let $\\theta_A$ be the conversion rate for version A. We place a Beta prior on $\\theta_A$, $p(\\theta_A|\\alpha_{A0}, \\beta_{A0}) = Beta(\\alpha_{A0}, \\beta_{A0})$.  After observing $k_A$ conversions out of $n_A$ visitors, the posterior is $p(\\theta_A|k_A, n_A, \\alpha_{A0}, \\beta_{A0}) = Beta(\\alpha_{A0} + k_A, \\beta_{A0} + n_A - k_A)$.\n\n* **Version B:**  Similarly, for version B, we have $\\theta_B$, a Beta prior $p(\\theta_B|\\alpha_{B0}, \\beta_{B0})$, and a posterior $p(\\theta_B|k_B, n_B, \\alpha_{B0}, \\beta_{B0}) = Beta(\\alpha_{B0} + k_B, \\beta_{B0} + n_B - k_B)$.\n\nWe can then compare the posterior distributions of $\\theta_A$ and $\\theta_B$ to determine which version has a higher conversion rate. We might compute the probability that $\\theta_A > \\theta_B$ using posterior samples.\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# A/B testing data\nkA = 15  # Conversions for version A\nnA = 100 # Visitors for version A\nkB = 20  # Conversions for version B\nnB = 100 # Visitors for version B\n\n# Weakly informative priors\nalpha_prior = 1\nbeta_prior = 1\n\nwith pm.Model() as model:\n    # Priors\n    theta_A = pm.Beta(\"theta_A\", alpha=alpha_prior, beta=beta_prior)\n    theta_B = pm.Beta(\"theta_B\", alpha=alpha_prior, beta=beta_prior)\n\n    # Likelihoods\n    obs_A = pm.Binomial(\"obs_A\", p=theta_A, n=nA, observed=kA)\n    obs_B = pm.Binomial(\"obs_B\", p=theta_B, n=nB, observed=kB)\n\n    # Posterior sampling\n    trace = pm.sample(10000, tune=1000)\n\npm.summary(trace)\npm.plot_posterior(trace, hdi_prob=0.95)\nplt.show()\n\n# Probability that theta_A > theta_B\nprob_A_gt_B = np.mean(trace.posterior[\"theta_A\"] > trace.posterior[\"theta_B\"])\nprint(f\"P(theta_A > theta_B) = {prob_A_gt_B}\")\n\n```\n\n\n### Estimating Population Means with Normal-Normal\n\nSuppose we want to estimate the average height of adult women in a certain city. We collect a random sample of $n$ heights, $x_1, x_2, ..., x_n$.  We can model the heights using a normal distribution with unknown mean $\\mu$ and known variance $\\sigma^2$.\n\nWe use a normal prior for $\\mu$, $p(\\mu|\\mu_0, \\sigma_0^2) = N(\\mu_0, \\sigma_0^2)$,  reflecting our prior belief about the average height.  The posterior distribution is also normal, with parameters derived as described in the \"Normal-Normal Model\" section.\n\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample heights (simulated data)\nnp.random.seed(42)\nheights = np.random.normal(loc=165, scale=5, size=30)\n\n# Prior parameters (weak prior)\nmu_prior = 170\nsigma_prior = 10\nsigma = 5  # Known standard deviation\n\n\nwith pm.Model() as model:\n    # Prior\n    mu = pm.Normal(\"mu\", mu=mu_prior, sigma=sigma_prior)\n\n    # Likelihood\n    y = pm.Normal(\"y\", mu=mu, sigma=sigma, observed=heights)\n\n    # Posterior sampling\n    trace = pm.sample(10000, tune=1000)\n\npm.summary(trace)\npm.plot_posterior(trace, hdi_prob=0.95)\nplt.show()\n```\n\nThis code estimates the population mean height using the Normal-Normal model.\n\n\n### Topic Modeling with Dirichlet-Multinomial\n\nTopic modeling aims to discover underlying thematic structures (topics) in a collection of documents.  The Latent Dirichlet Allocation (LDA) model is a probabilistic approach that utilizes the Dirichlet-Multinomial framework.\n\nIn LDA:\n\n* Each document is represented as a mixture of topics.\n* Each topic is a probability distribution over words.\n* The distribution of topics in a document follows a Dirichlet distribution.\n* The distribution of words given a topic follows a multinomial distribution.\n\nThe Dirichlet-Multinomial model forms the core of this generative process.  Inference in LDA typically involves MCMC methods, which are computationally intensive. Libraries like `gensim` provide efficient implementations.  Here's a conceptual outline using PyMC, but it won't be a fully functional LDA implementation due to the high dimensionality and computational challenges of this model:\n\n\n```{python}\n#| echo: true\n# Note: This is a simplified conceptual illustration and NOT a full LDA implementation\n# Full LDA implementation requires specialized libraries like gensim\n\nimport pymc as pm\nimport numpy as np\n\n# Simplified Example:  Assume we have 2 topics and 3 words\n\n# Number of documents\nn_docs = 10\n\n# Number of words per document (simplified example, varies in real LDA)\nn_words = 10\n\n#Simulate some data (This is a placeholder. Real data requires preprocessing)\nword_counts = np.random.randint(1, 10, size=(n_docs, 3))\n\nwith pm.Model() as model:\n    # Priors (Weak priors)\n    alpha = pm.HalfCauchy(\"alpha\", beta=1, shape=2)  # Prior for topic distribution\n    beta = pm.Dirichlet(\"beta\", a=np.ones(3), shape=2)  # Prior for word distribution per topic\n\n\n    # Likelihood (This is a placeholder, actual LDA has more complex structure)\n    # ...  (The likelihood would require modeling topic assignment and word generation) ...\n\n\n    # Posterior sampling (Would require complex samplers for full LDA)\n    # ... (Sampling process would be much more involved for full LDA) ...\n```\n\n\nThis conceptual example illustrates the role of Dirichlet and Multinomial distributions within LDA.  For real-world topic modeling, using specialized libraries such as `gensim` is strongly recommended due to the computational complexity of LDA inference.  These libraries often employ optimized variational inference or Gibbs sampling for efficient posterior approximation.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"conjugate-priors.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"conjugate-priors.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}