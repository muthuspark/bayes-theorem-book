{"title":"Introduction to Probability","markdown":{"headingText":"Introduction to Probability","containsRefs":false,"markdown":"\nThis chapter lays the groundwork for understanding Bayes' Theorem by reviewing essential concepts from probability theory.  A solid grasp of these fundamentals is crucial for effectively applying Bayes' Theorem and interpreting its results.  We will cover basic probability concepts, the relationship between set theory and probability, conditional probability, and a brief preview of Bayes' Theorem itself.\n\n### Basic Probability Concepts\n\nProbability quantifies the likelihood of an event occurring.  The probability of an event $A$, denoted as $P(A)$, is a number between 0 and 1 inclusive.  $P(A) = 0$ indicates that event $A$ is impossible, while $P(A) = 1$ indicates that event $A$ is certain.  \n\nThe probability of an event can be determined through various methods, including:\n\n* **Classical Approach:** If all outcomes are equally likely, the probability of an event is the ratio of favorable outcomes to the total number of possible outcomes.  For example, the probability of rolling a 6 on a fair six-sided die is $\\frac{1}{6}$.\n\n* **Frequentist Approach:** The probability of an event is estimated as the relative frequency of its occurrence in a large number of trials.  For instance, if a coin is flipped 1000 times and lands heads 505 times, the estimated probability of heads is $\\frac{505}{1000} = 0.505$.\n\n* **Subjective Approach:** The probability of an event is assigned based on an individual's belief or judgment, often incorporating prior knowledge and expert opinion. This approach is particularly relevant in Bayesian statistics.\n\n\n### Set Theory and Probability\n\nSet theory provides a powerful framework for representing and manipulating events.  An event can be viewed as a subset of the sample space, which is the set of all possible outcomes.\n\n* **Union:** The union of two events $A$ and $B$, denoted as $A \\cup B$, represents the event that either $A$ or $B$ (or both) occur.\n* **Intersection:** The intersection of two events $A$ and $B$, denoted as $A \\cap B$, represents the event that both $A$ and $B$ occur.\n* **Complement:** The complement of an event $A$, denoted as $A^c$ or $A'$, represents the event that $A$ does not occur.\n\n\nThe probability of the union of two events can be calculated using the addition rule:\n\n$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n\nIf events $A$ and $B$ are mutually exclusive (i.e., they cannot occur simultaneously, $A \\cap B = \\emptyset$), then $P(A \\cup B) = P(A) + P(B)$.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example: Visualizing the union of two sets using Venn diagram (simplified representation)\n\n#Create dummy data for visualization (replace with your own data)\nsetA = np.random.rand(20) < 0.6 #Simulate set A\nsetB = np.random.rand(20) < 0.4 #Simulate set B\n\nplt.figure(figsize=(6, 4))\nplt.scatter(setA, np.zeros_like(setA), label='Set A', color='blue', alpha=0.7)\nplt.scatter(setB, np.zeros_like(setB), label='Set B', color='red', alpha=0.7)\nplt.scatter(np.intersect1d(setA, setB), np.zeros_like(np.intersect1d(setA, setB)), label='A ∩ B', color='purple', alpha=0.9) #Visualize the intersection\nplt.xlabel('Data points')\nplt.yticks([])\nplt.title('Simplified Venn Diagram representation')\nplt.legend()\nplt.show()\n\n```\n\n### Conditional Probability\n\nConditional probability refers to the probability of an event occurring given that another event has already occurred.  The conditional probability of event $A$ given event $B$, denoted as $P(A|B)$, is defined as:\n\n$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$, provided $P(B) > 0$\n\nThis formula tells us that the probability of $A$ given $B$ is the ratio of the probability of both $A$ and $B$ occurring to the probability of $B$ occurring.\n\n### Bayes' Theorem (Preview)\n\nBayes' Theorem provides a way to update our beliefs about an event based on new evidence.  It's a powerful tool for revising probabilities in light of new information.  We'll explore it in detail in later chapters, but here's a preview:\n\nLet $A$ and $B$ be two events. Bayes' Theorem states:\n\n$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n\nWhere:\n\n* $P(A|B)$ is the posterior probability of $A$ given $B$.\n* $P(B|A)$ is the likelihood of $B$ given $A$.\n* $P(A)$ is the prior probability of $A$.\n* $P(B)$ is the marginal likelihood of $B$. (Often calculated as $P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$ )\n\nWe will delve deeper into the interpretation and applications of Bayes' Theorem in subsequent chapters.  For now, it's important to understand this fundamental equation as a cornerstone of Bayesian inference.\n\n\n```{mermaid}\ngraph LR\n    A[Prior Probability P(A)] --> B(Bayes' Theorem);\n    C[Likelihood P(B|A)] --> B;\n    B --> D[Posterior Probability P(A|B)];\n    E[Evidence P(B)] --> B;\n```\n\n\n## Random Variables\n\nThis chapter introduces the concept of random variables, a crucial building block for understanding probability distributions and their application in Bayesian statistics. We will explore different types of random variables, their associated probability functions, and how to represent them using Python.\n\n### Definition and Types of Random Variables\n\nA random variable is a variable whose value is a numerical outcome of a random phenomenon.  It's a function that maps the outcomes of a random experiment to numerical values.  Random variables are typically denoted by uppercase letters (e.g., $X$, $Y$, $Z$), while their specific values are denoted by lowercase letters (e.g., $x$, $y$, $z$).\n\nRandom variables can be broadly classified into two types:\n\n* **Discrete Random Variables:** These variables can only take on a finite number of values or a countably infinite number of values. Examples include the number of heads in three coin flips (0, 1, 2, 3), the number of cars passing a certain point on a highway in an hour, or the outcome of rolling a die.\n\n* **Continuous Random Variables:** These variables can take on any value within a given range or interval. Examples include the height of a person, the temperature of a room, or the time it takes to complete a task.\n\n\n### Discrete vs. Continuous Random Variables\n\nThe key difference lies in the possible values the variable can assume.  Discrete random variables have gaps between their possible values, while continuous random variables can take on any value within a continuous range.  This difference leads to different ways of describing their probability distributions.\n\n### Probability Mass Function (PMF)\n\nThe probability mass function (PMF) describes the probability distribution of a discrete random variable.  For a discrete random variable $X$, the PMF is denoted as $P(X = x)$ and represents the probability that $X$ takes on the specific value $x$.  The PMF must satisfy the following conditions:\n\n1. $P(X = x) \\ge 0$ for all $x$.\n2. $\\sum_{x} P(X = x) = 1$, where the sum is over all possible values of $x$.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example: PMF of a fair six-sided die\n\nx = np.arange(1, 7)  # Possible values of the die roll\npx = np.full(6, 1/6)  # Probability of each value (uniform distribution)\n\nplt.stem(x, px)\nplt.xlabel(\"X (Die Roll)\")\nplt.ylabel(\"P(X = x)\")\nplt.title(\"PMF of a Fair Six-Sided Die\")\nplt.show()\n```\n\n### Probability Density Function (PDF)\n\nThe probability density function (PDF) describes the probability distribution of a continuous random variable.  For a continuous random variable $X$, the PDF is denoted as $f(x)$.  Unlike the PMF, the PDF does *not* directly give the probability of $X$ taking on a specific value.  Instead, the probability of $X$ falling within a given interval $[a, b]$ is given by the integral of the PDF over that interval:\n\n$P(a \\le X \\le b) = \\int_{a}^{b} f(x) dx$\n\nThe PDF must satisfy the following conditions:\n\n1. $f(x) \\ge 0$ for all $x$.\n2. $\\int_{-\\infty}^{\\infty} f(x) dx = 1$\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# Example: PDF of a standard normal distribution\n\nx = np.linspace(-3, 3, 100)\ny = norm.pdf(x)\n\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"f(x)\")\nplt.title(\"PDF of a Standard Normal Distribution\")\nplt.show()\n```\n\n\n### Cumulative Distribution Function (CDF)\n\nThe cumulative distribution function (CDF) is a function that gives the probability that a random variable $X$ is less than or equal to a given value $x$.  It's denoted as $F(x)$ and is defined as:\n\n$F(x) = P(X \\le x)$\n\nThe CDF is defined for both discrete and continuous random variables.  For discrete variables, it's the sum of probabilities up to $x$. For continuous variables, it's the integral of the PDF up to $x$:\n\n$F(x) = \\int_{-\\infty}^{x} f(t) dt$\n\nThe CDF is a non-decreasing function, with $F(-\\infty) = 0$ and $F(\\infty) = 1$.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# Example: CDF of a standard normal distribution\n\nx = np.linspace(-3, 3, 100)\ny = norm.cdf(x)\n\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.ylabel(\"F(x)\")\nplt.title(\"CDF of a Standard Normal Distribution\")\nplt.show()\n```\n\n```{mermaid}\ngraph LR\n    A[Random Variable X] --> B(PMF/PDF);\n    B --> C[CDF F(x)];\n    subgraph Discrete\n        A --> D[Probability Mass Function P(X=x)];\n    end\n    subgraph Continuous\n        A --> E[Probability Density Function f(x)];\n    end\n```\n\n\n## Common Probability Distributions\n\nThis chapter introduces some of the most frequently encountered probability distributions in statistics and their applications. Understanding these distributions is fundamental for applying Bayes' Theorem effectively in various contexts. We will explore both discrete and continuous distributions and demonstrate how to work with them using Python.\n\n### Discrete Distributions: Bernoulli, Binomial, Poisson\n\n**Bernoulli Distribution:**  Models the outcome of a single Bernoulli trial—an experiment with only two possible outcomes, typically labeled \"success\" (1) and \"failure\" (0). The probability of success is denoted by $p$, and the probability of failure is $1-p$.\n\nThe PMF is:\n$P(X=k) = p^k (1-p)^{1-k}$, where $k \\in \\{0, 1\\}$.\n\n**Binomial Distribution:**  Models the number of successes in a fixed number of independent Bernoulli trials.  The parameters are $n$ (number of trials) and $p$ (probability of success in a single trial).\n\nThe PMF is:\n$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$, where $k \\in \\{0, 1, ..., n\\}$.\n\n**Poisson Distribution:** Models the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence ($\\lambda$).\n\nThe PMF is:\n$P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$, where $k \\in \\{0, 1, 2, ...\\}$.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import bernoulli, binom, poisson\n\n# Example plots (adjust parameters as needed)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Bernoulli\nx_bern = [0, 1]\np_bern = 0.7\nprob_bern = bernoulli.pmf(x_bern, p_bern)\naxes[0].stem(x_bern, prob_bern)\naxes[0].set_title(\"Bernoulli Distribution (p=0.7)\")\n\n# Binomial\nn_binom = 10\np_binom = 0.4\nx_binom = np.arange(n_binom + 1)\nprob_binom = binom.pmf(x_binom, n_binom, p_binom)\naxes[1].stem(x_binom, prob_binom)\naxes[1].set_title(\"Binomial Distribution (n=10, p=0.4)\")\n\n# Poisson\nlambda_poisson = 5\nx_poisson = np.arange(15)\nprob_poisson = poisson.pmf(x_poisson, lambda_poisson)\naxes[2].stem(x_poisson, prob_poisson)\naxes[2].set_title(\"Poisson Distribution (λ=5)\")\n\nplt.show()\n\n```\n\n\n### Continuous Distributions: Normal, Exponential, Uniform\n\n**Normal Distribution:**  A bell-shaped distribution characterized by its mean ($\\mu$) and standard deviation ($\\sigma$). It's also known as the Gaussian distribution. The PDF is:\n\n$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n\n**Exponential Distribution:** Models the time between events in a Poisson process. It's characterized by a rate parameter ($\\lambda$). The PDF is:\n\n$f(x) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$.\n\n**Uniform Distribution:**  Assigns equal probability to all values within a given range $[a, b]$. The PDF is:\n\n$f(x) = \\frac{1}{b-a}$ for $a \\le x \\le b$.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm, expon, uniform\n\n# Example plots (adjust parameters as needed)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Normal\nx_norm = np.linspace(-3, 3, 100)\nmu_norm = 0\nsigma_norm = 1\nprob_norm = norm.pdf(x_norm, mu_norm, sigma_norm)\naxes[0].plot(x_norm, prob_norm)\naxes[0].set_title(\"Normal Distribution (μ=0, σ=1)\")\n\n# Exponential\nx_exp = np.linspace(0, 5, 100)\nlambda_exp = 1\nprob_exp = expon.pdf(x_exp, scale=1/lambda_exp)\naxes[1].plot(x_exp, prob_exp)\naxes[1].set_title(\"Exponential Distribution (λ=1)\")\n\n# Uniform\na_unif = 0\nb_unif = 1\nx_unif = np.linspace(a_unif, b_unif, 100)\nprob_unif = uniform.pdf(x_unif, loc=a_unif, scale=b_unif - a_unif)\naxes[2].plot(x_unif, prob_unif)\naxes[2].set_title(\"Uniform Distribution (a=0, b=1)\")\n\n\nplt.show()\n```\n\n### Visualizing Probability Distributions\n\nVisualizing probability distributions provides valuable insights into their shapes, central tendencies, and spread.  Histograms, PMFs (for discrete distributions), PDFs (for continuous distributions), and CDFs are commonly used visualization tools.  The Python code examples above already showcase some of these visualizations.\n\n### Working with Distributions in Python\n\nPython libraries like SciPy and NumPy offer powerful tools for working with probability distributions. The examples above demonstrate how to generate random samples, calculate probabilities, and plot distributions using `scipy.stats`.  Further exploration of these libraries will greatly enhance your ability to perform Bayesian analysis.\n\n```{mermaid}\ngraph LR\n    A[Discrete Distributions] --> B(Bernoulli);\n    A --> C(Binomial);\n    A --> D(Poisson);\n    E[Continuous Distributions] --> F(Normal);\n    E --> G(Exponential);\n    E --> H(Uniform);\n    B -- PMF --> I[Visualization];\n    C -- PMF --> I;\n    D -- PMF --> I;\n    F -- PDF --> I;\n    G -- PDF --> I;\n    H -- PDF --> I;\n    I --> J[Python (SciPy, NumPy)];\n\n```\n\n\n## Expected Value and Variance\n\nExpected value and variance are two fundamental concepts in probability theory that describe the central tendency and spread of a probability distribution.  Understanding these concepts is crucial for interpreting statistical results and applying Bayesian methods.\n\n### Expected Value: Definition and Calculation\n\nThe expected value (or expectation) of a random variable $X$, denoted as $E[X]$ or $\\mu$, represents the average value of $X$ over many repeated trials.  For a discrete random variable with PMF $P(X=x)$, the expected value is:\n\n$E[X] = \\sum_{x} x \\cdot P(X=x)$\n\nFor a continuous random variable with PDF $f(x)$, the expected value is:\n\n$E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) dx$\n\nThe expected value is a weighted average, where each possible value of $X$ is weighted by its probability.\n\n### Variance and Standard Deviation\n\nThe variance of a random variable $X$, denoted as $Var(X)$ or $\\sigma^2$, measures the spread or dispersion of the distribution around its expected value.  It's the average squared deviation from the mean.  For a discrete random variable:\n\n$Var(X) = E[(X - \\mu)^2] = \\sum_{x} (x - \\mu)^2 \\cdot P(X=x)$\n\nFor a continuous random variable:\n\n$Var(X) = E[(X - \\mu)^2] = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\cdot f(x) dx$\n\nThe standard deviation, denoted as $\\sigma$, is the square root of the variance: $\\sigma = \\sqrt{Var(X)}$.  It's expressed in the same units as the random variable and provides a more interpretable measure of spread.\n\n### Properties of Expected Value and Variance\n\nSome important properties of expected value and variance include:\n\n* $E[c] = c$, where $c$ is a constant.\n* $E[aX + b] = aE[X] + b$, where $a$ and $b$ are constants.\n* $Var(c) = 0$, where $c$ is a constant.\n* $Var(aX + b) = a^2 Var(X)$, where $a$ and $b$ are constants.\n\n\n### Interpreting Expected Value and Variance\n\n* **Expected Value:**  Provides a measure of the central tendency or \"average\" of the distribution. It doesn't represent a value the variable will necessarily take on, but rather the long-run average of many observations.\n\n* **Variance/Standard Deviation:**  Indicates the variability or spread of the distribution. A larger variance means the values are more spread out from the mean, while a smaller variance suggests values are clustered closely around the mean.\n\n\n### Calculating Expected Value and Variance in Python\n\n```\\{python}\n#| echo: true\nimport numpy as np\nfrom scipy.stats import binom\n\n# Example: Expected value and variance of a binomial distribution\n\nn = 10  # Number of trials\np = 0.5  # Probability of success\n\n# Using scipy.stats\nmean_binom = binom.mean(n, p)\nvar_binom = binom.var(n, p)\nstd_binom = binom.std(n, p)\n\nprint(f\"Binomial Distribution (n={n}, p={p}):\")\nprint(f\"  Expected Value (Mean): {mean_binom}\")\nprint(f\"  Variance: {var_binom}\")\nprint(f\"  Standard Deviation: {std_binom}\")\n\n\n# Manual calculation (for demonstration)\n\nx = np.arange(n + 1)\npmf = binom.pmf(x, n, p)\nmanual_mean = np.sum(x * pmf)\nmanual_var = np.sum((x - manual_mean)**2 * pmf)\n\nprint(\"\\nManual Calculation:\")\nprint(f\"  Expected Value (Mean): {manual_mean}\")\nprint(f\"  Variance: {manual_var}\")\n\n\n```\n\n```{mermaid}\ngraph LR\n    A[Random Variable X] --> B(Expected Value E[X]);\n    A --> C(Variance Var(X));\n    C --> D(Standard Deviation σ);\n    B -. measures central tendency --> E[Interpretation];\n    C -. measures spread --> E;\n    E --> F[Understanding Distribution];\n```\n\n\n\n## Joint Probability Distributions\n\nThis chapter extends the concepts of probability distributions to scenarios involving multiple random variables.  Understanding joint distributions is crucial for tackling many real-world problems, particularly in Bayesian contexts where we often deal with multiple interacting variables.\n\n### Joint PMF and PDF\n\nWhen dealing with two or more random variables, we need to consider their joint distribution. This describes the probabilities of different combinations of values for all variables.\n\n* **Discrete Random Variables:** For discrete random variables $X$ and $Y$, the joint probability mass function (PMF) is denoted as $P(X=x, Y=y)$ and gives the probability that $X=x$ and $Y=y$ simultaneously.\n\n* **Continuous Random Variables:** For continuous random variables $X$ and $Y$, the joint probability density function (PDF) is denoted as $f(x, y)$.  The probability that $X$ falls within the interval $[a, b]$ and $Y$ falls within the interval $[c, d]$ is given by the double integral:\n\n   $P(a \\le X \\le b, c \\le Y \\le d) = \\int_{a}^{b} \\int_{c}^{d} f(x, y) \\, dy \\, dx$\n\n\n### Marginal Distributions\n\nThe marginal distribution of a single random variable is obtained from the joint distribution by summing or integrating over all possible values of the other variables.\n\n* **Discrete:** The marginal PMF of $X$ is: $P(X=x) = \\sum_{y} P(X=x, Y=y)$  (similarly for $Y$).\n\n* **Continuous:** The marginal PDF of $X$ is: $f_X(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dy$ (similarly for $Y$).\n\nThe marginal distributions describe the probability distribution of each variable individually, ignoring the other variable(s).\n\n\n### Conditional Distributions\n\nThe conditional distribution of one random variable given another describes the probability distribution of one variable when the value of the other variable is known.\n\n* **Discrete:** The conditional PMF of $X$ given $Y=y$ is: $P(X=x|Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}$, provided $P(Y=y) > 0$.\n\n* **Continuous:** The conditional PDF of $X$ given $Y=y$ is: $f(x|y) = \\frac{f(x, y)}{f_Y(y)}$, provided $f_Y(y) > 0$.\n\n\n### Independence of Random Variables\n\nTwo random variables $X$ and $Y$ are independent if knowing the value of one variable doesn't change the probability distribution of the other.  This means:\n\n* **Discrete:** $P(X=x, Y=y) = P(X=x)P(Y=y)$ for all $x$ and $y$.\n* **Continuous:** $f(x, y) = f_X(x)f_Y(y)$ for all $x$ and $y$.\n\nIf $X$ and $Y$ are independent, their joint distribution is simply the product of their marginal distributions.\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n\n# Example: Bivariate Normal Distribution (Illustrating marginal and conditional distributions)\n\n# Parameters for the bivariate normal distribution\nmean = [0, 0]\ncov = [[1, 0.8], [0.8, 1]]  # Covariance matrix (0.8 indicates correlation)\n\n# Generate data from bivariate normal distribution\ndata = multivariate_normal.rvs(mean=mean, cov=cov, size=1000)\nx, y = data[:, 0], data[:, 1]\n\n\n# Plot\nfig, ax = plt.subplots(figsize=(8,6))\nax.scatter(x,y, alpha=0.5, s=10)\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.set_title(\"Scatter Plot of Bivariate Normal Distribution\")\n\nplt.show()\n\n\n# Marginal distributions would require more complex integration for proper PDF calculation.\n# This section just highlights visualization of the joint distribution\n# Further analysis of marginal and conditional distributions requires more advanced techniques.\n\n```\n\n```{mermaid}\ngraph LR\n    A[X, Y (Joint Distribution)] --> B(Joint PMF/PDF);\n    B --> C(Marginal Distribution of X);\n    B --> D(Marginal Distribution of Y);\n    B --> E(Conditional Distribution X|Y);\n    B --> F(Conditional Distribution Y|X);\n    subgraph Independence\n        A -.  P(X,Y) = P(X)P(Y) --> G[Independent];\n    end\n    subgraph Dependence\n        A --> H[Dependent];\n    end\n```\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"probability-theory-essentials.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"probability-theory-essentials.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}