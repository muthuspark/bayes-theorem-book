{"title":"Basic Probability Concepts","markdown":{"headingText":"Basic Probability Concepts","containsRefs":false,"markdown":"\nThis section lays the groundwork for understanding Bayes' Theorem by reviewing fundamental concepts in probability theory.\n\n### Sample Space and Events\n\nThe sample space, often denoted as $\\Omega$ (Omega), represents the set of all possible outcomes of a random experiment.  An *event* is a subset of the sample space.  For example, if we roll a six-sided die, the sample space is $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$. The event \"rolling an even number\" is the subset $A = \\{2, 4, 6\\}$.  Another event could be \"rolling a number greater than 3\", represented by $B = \\{4, 5, 6\\}$.\n\nEvents can be combined using set operations:\n\n* **Union:** $A \\cup B$ represents the event that either A or B (or both) occur. In our example, $A \\cup B = \\{2, 4, 5, 6\\}$.\n* **Intersection:** $A \\cap B$ represents the event that both A and B occur.  In our example, $A \\cap B = \\{4, 6\\}$.\n* **Complement:** $A^c$ (or $A'$) represents the event that A does not occur.  The complement of A is $A^c = \\{1, 3, 5\\}$.\n\n\n### Probability Axioms\n\nProbability is a function $P$ that assigns a number between 0 and 1 to each event in the sample space, satisfying the following axioms:\n\n1. **Non-negativity:** For any event A, $P(A) \\ge 0$.\n2. **Normalization:** $P(\\Omega) = 1$. The probability of the entire sample space is 1.\n3. **Additivity:** For any two mutually exclusive events A and B (meaning $A \\cap B = \\emptyset$, the empty set), $P(A \\cup B) = P(A) + P(B)$. This extends to any finite or countable number of mutually exclusive events.\n\nFrom these axioms, several important properties can be derived, including:\n\n* $P(A^c) = 1 - P(A)$\n* $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$ (inclusion-exclusion principle)\n* $P(\\emptyset) = 0$\n\n\n### Conditional Probability\n\nConditional probability addresses the probability of an event given that another event has already occurred.  The conditional probability of event A given event B is denoted as $P(A|B)$ and is defined as:\n\n$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n\nprovided that $P(B) > 0$.  This formula expresses the probability of A occurring, *knowing* that B has already occurred.  The denominator adjusts the probability to reflect the reduced sample space.\n\nFor example, consider drawing two cards from a standard deck without replacement. Let A be the event that the second card is a king, and B be the event that the first card is a king. Then $P(A|B) = \\frac{3}{51}$ because given that the first card is a king, there are only 3 kings left out of 51 cards.\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\n\n# Example: visualizing conditional probability\nevents = ['King', 'Not King']\nprobabilities_before = [4/52, 48/52]\nprobabilities_after = [3/51, 48/51]\n\n\nplt.figure(figsize=(8, 6))\nplt.bar(events, probabilities_before, label='Before knowing the first card')\nplt.bar(events, probabilities_after, label='After knowing first card is King')\n\nplt.ylabel('Probability')\nplt.title('Conditional Probability Example')\nplt.legend()\nplt.show()\n\n```\n\n\n### The Concept of Independence\n\nTwo events A and B are independent if the occurrence of one does not affect the probability of the occurrence of the other. Mathematically, this is expressed as:\n\n$P(A|B) = P(A)$  or equivalently,  $P(B|A) = P(B)$\n\nor, most commonly used definition for independence:\n\n$P(A \\cap B) = P(A)P(B)$\n\nIf events are independent, the joint probability is simply the product of their individual probabilities.  If they are not independent, they are dependent.  The concept of independence is crucial in many areas of statistics and probability.\n\n\n```{mermaid}\ngraph LR\n    A[Event A] -->|P(A|B) = P(A)| B[Event B];\n    B -->|P(B|A) = P(B)| A;\n    subgraph Independent Events\n        style A fill:#ccf,stroke:#000,stroke-width:2px\n        style B fill:#ccf,stroke:#000,stroke-width:2px\n    end\n```\n\n\n## Joint and Marginal Probability\n\nThis section delves into joint and marginal probability distributions, essential concepts for understanding Bayes' Theorem and its applications.\n\n### Joint Probability Distributions\n\nA joint probability distribution describes the probability of two or more events occurring together.  For discrete random variables $X$ and $Y$, the joint probability mass function (PMF) is denoted as $P(X=x, Y=y)$ or more concisely as $P(x, y)$.  This function gives the probability that $X$ takes on the value $x$ and $Y$ takes on the value $y$ simultaneously.  For continuous random variables, we use the joint probability density function (PDF), denoted as $f(x, y)$.  The joint distribution must satisfy the following conditions:\n\n* $P(x, y) \\ge 0$ for all $x, y$ (or $f(x, y) \\ge 0$)\n* $\\sum_{x}\\sum_{y} P(x, y) = 1$ for discrete variables (or $\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x, y) \\,dx \\,dy = 1$ for continuous variables)\n\nThe joint distribution contains all the information about the individual variables and their relationship.\n\n\n### Calculating Marginal Probabilities from Joint Distributions\n\nMarginal probabilities represent the probability of a single event occurring, regardless of the outcome of other events.  They are obtained by \"marginalizing\" (summing or integrating) over the other variables in the joint distribution.\n\nFor discrete variables:\n\n* $P(X=x) = \\sum_{y} P(x, y)$  (Marginal PMF of X)\n* $P(Y=y) = \\sum_{x} P(x, y)$  (Marginal PMF of Y)\n\nFor continuous variables:\n\n* $f(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\,dy$ (Marginal PDF of X)\n* $f(y) = \\int_{-\\infty}^{\\infty} f(x, y) \\,dx$ (Marginal PDF of Y)\n\n\n### Visualizing Joint and Marginal Probabilities\n\nJoint and marginal probabilities can be visualized using various methods.  For discrete variables, a joint probability table or a bar chart is often used.  For continuous variables, we might use a heatmap or contour plot for the joint distribution, and histograms or density plots for the marginal distributions.\n\n\n### Joint Probability Tables\n\nA joint probability table is a convenient way to represent the joint probability distribution for discrete random variables.  Each cell in the table represents the joint probability $P(x, y)$ for a specific pair of values $(x, y)$.  The sums of the rows and columns give the marginal probabilities.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Example: Joint Probability Table and Visualization\n\n# Sample data (replace with your own data)\ndata = {'X': [1, 1, 2, 2], 'Y': [1, 2, 1, 2], 'Probability': [0.2, 0.1, 0.3, 0.4]}\ndf = pd.DataFrame(data)\njoint_prob_table = pd.pivot_table(df, values='Probability', index=['X'], columns=['Y'], aggfunc=np.sum)\n\n\n#Calculating Marginal Probabilities\nmarginal_x = joint_prob_table.sum(axis=1)\nmarginal_y = joint_prob_table.sum(axis=0)\n\nprint(\"Joint Probability Table:\\n\", joint_prob_table)\nprint(\"\\nMarginal Probability of X:\\n\", marginal_x)\nprint(\"\\nMarginal Probability of Y:\\n\", marginal_y)\n\n# Visualization\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nsns.heatmap(joint_prob_table, annot=True, cmap=\"Blues\", fmt=\".2f\")\nplt.title(\"Joint Probability Distribution\")\nplt.subplot(1, 2, 2)\nplt.bar(['X=1', 'X=2'], marginal_x, label='Marginal P(X)')\nplt.bar(['Y=1', 'Y=2'], marginal_y, label='Marginal P(Y)')\nplt.legend()\nplt.title(\"Marginal Probabilities\")\nplt.show()\n\n```\n\n```{mermaid}\ngraph LR\n    A[Joint Probability<br/>P(X,Y)] --> B(Marginal Probability<br/>P(X));\n    A --> C(Marginal Probability<br/>P(Y));\n```\n\n\n## Conditional Probability and Bayes' Theorem (Preview)\n\nThis section provides a gentle introduction to conditional probability and offers a preview of Bayes' Theorem, which will be explored in detail in subsequent chapters.\n\n### Understanding Conditional Probability\n\nConditional probability quantifies the likelihood of an event occurring given that another event has already happened.  Recall the definition from the previous section:\n\n$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n\nwhere $P(A|B)$ is the conditional probability of event A given event B, $P(A \\cap B)$ is the joint probability of both A and B occurring, and $P(B)$ is the probability of event B.  The key insight is that knowing B has occurred changes the sample space, affecting the probability of A.\n\n\n### Bayes' Theorem: An Intuitive Introduction\n\nBayes' Theorem provides a way to update our beliefs about an event based on new evidence.  It's a powerful tool for revising probabilities in light of observed data.  In its simplest form, Bayes' Theorem states:\n\n$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n\nHere:\n\n* $P(A|B)$ is the posterior probability of A given B (what we want to find).\n* $P(B|A)$ is the likelihood of B given A.\n* $P(A)$ is the prior probability of A (our initial belief).\n* $P(B)$ is the probability of B (evidence).\n\n\nThe denominator, $P(B)$, can be expanded using the law of total probability (discussed in more detail later):\n\n$P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$\n\n\n### Illustrative Examples of Conditional Probability\n\nLet's illustrate conditional probability with a simple example. Suppose we have a bag containing 3 red marbles and 2 blue marbles.\n\n* **Event A:** Drawing a red marble.\n* **Event B:** Drawing a blue marble.\n\n$P(A) = \\frac{3}{5}$  and  $P(B) = \\frac{2}{5}$\n\nNow, let's consider drawing two marbles without replacement. What's the probability of drawing a red marble second, given that the first marble was red?  This is $P(A_2|A_1)$.\n\n$P(A_2|A_1) = \\frac{P(A_2 \\cap A_1)}{P(A_1)} = \\frac{2/10}{3/5} = \\frac{2}{6} = \\frac{1}{3}$\n\nThere are only 2 red marbles left out of 4 total marbles after drawing one red marble.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\n\n# Visualizing the Marble Example\n\nevents = ['Red', 'Blue']\nprobabilities_before = [3/5, 2/5]\nprobabilities_after_red = [2/4, 2/4]\n\nplt.figure(figsize=(8, 6))\nplt.bar(events, probabilities_before, label='Before First Draw')\nplt.bar(events, probabilities_after_red, label='After First Draw (Red)')\n\nplt.ylabel('Probability')\nplt.title('Conditional Probability: Marble Example')\nplt.legend()\nplt.show()\n\n```\n\n### Setting the Stage for Bayes' Theorem\n\nThe examples above highlight the importance of conditional probability.  Bayes' Theorem essentially provides a formal framework for updating our understanding of probabilities given new information.  We use prior knowledge ($P(A)$) and new evidence ($P(B|A)$) to calculate a refined posterior probability ($P(A|B)$).  The subsequent chapters will delve into the application of Bayes' Theorem in more complex scenarios, and demonstrate its usefulness in various fields.\n\n```{mermaid}\ngraph LR\n    A[Prior Probability P(A)] --> B(Evidence P(B|A));\n    B --> C[Posterior Probability P(A|B)];\n    subgraph Bayes' Theorem\n        style A fill:#ccf,stroke:#000,stroke-width:2px\n        style B fill:#ccf,stroke:#000,stroke-width:2px\n        style C fill:#ccf,stroke:#000,stroke-width:2px\n    end\n```\n\n\n## Independence of Events\n\nUnderstanding independence is crucial for applying Bayes' Theorem and for many probability calculations. This section explores the concept of independence in detail.\n\n### Definition of Independence\n\nTwo events, A and B, are independent if the occurrence of one event does not affect the probability of the occurrence of the other event.  Mathematically, this is expressed as:\n\n$P(A|B) = P(A)$\n\nor equivalently:\n\n$P(B|A) = P(B)$\n\nThe most common and useful definition for independence is:\n\n$P(A \\cap B) = P(A)P(B)$\n\nThis means the joint probability of A and B occurring is simply the product of their individual probabilities.  If this equation holds, then A and B are independent.  If it doesn't, they are dependent.\n\n\n### Testing for Independence\n\nTo test whether two events are independent, we compare the joint probability $P(A \\cap B)$ with the product of the individual probabilities $P(A)P(B)$. If they are approximately equal (allowing for some margin of error due to sampling variation), we can conclude that the events are likely independent. A statistically rigorous test would involve hypothesis testing, which is beyond the scope of this introductory section.\n\n\n### Conditional Independence\n\nEvents A and B are conditionally independent given a third event C if:\n\n$P(A \\cap B | C) = P(A|C)P(B|C)$\n\nThis means that given the knowledge that C has occurred, the occurrence of A doesn't influence the probability of B and vice versa.  Note that conditional independence does not imply (and is not implied by) marginal independence.  Events can be independent in one context (given C) but dependent in another.\n\n\n### Consequences of Independence for Probability Calculations\n\nIndependence significantly simplifies probability calculations. When events are independent, the joint probability is simply the product of the individual probabilities, as shown above. This makes calculating probabilities of complex events much easier.  For example, if we have *n* independent events $A_1, A_2, \\dots, A_n$, the probability that all of them occur is:\n\n\n$P(A_1 \\cap A_2 \\cap \\dots \\cap A_n) = P(A_1)P(A_2) \\dots P(A_n)$\n\nThis result extends to more complex scenarios, making it a powerful tool in probability modeling.  Conversely, the absence of independence requires more complex calculations involving conditional probabilities and joint distributions.\n\n\n```\\{python}\n#| echo: true\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example illustrating independence vs. dependence\n\n#Independent Events\nprob_A = 0.6\nprob_B = 0.4\nprob_A_and_B = prob_A * prob_B\n\n#Dependent Events (Example)\nprob_C = 0.5\nprob_D_given_C = 0.8  #probability of D given C occurred\nprob_D_and_C = prob_C * prob_D_given_C\n\n\nevents = ['A and B','C and D']\nprobabilities = [prob_A_and_B, prob_D_and_C]\nplt.figure(figsize=(8,6))\nplt.bar(events,probabilities)\nplt.ylabel('Probability')\nplt.title('Independent vs Dependent Events')\nplt.show()\n\nprint(f\"Probability of A and B (independent): {prob_A_and_B}\")\nprint(f\"Probability of C and D (dependent): {prob_D_and_C}\")\n\n```\n\n```{mermaid}\ngraph LR\n    A[Event A] -.-> B[Event B];\n    subgraph Independent Events\n        style A fill:#ccf,stroke:#000,stroke-width:2px\n        style B fill:#ccf,stroke:#000,stroke-width:2px\n        linkStyle 0,1,2,3 stroke:#000,stroke-width:2px,stroke-dasharray: 5 5\n    end\n    C[Event C] --> D[Event D];\n    subgraph Dependent Events\n        style C fill:#ccf,stroke:#000,stroke-width:2px\n        style D fill:#ccf,stroke:#000,stroke-width:2px\n    end\n```\n\n\n\n## Discrete and Continuous Random Variables\n\nThis section distinguishes between discrete and continuous random variables, introducing key concepts for describing their probability distributions.\n\n### Discrete Random Variables and Probability Mass Functions\n\nA discrete random variable is a variable whose value can only take on a finite number of values or a countably infinite number of values.  The probability distribution of a discrete random variable is described by its probability mass function (PMF), denoted as $P(X=x)$ or $p(x)$.  This function assigns a probability to each possible value of the variable.  The PMF must satisfy:\n\n* $P(X=x) \\ge 0$ for all x\n* $\\sum_{x} P(X=x) = 1$  (The sum of probabilities over all possible values is 1)\n\nFor example, the outcome of rolling a fair six-sided die is a discrete random variable.  The PMF would be $P(X=x) = \\frac{1}{6}$ for $x \\in \\{1, 2, 3, 4, 5, 6\\}$.\n\n\n### Continuous Random Variables and Probability Density Functions\n\nA continuous random variable is a variable whose value can take on any value within a given range.  The probability distribution of a continuous random variable is described by its probability density function (PDF), denoted as $f(x)$.  Unlike the PMF, the PDF doesn't directly give the probability of a specific value; instead, the probability of the variable falling within a certain interval is given by the integral of the PDF over that interval:\n\n$P(a \\le X \\le b) = \\int_{a}^{b} f(x) \\,dx$\n\nThe PDF must satisfy:\n\n* $f(x) \\ge 0$ for all x\n* $\\int_{-\\infty}^{\\infty} f(x) \\,dx = 1$\n\n\nFor example, the height of a randomly selected adult is a continuous random variable.\n\n\n### Cumulative Distribution Functions (CDFs)\n\nThe cumulative distribution function (CDF), denoted as $F(x)$, gives the probability that a random variable X is less than or equal to a given value x.  It's defined for both discrete and continuous random variables:\n\n* **Discrete:** $F(x) = P(X \\le x) = \\sum_{k \\le x} P(X=k)$\n* **Continuous:** $F(x) = P(X \\le x) = \\int_{-\\infty}^{x} f(t) \\,dt$\n\nThe CDF is a non-decreasing function, ranging from 0 to 1.\n\n\n### Expected Value and Variance\n\nThe expected value (or mean) of a random variable, denoted as $E[X]$ or $\\mu$, represents the average value of the variable over many repetitions of the experiment.\n\n* **Discrete:** $E[X] = \\sum_{x} x P(X=x)$\n* **Continuous:** $E[X] = \\int_{-\\infty}^{\\infty} x f(x) \\,dx$\n\n\nThe variance, denoted as $Var(X)$ or $\\sigma^2$, measures the spread or dispersion of the distribution around the mean.  It's the expected value of the squared difference between the variable and its mean:\n\n$Var(X) = E[(X - \\mu)^2]$\n\nThe standard deviation, $\\sigma = \\sqrt{Var(X)}$, is the square root of the variance and has the same units as the random variable.\n\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm\n\n# Example: Discrete (Binomial) and Continuous (Normal) distributions\n\n# Binomial Distribution (Discrete)\nn = 10  # Number of trials\np = 0.5 # Probability of success\nx = np.arange(0, n + 1)\npmf = binom.pmf(x, n, p)\ncdf = binom.cdf(x, n, p)\n\n# Normal Distribution (Continuous)\nmu = 0   # Mean\nsigma = 1 # Standard Deviation\nx_cont = np.linspace(-3, 3, 100)\npdf_cont = norm.pdf(x_cont, mu, sigma)\ncdf_cont = norm.cdf(x_cont, mu, sigma)\n\n#Plotting\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.stem(x, pmf, label='PMF', use_line_collection=True)\nplt.plot(x, cdf, label='CDF', drawstyle='steps-post')\nplt.xlabel('x')\nplt.ylabel('Probability')\nplt.title('Binomial Distribution')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(x_cont, pdf_cont, label='PDF')\nplt.plot(x_cont, cdf_cont, label='CDF')\nplt.xlabel('x')\nplt.ylabel('Probability Density')\nplt.title('Normal Distribution')\nplt.legend()\nplt.show()\n\n\n#Expected Value and Variance (Binomial)\nexpected_value = n*p\nvariance = n*p*(1-p)\nprint(\"Binomial Distribution:\")\nprint(f\"Expected Value: {expected_value}\")\nprint(f\"Variance: {variance}\")\n\n#Expected Value and Variance (Normal) - parameters are mu and sigma\nprint(\"\\nNormal Distribution:\")\nprint(f\"Expected Value (mu): {mu}\")\nprint(f\"Variance (sigma^2): {sigma**2}\")\n```\n\n```{mermaid}\ngraph LR\n    A[Discrete Random Variable] --> B(Probability Mass Function (PMF));\n    C[Continuous Random Variable] --> D(Probability Density Function (PDF));\n    B --> E(Cumulative Distribution Function (CDF));\n    D --> E;\n    E --> F(Expected Value & Variance);\n\n```\n\n\n## Common Probability Distributions\n\nThis section introduces several commonly encountered probability distributions, highlighting their properties and applications.\n\n### Bernoulli Distribution\n\nThe Bernoulli distribution models a single trial with two possible outcomes: success (1) or failure (0).  The probability of success is denoted by $p$, and the probability of failure is $1-p$.  The PMF is:\n\n$P(X=k) = p^k (1-p)^{1-k}$  for  $k \\in \\{0, 1\\}$\n\nwhere $X$ is the Bernoulli random variable.\n\nThe expected value is $E[X] = p$, and the variance is $Var(X) = p(1-p)$.\n\n### Binomial Distribution\n\nThe binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.  It's characterized by two parameters: $n$ (number of trials) and $p$ (probability of success in each trial).  The PMF is:\n\n$P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}$  for  $k \\in \\{0, 1, \\dots, n\\}$\n\nwhere $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the binomial coefficient.\n\nThe expected value is $E[X] = np$, and the variance is $Var(X) = np(1-p)$.\n\n\n### Poisson Distribution\n\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space, given a constant average rate of events.  It's characterized by a single parameter, $\\lambda$ (the average rate of events).  The PMF is:\n\n$P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$  for  $k \\in \\{0, 1, 2, \\dots\\}$\n\nwhere $e$ is the base of the natural logarithm.\n\nThe expected value is $E[X] = \\lambda$, and the variance is $Var(X) = \\lambda$.\n\n\n### Normal Distribution\n\nThe normal (or Gaussian) distribution is a continuous distribution, symmetric around its mean, and characterized by its mean $\\mu$ and standard deviation $\\sigma$.  Its PDF is:\n\n$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n\nThe expected value is $E[X] = \\mu$, and the variance is $Var(X) = \\sigma^2$.  The normal distribution is crucial in many areas of statistics due to the central limit theorem.\n\n\n### Uniform Distribution\n\nThe uniform distribution assigns equal probability to all values within a given range $[a, b]$.  Its PDF is:\n\n$f(x) = \\begin{cases} \\frac{1}{b-a} & a \\le x \\le b \\\\ 0 & \\text{otherwise} \\end{cases}$\n\nThe expected value is $E[X] = \\frac{a+b}{2}$, and the variance is $Var(X) = \\frac{(b-a)^2}{12}$.\n\n\n```\\{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli, binom, poisson, norm, uniform\n\n# Plotting various distributions\nx_bernoulli = [0, 1]\nx_binom = np.arange(0, 11)\nx_poisson = np.arange(0, 11)\nx_normal = np.linspace(-3, 3, 100)\nx_uniform = np.linspace(0, 1, 100)\n\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 3, 1)\nplt.bar(x_bernoulli, bernoulli.pmf(x_bernoulli, 0.6))\nplt.title('Bernoulli Distribution (p=0.6)')\n\nplt.subplot(2, 3, 2)\nplt.bar(x_binom, binom.pmf(x_binom, 10, 0.5))\nplt.title('Binomial Distribution (n=10, p=0.5)')\n\nplt.subplot(2, 3, 3)\nplt.bar(x_poisson, poisson.pmf(x_poisson, 3))\nplt.title('Poisson Distribution (λ=3)')\n\nplt.subplot(2, 3, 4)\nplt.plot(x_normal, norm.pdf(x_normal, 0, 1))\nplt.title('Normal Distribution (μ=0, σ=1)')\n\nplt.subplot(2, 3, 5)\nplt.plot(x_uniform, uniform.pdf(x_uniform, 0, 1))\nplt.title('Uniform Distribution (a=0, b=1)')\n\nplt.tight_layout()\nplt.show()\n```\n\n```{mermaid}\ngraph LR\n    A[Common Distributions] --> B(Bernoulli);\n    A --> C(Binomial);\n    A --> D(Poisson);\n    A --> E(Normal);\n    A --> F(Uniform);\n```\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"statistical-concepts.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"statistical-concepts.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}