{"title":"Scientific Applications","markdown":{"headingText":"Scientific Applications","containsRefs":false,"markdown":"\n### Prior Elicitation for Experimental Parameters\n\nIn scientific experiments, choosing appropriate prior distributions for parameters is essential for Bayesian analysis.  Prior elicitation involves translating expert knowledge or previous data into a probability distribution.  This process can be subjective but aims to reflect the pre-experimental belief about the parameter.  For example, if we're investigating the effectiveness of a new drug, we might use a weakly informative prior that allows for a wide range of effects but penalizes extremely large or small effects.  Suppose we believe the drug's effect size ($\\theta$) is likely between -0.2 and 0.8, and we model this with a Beta distribution.  We can use the `scipy.stats` library to define this prior:\n\n\n```{python}\n#| echo: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n# Prior parameters (adjust to reflect prior belief)\nalpha_prior = 2  \nbeta_prior = 2\n\n# Create the prior distribution\nx = np.linspace(-0.2, 0.8, 100)\nprior = beta(alpha_prior, beta_prior).pdf((x + 0.2) / 1)  # Scale and shift for desired range\n\n# Plot the prior\nplt.plot(x, prior)\nplt.xlabel(\"Effect Size (Î¸)\")\nplt.ylabel(\"Prior Density\")\nplt.title(\"Prior Distribution for Drug Effect Size\")\nplt.show()\n```\n\nThis code generates a plot showing our prior belief.  The choice of Beta distribution allows for bounded support between -0.2 and 0.8, reflecting our prior knowledge.  For complicated cases, we might need a more detailed initial assessment, possibly involving expert interviews or hierarchical models. The choice of the prior should always be documented and justified.\n\n### Bayesian Optimization for Experimental Design\n\nBayesian optimization is a powerful technique for efficiently exploring the parameter space of an experiment.  It uses a surrogate model (often a Gaussian process) to approximate the objective function, which is usually computationally expensive to evaluate. This surrogate model allows the algorithm to intelligently choose the next experiment's parameters to maximize information gain or minimize expected loss.\n\nConsider optimizing a function $f(x)$, where $x$ is a vector of experimental parameters.  A Gaussian process (GP) models the function as:\n\n$f(x) \\sim GP(m(x), k(x, x'))$\n\nwhere $m(x)$ is the mean function and $k(x, x')$ is the covariance function (kernel). Bayesian optimization iteratively updates the GP based on observed data, using an acquisition function (e.g., Expected Improvement, Upper Confidence Bound) to select promising points for the next experiment.\n\nThe following (simplified) code illustrates the core idea using the `scikit-optimize` library:\n\n```{python}\n#| echo: true\nfrom skopt import gp_minimize\n\n# Define the objective function (replace with your experiment)\ndef objective_function(x):\n    #This is a placeholder, replace with actual experimental function.\n    return (x[0] - 2)**2 + (x[1] - 3)**2\n\n\n# Define the search space\nfrom skopt.space import Real\nspace = [Real(-5, 10, name='x'), Real(-5, 10, name='y')]\n\n# Perform Bayesian Optimization\nres = gp_minimize(objective_function, space, n_calls=20, random_state=0)\n\n# Print the best parameters and objective function value\nprint(\"Best parameters:\", res.x)\nprint(\"Best objective function value:\", res.fun)\n```\n\n\n### A/B Testing with Bayesian Inference\n\nA/B testing is used to compare two versions (A and B) of a system, such as a website or an advertisement. Bayesian inference provides a better approach to A/B testing compared to frequentist methods.  Instead of simply calculating p-values, we obtain posterior distributions for the conversion rates of both versions.  This allows for more informed decision-making, considering uncertainty in the estimates.\n\n\nLet's assume we have $n_A$ trials for version A with $s_A$ successes and $n_B$ trials for version B with $s_B$ successes. We can model the conversion rates ($\\theta_A$ and $\\theta_B$) using Beta distributions:\n\n$p(\\theta_A|s_A, n_A) \\sim Beta(s_A + 1, n_A - s_A + 1)$\n$p(\\theta_B|s_B, n_B) \\sim Beta(s_B + 1, n_B - s_B + 1)$\n\nWe can then calculate the probability that version A is better than version B:\n\n$P(\\theta_A > \\theta_B | data) = \\int_0^1 \\int_0^{\\theta_A} p(\\theta_A|s_A, n_A) p(\\theta_B|s_B, n_B) d\\theta_B d\\theta_A$\n\n\nThis integral can be approximated using Monte Carlo sampling:\n\n\n```{python}\n#| echo: true\nimport numpy as np\nfrom scipy.stats import beta\n\n#Observed data (replace with your data)\ns_A = 10\nn_A = 100\ns_B = 15\nn_B = 100\n\n# Sample from posterior distributions\nsamples_A = beta(s_A + 1, n_A - s_A + 1).rvs(10000)\nsamples_B = beta(s_B + 1, n_B - s_B + 1).rvs(10000)\n\n# Estimate the probability that A is better than B\nprob_A_better = np.mean(samples_A > samples_B)\nprint(f\"Probability that A is better than B: {prob_A_better}\")\n\n```\n\n### Adaptive Experimental Designs\n\nAdaptive experimental designs adjust the experimental procedure based on the data collected so far.  This is particularly useful when resources are limited or when the goal is to quickly find the optimal treatment.  Bayesian methods are well-suited for adaptive designs, as they allow for efficient updating of beliefs as data becomes available.  For example, in clinical trials, an adaptive design might change the allocation of patients to different treatment arms based on the accumulating evidence of their efficacy.\n\nA simple example would be a sequential design where a new data point is added and the posterior is updated after each experiment. This approach allows for a dynamic adjustment of experimental parameters based on accruing evidence.  More complex adaptive designs involve complex algorithms for optimizing the allocation of resources and exploring the experimental space efficiently.  The choice of a specific adaptive design depends heavily on the particular problem and experimental constraints.\n\n\n```{mermaid}\ngraph LR\nA[Start] --> B{Data Available?};\nB -- Yes --> C[Update Posterior];\nC --> D{Stopping Criteria Met?};\nD -- No --> E[Select Next Experiment];\nE --> B;\nD -- Yes --> F[End];\n```\n\nThis mermaid diagram shows the basic flow of an adaptive Bayesian experimental design.  The design continuously updates its knowledge (posterior) and decides whether to continue or stop based on predefined criteria.  More elaborate designs can be developed using tools from reinforcement learning or multi-armed bandit problems.\n\n\n## Bayesian Data Analysis Techniques\n\n### Bayesian Linear Regression\n\nBayesian linear regression extends the classical linear regression model by incorporating prior distributions for the model parameters.  This allows us to quantify uncertainty in the parameter estimates and make more robust predictions.  Consider the standard linear model:\n\n$y_i = \\mathbf{x}_i^T \\mathbf{\\beta} + \\epsilon_i$,  where $\\epsilon_i \\sim N(0, \\sigma^2)$\n\nHere, $y_i$ is the response variable, $\\mathbf{x}_i$ is a vector of predictors, $\\mathbf{\\beta}$ is the vector of regression coefficients, and $\\epsilon_i$ is the error term. In the Bayesian framework, we assign prior distributions to $\\mathbf{\\beta}$ and $\\sigma^2$.  Common choices include a normal prior for $\\mathbf{\\beta}$ and an inverse-gamma prior for $\\sigma^2$:\n\n$p(\\mathbf{\\beta}) \\sim N(\\mathbf{\\mu}_0, \\mathbf{\\Sigma}_0)$\n$p(\\sigma^2) \\sim InvGamma(a, b)$\n\nUsing Bayes' theorem, the posterior distribution is proportional to the likelihood times the prior:\n\n$p(\\mathbf{\\beta}, \\sigma^2 | \\mathbf{y}, \\mathbf{X}) \\propto p(\\mathbf{y} | \\mathbf{X}, \\mathbf{\\beta}, \\sigma^2) p(\\mathbf{\\beta}) p(\\sigma^2)$\n\nWe can use Markov Chain Monte Carlo (MCMC) methods, such as PyMC, to sample from the posterior distribution:\n\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some sample data\nnp.random.seed(0)\nn = 100\nX = np.random.rand(n, 2)\nbeta_true = np.array([1, 2])\nsigma_true = 0.5\ny = X.dot(beta_true) + np.random.normal(0, sigma_true, n)\n\n# Bayesian Linear Regression with PyMC\nwith pm.Model() as model:\n    # Priors\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n    beta = pm.Normal(\"beta\", mu=0, sigma=10, shape=2)\n\n    # Likelihood\n    mu = pm.Deterministic(\"mu\", X.dot(beta))\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y)\n\n    # Posterior sampling\n    trace = pm.sample(2000, tune=1000)\n\n# Plot posterior distributions\npm.plot_trace(trace);\nplt.show()\n```\n\n\n### Bayesian Logistic Regression\n\nBayesian logistic regression models the probability of a binary outcome using a logistic function:\n\n$P(y_i = 1 | \\mathbf{x}_i, \\mathbf{\\beta}) = \\frac{1}{1 + exp(-\\mathbf{x}_i^T \\mathbf{\\beta})}$\n\nSimilar to linear regression, we assign prior distributions to the regression coefficients $\\mathbf{\\beta}$.  Common choices include normal priors.  The posterior distribution is again obtained using Bayes' theorem and sampled using MCMC:\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(0)\nn = 100\nX = np.random.rand(n, 2)\nbeta_true = np.array([1, -2])\np = 1 / (1 + np.exp(-X.dot(beta_true)))\ny = np.random.binomial(1, p, size=n)\n\nwith pm.Model() as model:\n    beta = pm.Normal(\"beta\", mu=0, sigma=10, shape=2)\n    p = pm.Deterministic(\"p\", pm.math.sigmoid(X.dot(beta)))\n    y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)\n    trace = pm.sample(2000, tune=1000)\n\npm.plot_trace(trace);\nplt.show()\n```\n\n\n### Bayesian Model Selection (BIC, DIC)\n\nBayesian model selection involves comparing different models to find the one that best explains the data.  The Bayesian Information Criterion (BIC) and Deviance Information Criterion (DIC) are commonly used for this purpose.  BIC penalizes model complexity more strongly than DIC.  Both are calculated from the posterior samples.  Lower values indicate better models.\n\n* **BIC:** $BIC = -2\\log(p(y|\\theta^*)) + k \\log(n)$  where $\\theta^*$ are the maximum a posteriori (MAP) estimates, k is the number of parameters and n is the sample size.\n\n* **DIC:** $DIC = \\bar{D} + p_D$, where $\\bar{D}$ is the posterior mean of the deviance and $p_D$ is the effective number of parameters.\n\n\n### Markov Chain Monte Carlo (MCMC) Methods\n\nMCMC methods are essential for sampling from complex posterior distributions in Bayesian analysis.  They involve constructing a Markov chain whose stationary distribution is the target posterior.  Popular MCMC algorithms include Metropolis-Hastings and Hamiltonian Monte Carlo (HMC).  PyMC uses HMC by default, which is generally more efficient than Metropolis-Hastings for many problems.  The efficiency of MCMC depends on careful tuning of parameters and appropriate proposal distributions.\n\n\n\n### Handling Missing Data with Bayesian Methods\n\nBayesian methods offer a natural framework for handling missing data.  Instead of simply imputing missing values, Bayesian methods treat them as unknown parameters and integrate over their possible values during posterior inference.  This approach accounts for the uncertainty associated with the missing data, leading to more accurate and reliable results.  This can be done by including the missing data as parameters in the model and specifying appropriate priors for them.\n\n\n### Model Checking and Diagnostics\n\nAfter fitting a Bayesian model, it's essential to check its adequacy and diagnose potential problems.  This involves examining the posterior distributions of the parameters, assessing the goodness of fit, and checking for model misspecification.  Diagnostic tools include:\n\n* **Trace plots:** Visualize the MCMC chains to assess convergence and mixing.\n* **Posterior predictive checks:** Compare observed data with simulated data from the posterior predictive distribution.  Significant discrepancies suggest model misspecification.\n* **Gelman-Rubin statistic:** Quantifies the convergence of multiple MCMC chains.\n* **Autocorrelation plots:** Assess the correlation between successive samples in the MCMC chains.\n\n\n```{mermaid}\ngraph LR\nA[Fit Bayesian Model] --> B(Trace Plots);\nA --> C(Posterior Predictive Checks);\nA --> D(Gelman-Rubin Statistic);\nA --> E(Autocorrelation Plots);\nB --> F[Diagnostics];\nC --> F;\nD --> F;\nE --> F;\nF --> G[Model Adequacy?];\nG -- Yes --> H[Inference];\nG -- No --> I[Model Refinement];\n```\n\nThis mermaid diagram illustrates the model checking process.  If the model diagnostics are satisfactory, we proceed with inference. Otherwise, we need to refine the model before drawing conclusions.\n\n\n## Bayesian Hypothesis Testing\n\n### Bayes Factors\n\nBayes factors provide a principled way to compare the evidence for two competing hypotheses, $H_1$ and $H_0$.  They are defined as the ratio of the marginal likelihoods of the data under each hypothesis:\n\n$BF_{10} = \\frac{p(D|H_1)}{p(D|H_0)}$\n\nA Bayes factor greater than 1 provides evidence in favor of $H_1$, while a value less than 1 supports $H_0$.  The strength of evidence is often interpreted using scales proposed by Jeffreys:\n\n| $BF_{10}$ | Evidence for $H_1$ |\n|---|---|\n| 1-3 | Weak |\n| 3-10 | Moderate |\n| 10-30 | Strong |\n| >30 | Very Strong |\n\n\nCalculating the marginal likelihoods can be challenging, often requiring complex computational techniques like thermodynamic integration or bridge sampling.  However, for some models, analytical approximations are available.  Here's a simple example using simulated data:\n\n```{python}\n#| echo: true\nimport numpy as np\nfrom scipy.stats import norm\n\n#Simulate data under two hypotheses\nn = 100\nmu_true_h1 = 1\nmu_true_h0 = 0\nsigma = 1\ndata = np.random.normal(mu_true_h1, sigma, n)\n\n#Assume Normal priors on mu\nsigma_prior = 10\nmu_prior_h1 = 0\nmu_prior_h0 = 0\n#Likelihood under each hypothesis\nlikelihood_h1 = norm.pdf(data, loc = mu_true_h1, scale=sigma).prod()\nlikelihood_h0 = norm.pdf(data, loc = mu_true_h0, scale=sigma).prod()\n\n#Bayes factor\nBF_10 = likelihood_h1/likelihood_h0\nprint(f\"Bayes factor: {BF_10}\")\n\n```\n\n### Posterior Predictive Checks\n\nPosterior predictive checks assess the compatibility of the model with the observed data.  They involve simulating new data from the posterior predictive distribution, $p(y_{rep}|y)$, and comparing these replicated datasets to the observed data.  Large discrepancies suggest model inadequacy.  This is a visual approach rather than providing a single numerical value.\n\n```{python}\n#| echo: true\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Example using a simple normal model\nwith pm.Model() as model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=10)\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=data)\n\n    # Posterior predictive samples\n    y_rep = pm.Normal(\"y_rep\", mu=mu, sigma=sigma, shape=len(data))\n\n    trace = pm.sample(2000, tune=1000)\n\n#Plot observed vs replicated data\nplt.hist(data, alpha=0.5, label=\"Observed Data\")\nplt.hist(trace[\"y_rep\"].mean(axis=0), alpha=0.5, label=\"Replicated Data\")\nplt.legend()\nplt.show()\n\n```\n\n### Comparing Competing Hypotheses\n\nBayesian hypothesis testing allows for the comparison of multiple hypotheses simultaneously.  Instead of using a single null hypothesis, we can specify many alternative hypotheses and calculate the Bayes factors among them.  This provides a more detailed assessment of the relative evidence for each hypothesis.\n\n\n### Interpreting Bayesian p-values\n\nUnlike frequentist p-values, Bayesian p-values (also called posterior predictive p-values) represent the probability of observing data as extreme as or more extreme than the observed data, given the model.  They are calculated by simulating data from the posterior predictive distribution and computing the proportion of simulated datasets that are as or more extreme than the observed data. A small Bayesian p-value suggests potential model inadequacy.\n\n\n### Bayesian Credible Intervals\n\nA credible interval is an interval within which a parameter lies with a specified probability, according to the posterior distribution. For example, a 95% credible interval means that there's a 95% probability that the true parameter value lies within the interval.   This is a direct probability statement about the parameter, unlike the frequentist confidence interval.  Credible intervals are easily obtained from the posterior samples:\n\n```{python}\n#| echo: true\nimport numpy as np\nimport pymc as pm\n\n#Extract the posterior samples for mu\nmu_samples = trace[\"mu\"]\n\n#Calculate the 95% credible interval\ncred_interval = np.percentile(mu_samples, [2.5, 97.5])\nprint(f\"95% Credible Interval for mu: {cred_interval}\")\n\n```\n\nThis code snippet shows how to compute a 95% credible interval from posterior samples of a parameter.  Note that this calculation is straightforward given the posterior samples produced by an MCMC sampler.\n\n\n## Case Studies: Scientific Applications of Bayes' Theorem\n\n### Application in Astronomy\n\nBayesian methods are extensively used in astronomy for parameter estimation and model selection in various contexts. One common application is analyzing astronomical images to detect and characterize celestial objects.  For example, consider detecting exoplanets using transit photometry.  The observed light curve (flux vs. time) shows periodic dips if a planet transits its star.  A Bayesian model can incorporate the expected shape of the transit, instrumental noise, and stellar variability to estimate the planet's radius, orbital period, and other parameters.\n\nA simplified model could use a Gaussian process to model stellar variability and a parameterized function for the transit shape. The posterior distribution over model parameters is then sampled using MCMC. This approach provides not just point estimates but also quantifies the uncertainty associated with these parameters.\n\n```{python}\n#| echo: true\n# This is a placeholder. A real implementation would require a dedicated astronomy package\n# and a more complex model. This demonstrates the general Bayesian workflow.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate some transit data (placeholder)\ntime = np.linspace(0, 10, 100)\nflux = 1 - 0.01 * np.exp(-((time - 5)**2) / 2) + np.random.normal(0, 0.005, 100)\n\n\nplt.plot(time, flux)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Flux\")\nplt.title(\"Simulated Transit Photometry Data\")\nplt.show()\n\n\n# Bayesian analysis (placeholder - would require PyMC or similar)\n# ...  (Code for setting up a Bayesian model with priors and likelihood, then sampling) ...\n\n# Results (placeholder)\n#print(\"Posterior mean of planet radius:\", radius_mean)\n#print(\"95% Credible interval for planet radius:\", radius_interval)\n```\n\n### Application in Medicine\n\nBayesian methods are essential in medical diagnosis, prognosis, and treatment optimization. A classic example is diagnostic testing.  Let's say we have a test for a disease with sensitivity $P(+\\text{test}| \\text{disease})$ and specificity $P(-\\text{test}|\\neg\\text{disease})$.  Given the prior probability of the disease, $P(\\text{disease})$, and a positive test result, we can use Bayes' theorem to calculate the posterior probability of having the disease:\n\n$P(\\text{disease}|+\\text{test}) = \\frac{P(+\\text{test}|\\text{disease}) P(\\text{disease})}{P(+\\text{test})}$\n\nwhere $P(+\\text{test})$ can be calculated using the law of total probability. This allows for a better understanding of the test's result, accounting for the prevalence of the disease.\n\n```{python}\n#| echo: true\n# Example calculation\nsensitivity = 0.95\nspecificity = 0.90\nprior_prob = 0.01  # Prior probability of disease\n\n# Calculate P(+test)\np_pos_test = (sensitivity * prior_prob) + ((1 - specificity) * (1 - prior_prob))\n\n# Calculate posterior probability\nposterior_prob = (sensitivity * prior_prob) / p_pos_test\n\nprint(f\"Posterior probability of disease given a positive test: {posterior_prob}\")\n\n```\n\n### Application in Genetics\n\nBayesian methods are widely used in genetic analysis, particularly in areas like linkage analysis, genome-wide association studies (GWAS), and gene expression analysis.  For instance, in GWAS, we might test for associations between single nucleotide polymorphisms (SNPs) and a trait of interest. A Bayesian approach allows for incorporating prior information about the genetic architecture of the trait, leading to more powerful and robust results.  This often involves hierarchical models to account for the relationships between SNPs and the trait across different individuals.  Posterior probabilities for associations can provide better interpretations compared to frequentist p-values.\n\n\n### Application in Climate Science\n\nBayesian methods are increasingly important in climate science for handling uncertainty and integrating various sources of data.  Examples include:\n\n* **Parameter estimation in climate models:**  Climate models have many parameters, and Bayesian methods provide a framework for estimating their values by combining observational data with prior information from physical understanding.\n* **Attribution studies:** Determining the influence of human activities on observed climate changes often involves Bayesian model comparison to assess the relative likelihood of different scenarios (e.g., natural variability vs. anthropogenic forcing).\n* **Climate change impact assessment:** Bayesian networks can be used to model complex systems and assess the probability of various impacts, such as sea-level rise or extreme weather events, under different climate scenarios.  This approach can handle uncertainties in both the climate projections and the impact models.\n\n\n```{mermaid}\ngraph LR\nA[Climate Model] --> B(Observed Data);\nA --> C(Prior Information);\nB --> D{Bayesian Inference};\nC --> D;\nD --> E[Posterior Distribution of Parameters];\nE --> F[Uncertainty Quantification];\nF --> G[Climate Projections];\n\n```\n\nThis diagram summarizes a Bayesian approach to parameter estimation in climate modeling.  The posterior distribution provides a full probabilistic description of the model parameters, which then allows for more robust uncertainty quantification in climate projections.  Similar Bayesian frameworks can be applied to other aspects of climate science mentioned above.\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"output-file":"scientific-applications.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"printing","toc":true,"toc-depth":2,"include-in-header":{"text":"\\usepackage{geometry}\n\\usepackage{wrapfig}\n\\usepackage{fvextra}\n\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n\\geometry{\n    paperwidth=6in,\n    paperheight=9in,\n    textwidth=4.5in, % Adjust this to your preferred text width\n    textheight=6.5in,  % Adjust this to your preferred text height\n    inner=0.75in,    % Adjust margins as needed\n    outer=0.75in,\n    top=0.75in,\n    bottom=1in\n}\n\\usepackage{makeidx}\n\\usepackage{tabularx}\n\\usepackage{float}\n\\usepackage{graphicx}\n\\usepackage{array}\n\\graphicspath{{diagrams/}}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"scientific-applications.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"jupyter":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"bibliography":["../../references.bib"],"documentclass":"scrreprt","lof":false,"lot":false,"float":true,"classoption":"paper=6in:9in,pagesize=pdftex,footinclude=on,11pt","fig-cap-location":"top","urlcolor":"blue","linkcolor":"black","biblio-style":"apalike","code-block-bg":"#f0f0f0","code-block-border-left":"#000000","mermaid":{"theme":"neutral"},"fontfamily":"libertinus","monofont":"Consolas","monofontoptions":["Scale=0.7"],"template-partials":["../../before-body.tex"],"indent":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}