{
  "hash": "eb3a852ad19b6c8e839bc006c70306d2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nexecute:\n  echo: false\njupyter: python3\n---\n\n\n\n\n\n\n\n## Introduction to Probability\n\n### What is Probability?\n\nProbability is a branch of mathematics that deals with the likelihood of occurrence of events.  It quantifies uncertainty.  An event is a particular outcome or a set of outcomes of a random phenomenon. The probability of an event is a number between 0 and 1, inclusive. A probability of 0 indicates that the event is impossible, while a probability of 1 indicates that the event is certain.  Probabilities are often expressed as fractions, decimals, or percentages.\n\nFor example, if we flip a fair coin, the probability of getting heads is 1/2, or 0.5, or 50%. This means that if we flip the coin many times, we expect heads to appear approximately half the time.  The foundation of probability lies in understanding the sample space (all possible outcomes) and the events within that space.\n\n### Types of Probability\n\nThere are several ways to interpret and calculate probabilities.  Three common types are:\n\n* **Classical Probability:**  This approach assumes that all outcomes in the sample space are equally likely. The probability of an event is calculated as the ratio of the number of favorable outcomes to the total number of possible outcomes.\n\n    For example, the probability of rolling a 3 on a six-sided die is 1/6 because there is one favorable outcome (rolling a 3) out of six possible outcomes (rolling 1, 2, 3, 4, 5, or 6).\n\n* **Empirical Probability (or Frequentist Probability):** This approach is based on observing the frequency of an event in a large number of trials.  The probability is estimated as the ratio of the number of times the event occurred to the total number of trials.\n\n::: {#61a8f8a4 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport random\n\n# Simulate coin tosses\nnum_tosses = 1000\nheads = sum(random.random() < 0.5 for _ in range(num_tosses))\ntails = num_tosses - heads\n\n# Calculate empirical probabilities\nprob_heads = heads / num_tosses\nprob_tails = tails / num_tosses\n\n# Plot results\nlabels = 'Heads', 'Tails'\nsizes = [prob_heads, prob_tails]\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.title('Empirical Probabilities of Coin Tosses')\nplt.show()\n\nprint(f\"Empirical probability of heads: {prob_heads:.4f}\")\nprint(f\"Empirical probability of tails: {prob_tails:.4f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-2-output-1.png){width=540 height=409}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEmpirical probability of heads: 0.4900\nEmpirical probability of tails: 0.5100\n```\n:::\n:::\n\n\n* **Subjective Probability:** This approach relies on an individual's judgment or belief about the likelihood of an event.  It is often used when there is limited data or when events are unique and not easily repeatable. For example, the probability of a particular company's stock price increasing next week might be based on an analyst's subjective assessment.\n\n\n\n\n\n\n\n\n\n```{mermaid}\ngraph LR\n    A[Probability] --> B(Classical);\n    A --> C(Empirical);\n    A --> D(Subjective);\n    B --> E{Equally Likely Outcomes};\n    C --> F{Observed Frequency};\n    D --> G{Expert Judgment/Belief};\n```\n\n\n\n\n\n\n\n\n\n## Frequentist vs. Bayesian Approaches\n\n### Frequentist Interpretation of Probability\n\nThe frequentist interpretation defines probability as the long-run frequency of an event.  It's based on the idea of repeating an experiment many times under identical conditions. The probability of an event is the limit of its relative frequency as the number of trials approaches infinity.  Frequentists focus on objective evidence obtained from data.  They don't assign probabilities to hypotheses; instead, they assess hypotheses based on the observed data and the likelihood of observing that data given the hypothesis.  Frequentist methods often rely on p-values and confidence intervals to make inferences.\n\nFor example, a frequentist would estimate the probability of getting heads when flipping a coin by flipping the coin many times and calculating the proportion of heads obtained.  The more flips, the better the estimate of the true probability (assuming a fair coin).  There's no inherent prior belief about the fairness of the coin; the probability is derived solely from the observed data.\n\n\n### Bayesian Interpretation of Probability\n\nThe Bayesian interpretation views probability as a degree of belief or uncertainty about an event. This belief is updated as new evidence becomes available. Bayesian methods incorporate prior knowledge or beliefs (prior probabilities) about the event.  When new data is observed, Bayes' theorem is used to update the prior probability, resulting in a posterior probability.  This posterior probability reflects the updated belief about the event after considering the new evidence.  Bayesian methods allow for quantifying uncertainty in parameters and predictions.\n\n\n### Key Differences and When to Use Each Approach\n\n| Feature          | Frequentist                               | Bayesian                                     |\n|-----------------|-------------------------------------------|---------------------------------------------|\n| **Probability** | Long-run frequency                        | Degree of belief                             |\n| **Prior Knowledge** | Not incorporated                         | Explicitly incorporated (prior probability) |\n| **Inference**    | Based on data frequency; p-values, confidence intervals | Based on updating prior beliefs with data; posterior probability |\n| **Parameters**   | Treated as fixed, unknown values         | Treated as random variables with probability distributions |\n| **Uncertainty**  | Measured by confidence intervals           | Measured by probability distributions        |\n\n\n**When to use which approach:**\n\n* **Frequentist:**  Suitable when you have a large amount of data and the goal is to make objective inferences based on the data alone.  Useful for hypothesis testing, estimating population parameters, and establishing confidence intervals.\n\n* **Bayesian:**  Suitable when you have limited data, prior knowledge is available, or the goal is to quantify uncertainty about parameters or predictions.  Useful for modeling complex systems, incorporating expert opinion, and making predictions with uncertainty quantification.\n\n\n### Illustrative Examples\n\nLet's consider a simple example of testing whether a coin is fair.\n\n**Frequentist approach:**\n\nWe flip the coin 10 times and observe 7 heads. A frequentist might conduct a hypothesis test to determine if the proportion of heads is significantly different from 0.5 (the expected proportion for a fair coin).\n\n\n**Bayesian approach:**\n\nWe start with a prior belief about the probability of heads (e.g., a uniform prior between 0 and 1, reflecting no strong prior belief). We then update this prior using Bayes' theorem after observing 7 heads in 10 flips to obtain a posterior probability distribution for the probability of heads.  This posterior distribution will reflect our updated belief, taking into account both the prior belief and the observed data.\n\n::: {#44b66850 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Bayesian approach example (simplified)\n\n# Prior (uniform distribution)\nprior = stats.uniform(0, 1)\n\n# Likelihood (Binomial distribution)\nlikelihood = stats.binom(10, 0.7) # Observed 7 heads in 10 tosses\n\n# Posterior (proportional to prior * likelihood)\n# Numerical integration is needed to get exact posterior\n# We'll use sampling for approximation here.\n\n\nnum_samples = 10000\nsamples = prior.rvs(num_samples)\nposterior_probs = likelihood.pmf(7) * stats.beta(8, 4).pdf(samples)\nposterior_probs = posterior_probs / np.sum(posterior_probs) #Normalize to get probability distribution\n\nplt.hist(samples[posterior_probs > 0], weights=posterior_probs[posterior_probs > 0], bins=20)\nplt.title('Posterior Distribution for Probability of Heads')\nplt.xlabel('Probability of Heads')\nplt.ylabel('Density')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-3-output-1.png){width=597 height=449}\n:::\n:::\n\n\nThis Python code demonstrates a simplified Bayesian approach using sampling to approximate the posterior distribution.  A more accurate approach would involve numerical integration or Markov Chain Monte Carlo (MCMC) methods, which are beyond the scope of this introductory section.  The plot shows the posterior distribution of the probability of heads after observing the data.  We can see that the posterior distribution is shifted towards higher probabilities of heads compared to the uniform prior, reflecting the observed data.\n\n\n## Probability Axioms and Rules\n\n### Kolmogorov's Axioms\n\nAndrey Kolmogorov formalized the foundations of probability theory with his three axioms:\n\n1. **Non-negativity:** The probability of any event A is non-negative: $P(A) \\geq 0$.\n\n2. **Normalization:** The probability of the sample space (the set of all possible outcomes) is 1: $P(\\Omega) = 1$.\n\n3. **Additivity:** For any countable sequence of mutually exclusive events $A_1, A_2, A_3,...$, the probability of their union is the sum of their individual probabilities: $P(\\cup_i A_i) = \\sum_i P(A_i)$. Mutually exclusive means that no two events can occur simultaneously.\n\nThese axioms provide a rigorous mathematical framework for defining and manipulating probabilities.\n3. **Additivity:** For any countable sequence of mutually exclusive events A₁, A₂, A₃,..., the probability of their union is the sum of their individual probabilities:  P(∪ᵢAᵢ) = ΣᵢP(Aᵢ).  Mutually exclusive means that no two events can occur simultaneously.\n3. **Additivity:** For any countable sequence of mutually exclusive events $A_1, A_2, A_3,...$, the probability of their union is the sum of their individual probabilities: $P(\\cup_i A_i) = \\sum_i P(A_i)$. Mutually exclusive means that no two events can occur simultaneously.\n\nThese axioms provide a rigorous mathematical framework for defining and manipulating probabilities.\n\n\n### Addition Rule\n\nThe addition rule describes the probability of the union of two events.\n\n* **For any two events A and B:** $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$.  The term $P(A \\cap B)$ (the probability of both A and B occurring) is subtracted to avoid double-counting the overlap between A and B.\n\n* **If A and B are mutually exclusive (disjoint), then $P(A \\cap B) = 0$,** simplifying the addition rule to: $P(A \\cup B) = P(A) + P(B)$.\n* **If A and B are mutually exclusive (disjoint), then $P(A \\cap B) = 0$,** simplifying the addition rule to: $P(A \\cup B) = P(A) + P(B)$.\n* **If A and B are mutually exclusive (disjoint), then $P(A \\cap B) = 0$,** simplifying the addition rule to: $P(A \\cup B) = P(A) + P(B)$.\n\n\n### Multiplication Rule\n\nThe multiplication rule describes the probability of the intersection of two events.\n\n* **For any two events A and B:** $P(A \\cap B) = P(A | B) * P(B)  = P(B | A) * P(A)$. This states that the probability of both A and B occurring is equal to the probability of A given B (conditional probability), multiplied by the probability of B.  The same holds true if we reverse A and B.\n\n* **If A and B are independent,** meaning that the occurrence of one event doesn't affect the probability of the other, then $P(A | B) = P(A)$ and $P(B | A) = P(B)$.  This simplifies the multiplication rule to: $P(A \\cap B) = P(A) * P(B)$.\n\n\n### Conditional Probability\n\nConditional probability describes the probability of an event occurring given that another event has already occurred.\n\nThe conditional probability of event A given event B is denoted as $P(A | B)$ and calculated as:\n\n$P(A | B) = \\frac{P(A \\cap B)}{P(B)}$, provided $P(B) > 0$.\nP(A | B) = P(A ∩ B) / P(B), provided P(B) > 0.\n$P(A | B) = \\frac{P(A \\cap B)}{P(B)}$, provided $P(B) > 0$.\n\n\n### Law of Total Probability\n\nThe law of total probability allows us to calculate the probability of an event by considering all possible mutually exclusive and exhaustive ways that the event could occur.\nLet $A$ be an event, and let $B_1, B_2, \\dots, B_n$ be a partition of the sample space (meaning they are mutually exclusive and their union is the entire sample space). Then:\n\n$P(A) = \\sum_{i=1}^{n} P(A | B_i) P(B_i)$\nThis formula breaks down the probability of $A$ into the probabilities of $A$ occurring given each of the possible scenarios ($B_i$), weighted by the probability of each scenario occurring.\n\n::: {#4f9946f7 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n#Illustrative Example for Law of Total Probability\n\n#Suppose we have two boxes, Box1 and Box2.\n#Box1 contains 3 red balls and 2 blue balls.\n#Box2 contains 1 red ball and 4 blue balls.\n#We randomly choose a box and then randomly draw a ball from that box.\n\n#Probabilities:\nP_Box1 = 0.5  #Probability of choosing Box1\nP_Box2 = 0.5  #Probability of choosing Box2\nP_Red_given_Box1 = 3/5  #Probability of drawing a red ball given Box1 was chosen\nP_Red_given_Box2 = 1/5  #Probability of drawing a red ball given Box2 was chosen\n\n\n#Using the law of total probability to find the overall probability of drawing a red ball:\nP_Red = P_Red_given_Box1 * P_Box1 + P_Red_given_Box2 * P_Box2\n\nprint(f\"The overall probability of drawing a red ball is: {P_Red}\")\n\n#Visualization using a bar chart\nlabels = 'Red from Box1', 'Red from Box2'\nsizes = [P_Red_given_Box1 * P_Box1, P_Red_given_Box2 * P_Box2]\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\nplt.title('Probabilities of Drawing a Red Ball')\nplt.axis('equal')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe overall probability of drawing a red ball is: 0.4\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-4-output-2.png){width=540 height=409}\n:::\n:::\n\n\nThis code provides a simple illustration of the Law of Total Probability.  The chart visually represents the contribution of each scenario to the overall probability.  More complex examples would require more sophisticated calculations and might use simulation methods.\n\n\n## Conditional Probability and Bayes' Theorem (Preview)\n\n### Defining Conditional Probability\n\nConditional probability quantifies the likelihood of an event occurring given that another event has already happened.  We denote the conditional probability of event A occurring given that event B has occurred as $P(A|B)$, read as \"the probability of A given B\".  Formally, it's defined as:\n\n$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$, provided $P(B) > 0$\n\nThis formula states that the conditional probability of A given B is the ratio of the probability of both A and B occurring to the probability of B occurring.  It essentially restricts the sample space to only those outcomes where B has occurred.\n\n\n### Understanding Conditional Probability with Examples\n\n**Example 1:  Drawing Cards**\n\nConsider drawing two cards from a standard deck without replacement.  What is the probability that the second card is a King, given that the first card is a Queen?\n\nLet $A$ be the event that the second card is a King.\nLet $B$ be the event that the first card is a Queen.\n\n$P(A|B)$ is not simply $4/52$ (there are 4 Kings in a 52-card deck).  The fact that the first card was a Queen changes the probability. There are now only 51 cards remaining, and still 4 Kings.  Therefore:\n\n$P(A|B) = 4/51$\n\n\n**Example 2: Medical Testing**\n\nSuppose a test for a disease has a 95% sensitivity (probability of correctly identifying a person with the disease) and a 90% specificity (probability of correctly identifying a person without the disease).  If 1% of the population has the disease, what is the probability that a person who tests positive actually has the disease?  (This example highlights the importance of considering base rates).  This problem requires Bayes' theorem, which we will introduce shortly.\n\n\n### Introduction to Bayes' Theorem\n\nBayes' theorem provides a mathematical formula for calculating conditional probabilities.  It's a powerful tool for updating beliefs in light of new evidence.  It's derived directly from the definition of conditional probability and the multiplication rule:\n\n$P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$\n\nwhere:\n\n* $P(A|B)$ is the posterior probability of A given B.  This is what we want to calculate.\n* $P(B|A)$ is the likelihood of B given A.\n* $P(A)$ is the prior probability of A.\n* $P(B)$ is the marginal likelihood of B (often calculated using the law of total probability).\n\n\n### Intuition Behind Bayes' Theorem\nBayes' theorem describes how to update our beliefs about an event ($A$) when we get new evidence ($B$).\n\n* **Prior Probability ($P(A)$):** This is our initial belief about the probability of $A$ before we see any new evidence.\n\n* **Likelihood ($P(B|A)$):**  This is the probability of observing the evidence $B$, given that $A$ is true.\n\n* **Posterior Probability ($P(A|B)$):** This is our updated belief about the probability of $A$ *after* considering the new evidence $B$.  It combines the prior belief with the new evidence.\n\n* **Marginal Likelihood ($P(B)$):** This is the probability of the evidence $B$, regardless of whether $A$ is true or false. It acts as a normalization factor.\n\nBayes' theorem allows us to formally quantify this updating process, moving from prior belief to a more informed posterior belief based on data.  The theorem is stated as:\n\n$P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$\n\n::: {#7f0de5fa .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#Illustrative Example of Bayes' Theorem (Medical Testing)\n\n#Prior Probability (Disease Prevalence)\nprior_prob_disease = 0.01 # 1%\n\n#Likelihood (Test accuracy)\nsensitivity = 0.95 #95% sensitivity\nspecificity = 0.90 #90% specificity\n\n#Let's say a person tests positive: What is the probability they actually have the disease?\n\n# Calculate some probabilities (we'll use these for the visualization)\nprob_pos_given_disease = sensitivity\nprob_neg_given_no_disease = specificity\nprob_pos_given_no_disease = 1 - specificity\nprob_neg_given_disease = 1 - sensitivity\n\n#Calculate the probability of a positive test result (using the law of total probability)\nprob_positive_test = (prob_pos_given_disease * prior_prob_disease) + (prob_pos_given_no_disease * (1 - prior_prob_disease))\n\n#Bayes' Theorem:\nposterior_prob_disease = (prob_pos_given_disease * prior_prob_disease) / prob_positive_test\n\nprint(f\"Posterior probability of having the disease given a positive test result: {posterior_prob_disease:.4f}\")\n\n\n#Visualization:\nlabels = 'Disease', 'No Disease'\nprior_sizes = [prior_prob_disease, 1 - prior_prob_disease]\nposterior_sizes = [posterior_prob_disease, 1 - posterior_prob_disease]\n\nfig, axes = plt.subplots(1,2, figsize=(10,5))\naxes[0].pie(prior_sizes, labels=labels, autopct='%1.1f%%', startangle=90)\naxes[0].set_title('Prior Probability (Disease Prevalence)')\naxes[1].pie(posterior_sizes, labels=labels, autopct='%1.1f%%', startangle=90)\naxes[1].set_title('Posterior Probability (After Positive Test)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPosterior probability of having the disease given a positive test result: 0.0876\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-5-output-2.png){width=763 height=377}\n:::\n:::\n\n\nThe code and chart illustrate how Bayes' theorem updates our belief about the probability of having a disease after a positive test result.  The shift from prior to posterior probability demonstrates the power of incorporating new data to refine our understanding.\n\n\n## Common Probability Distributions (Preview)\n\n### Discrete vs. Continuous Distributions\n\nProbability distributions describe the probability of different outcomes for a random variable.  A random variable is a variable whose value is a numerical outcome of a random phenomenon.  Distributions are categorized as either discrete or continuous:\n\n* **Discrete Distributions:**  The random variable can only take on a finite number of values or a countably infinite number of values.  Examples include the number of heads in three coin flips (0, 1, 2, or 3) or the number of cars passing a point on a highway in an hour.\n\n* **Continuous Distributions:** The random variable can take on any value within a given range or interval.  Examples include height, weight, temperature, or time.\n\n\n### Bernoulli Distribution\n\nThe Bernoulli distribution models the probability of success or failure in a single trial.  It's a discrete distribution with only two possible outcomes:\n\n* Success (usually denoted as 1) with probability $p$\n* Failure (usually denoted as 0) with probability $1-p$\n\n::: {#c9eb59e8 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\n\n#Parameters\np = 0.6 #Probability of success\n\n#Generate random samples\nsamples = bernoulli.rvs(p, size=1000)\n\n#Plot the distribution\nplt.hist(samples, bins=[-0.5, 0.5, 1.5], align='mid', rwidth=0.8, density=True)\nplt.xticks([0,1])   \nplt.xlabel('Outcome (0=Failure, 1=Success)')\nplt.ylabel('Probability')\nplt.title('Bernoulli Distribution (p=0.6)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-6-output-1.png){width=589 height=449}\n:::\n:::\n\n\n### Binomial Distribution\n\nThe binomial distribution models the probability of getting a certain number of successes in a fixed number of independent Bernoulli trials.  It's a discrete distribution.\n\nThe probability of getting exactly $k$ successes in $n$ trials is given by the probability mass function:\n\n$P(X = k) = \\binom{n}{k} p^k (1-p)^{(n-k)}$\nwhere:\n\n* $n$ is the number of trials\n* $k$ is the number of successes\n* $p$ is the probability of success in a single trial\n* $\\binom{n}{k}$ is the binomial coefficient, calculated as $n! / (k! * (n-k)!)$\n\n::: {#d787748f .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n# Parameters\nn = 10  # Number of trials\np = 0.5  # Probability of success\n\n# Generate random samples\nsamples = binom.rvs(n, p, size=10000)\n\n# Plot the distribution\nplt.hist(samples, bins=range(n + 2), align='left', rwidth=0.8, density=True)\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.title('Binomial Distribution (n=10, p=0.5)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-7-output-1.png){width=597 height=449}\n:::\n:::\n\n\n### Normal Distribution\n\nThe normal (or Gaussian) distribution is a continuous distribution that's bell-shaped and symmetrical.  It's characterized by its mean ($\\mu$) and standard deviation ($\\sigma$).  Many natural phenomena approximately follow a normal distribution.\n\n::: {#df51bb17 .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parameters\nmu = 0  # Mean\nsigma = 1  # Standard deviation\n\n# Generate random samples\nsamples = norm.rvs(loc=mu, scale=sigma, size=10000)\n\n# Plot the distribution\nplt.hist(samples, bins=50, density=True, alpha=0.6, color='skyblue')\nx = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 100)\nplt.plot(x, norm.pdf(x, mu, sigma), color='red', linewidth=2)\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Normal Distribution (μ=0, σ=1)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-8-output-1.png){width=597 height=449}\n:::\n:::\n\n\n### Uniform Distribution\n\nThe uniform distribution assigns equal probability to all outcomes within a given range.  It can be either discrete or continuous.\n\n* **Continuous Uniform Distribution:** The probability density function is constant within the specified interval [a, b].\n\n::: {#2c221b90 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform\n\n# Parameters\na = 0  # Lower bound\nb = 10  # Upper bound\n\n# Generate random samples\nsamples = uniform.rvs(loc=a, scale=b, size=10000)\n\n# Plot the distribution\nplt.hist(samples, bins=50, density=True, alpha=0.6, color='skyblue')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Uniform Distribution (a=0, b=10)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](understanding-probability_files/figure-html/cell-9-output-1.png){width=597 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "understanding-probability_files"
    ],
    "filters": [],
    "includes": {}
  }
}